[{"path":"index.html","id":"about","chapter":"1 About","heading":"1 About","text":"book dedicated family, Balqisu, Nyarko Ekuba.young scientists benefit book colleagues , every way, helped write , say big thank . motivation comes dire urgent need many, especially developing countries, use freely available yet sophisticated statistical software analysing clinical data. regions buying statistical software often affordable people, R comes great relief. huge gap theoretical practical knowledge statistical applications many scientists. R, open-source statistical software, offers unique vital opportunity bridge gap.book introduces data analysis R persons little knowledge . step--step introduction data analysis R deliberately organised limited text many practical examples.Readers briefly introduced R RStudio. Required packages used chapters require . However, using function package explained.","code":""},{"path":"introduction.html","id":"introduction","chapter":"2 Introduction","heading":"2 Introduction","text":"","code":""},{"path":"introduction.html","id":"the-statistical-data-analyst","chapter":"2 Introduction","heading":"2.1 The statistical data analyst","text":"Statistical data analysis just using computer software generate results. involves basic understanding data type best way analyse present data make meaning general population. Thus, data analyst:Must understand genesis (study methodology) involved obtaining data first place. conclusion data may differ depending study methodology used hypothesis tested. prudent, therefore, statistical data analyst involved data collection process right beginning.able point errors data collection process early stages. avoids wasting valuable resources data may answer question.Provide valuable advice best method analysing data hand.Perform analysis scientifically soundly applying current statistically appropriate principles.Present result analysis manner makes easy persons without statistical analytical expertise understand least effort. requires statistical data analyst position explain analysis common language.Finally, data analyst must know limit. often instances analyst seek ”professional” help, even though may feel right path. never hurts seek second opinion peers.\n, therefore, goes without saying prior discussion data analyst must firm understanding statistical research methodology.","code":""},{"path":"introduction.html","id":"statistical-software","chapter":"2 Introduction","heading":"2.2 Statistical software","text":"years back, statistical analysis one tedious processes done mainly dedicated statisticians. advent computers statistical software, become rather handy, many advantages disadvantages.\nmain advantages :\n1. tremendous speed large data processed results obtained.\n1. accuracy statistical calculations performed. Computers make mistakes one beware rounding software. software can perform calculations specific number decimal places. Therefore, one confronted figure 1.00377655432, software may work 1.0037765, leaving last four digits. Calculations using truncated value likely different result non-truncated figure, thus affecting accuracy final result.\n1. Many modern statistical software can read data varied sources formats. makes easy transfer data one software another without re-enter data collected second computer software. transferability enabled use digital equipment smartphones, personal digital assistants tablets data collection. Data collected manner said ready cleaning analysis, bypassing data entry stage.\n1. Plotting graphs one important uses modern-day computerised data analysis. Statistical software tends make rather tedious process almost hassle-free accords us ability redo plot scratch click button.Despite advantages, many disadvantages also inherent use computers statistical software. include:Many people little statistical knowledge can manipulate data come conclusions often tend spurious. cliche ”Garbage Garbage ” apply better situation.Many commonly used software tend reliable accurate. large number often user-written statistical software freely available online, one needs cautious output generated. wrongly written codes errors, thus producing faulty results.Unfortunately, used, reliable accurate statistical software tends expensive well. notwithstanding, , R, combine free open source versatility reliability. forms basis choice R book.","code":""},{"path":"introduction.html","id":"obtaining-and-installing-r","chapter":"2 Introduction","heading":"2.3 Obtaining and installing R","text":"R free software programming language environment data manipulation, calculation graphical display. can run Windows, MacOS X Unix systems. great applications many academic fields, including mathematics, economics epidemiology. capability enhanced many packages written individuals years. R great advantage able handle many datasets simultaneously. However, functionality comes cost, discussed subsequent chapters. R also great graphics functionality requires practice.Several advanced statistical mathematical functions, regression survival analysis, also implemented R.\nR many packages obtainable free http://cran.r-project.org/. current version time writing book R-4.2.1. Windows operating system version can installed 32 64-bit operating systems. Download base file https://cran.r-project.org/bin/windows/base/, save computer install , preferably administrator, following -screen instructions.","code":""},{"path":"introduction.html","id":"obtaining-and-installing-rstudio","chapter":"2 Introduction","heading":"2.4 Obtaining and installing RStudio","text":"RStudio Integrated Development Environment R software Python. provides interface adds functionality automation R. downloadable https://posit.co/download/rstudio-desktop/. two forms, desktop version Server version used within web browser. simplicity easy following processes book, preferable download install RStudio.","code":""},{"path":"statistical-data-types.html","id":"statistical-data-types","chapter":"3 Statistical data types","heading":"3 Statistical data types","text":"begin proper data analysis revising statistical data types . two broad types data; Quantitative Qualitative","code":""},{"path":"statistical-data-types.html","id":"qualitative-data","chapter":"3 Statistical data types","heading":"3.1 Qualitative data","text":"data type, observations fall distinctive categories. \nusually scale applicable qualitative data type. divided :","code":""},{"path":"statistical-data-types.html","id":"nominal","chapter":"3 Statistical data types","heading":"3.1.1 Nominal","text":"qualitative data types order. colors flag instance can ”red” ”yellow” ”green”. None can said coming . Contrast one immediately .","code":""},{"path":"statistical-data-types.html","id":"binary","chapter":"3 Statistical data types","heading":"3.1.2 Binary","text":"special type nominal data type binary data. common statistical analysis. observations can take two values. question instance records presence disease ”Yes” ”” answer. Sex usually recorded ”Male” ”Female”.","code":""},{"path":"statistical-data-types.html","id":"ordinal","chapter":"3 Statistical data types","heading":"3.1.3 Ordinal","text":"ordinal qualitative data type order . commonly used one socioeconomic status, often categorised ”Low”, ”Middle” ”High”. Although say interval ”Low” ”Middle” ”Middle” ”High”, know ”Low” lower ”Middle” turn also lower ”High”. Likert scale, well-known scale many social science research also example ordinal scale. Ordinal variables often created quantitative (see )variables. E.g. ages group men\ncan converted age groups desired number categories.","code":""},{"path":"statistical-data-types.html","id":"quantitative-data","chapter":"3 Statistical data types","heading":"3.2 Quantitative data","text":"Quantitative numerical data observations numbers can measured. two types:","code":""},{"path":"statistical-data-types.html","id":"discrete","chapter":"3 Statistical data types","heading":"3.2.1 Discrete","text":"Discrete quantitative data one specific values can obtained. number persons attending theatre can whole number. number votes obtained election. Although discrete quantitative variables often analysed continuous ones can occasionally pose problems analysed . dealing subsequent chapters.","code":""},{"path":"statistical-data-types.html","id":"continuous","chapter":"3 Statistical data types","heading":"3.2.2 Continuous","text":"Continuous quantitative variables hand can measured precision, thereby making figures present precise experimenter desires. instance, distance two towns can measured kilometres much precision possible. Theoretically, can 12.0kms much 12.0234278kms.","code":""},{"path":"statistical-data-types.html","id":"other-specific-data-types","chapter":"3 Statistical data types","heading":"3.3 Other specific data types","text":"specific subtypes data encountered statistical analysis. :","code":""},{"path":"statistical-data-types.html","id":"ratios","chapter":"3 Statistical data types","heading":"3.3.1 Ratios","text":"Ratios special continuous variables generated two variables. example, ratio boys girls sample can determined dividing number boys girls. figures obtained similar continuous variables require special techniques analysis.","code":""},{"path":"statistical-data-types.html","id":"rates","chapter":"3 Statistical data types","heading":"3.3.2 Rates","text":"Rates population parameters often used medicine epidemiology. Examples include population growth rate mortality rate. also statistic parameter generated two\nothers. mortality rate generated instance number deaths time interval. case neonatal mortality rate, generated number deaths number live births period. analysis, often treated indicated ratios .","code":""},{"path":"statistical-data-types.html","id":"percentage","chapter":"3 Statistical data types","heading":"3.3.3 Percentage","text":"Percentages peculiar often definite maximum minimum 0% 100% respectively. However, percentage changes can take value. instance, change 4 3 yield negative (-25%) change. avoid tedious nature percentages advisable often retain work specific values involved determining percentage rather percentages derived.","code":""},{"path":"statistical-data-types.html","id":"ranks","chapter":"3 Statistical data types","heading":"3.3.4 Ranks","text":"Ranks often treated continuous variables though often . well-known example position student class test. position just relative others differs actual mark scored. ranks may give impression equal space consecutive ranks actual difference may much smaller bigger rank difference.","code":""},{"path":"r-data-types.html","id":"r-data-types","chapter":"4 R data types","heading":"4 R data types","text":"data objects R made smaller units referred ”Atomic” data.\nvarious atomic data types Integer, Double, Complex, Logical,\nCharacter, Factor Date Time. explained\nsubsequently. determine data types R commonly used function\nclass(). example uses function determine data type \n4.2.expected numeric variable. Next, determine class data\n””R classifies character data type. Finally, FALSE?logical data type.","code":"class(4.2)\n[1] \"numeric\"class(\"A\")\n[1] \"character\"class(FALSE)\n[1] \"logical\""},{"path":"r-data-types.html","id":"integer","chapter":"4 R data types","heading":"4.1 Integer","text":"integer data types made numeric variables can counted,\nsimilar discrete quantitative variable described . default, R\nstore numbers integers may arise situations numbers\nconverted integers facilitate manipulations. determine\ndata class integer use function .integer(). convert data\nanother type integer use function .integer(). illustration,\ndetermine number 9 R integer. first, assign number 9\nX ask X stored integer.! Next, convert integer, time calling Y. find\n.integer now.","code":"X <- 9\nis.integer(X)\n[1] FALSEY <- as.integer(X)\nis.integer(Y)\n[1] TRUE"},{"path":"r-data-types.html","id":"double","chapter":"4 R data types","heading":"4.2 Double","text":"number can take value including decimals, similar \ncontinuous quantitative variable discussed . Double default type\nnumeric variable used R. illustrate point let’s look \nproperties number 7.1 R. First assign name “” 7.1.find class “Double”.Numeric data types stored double never stored exact rather \napproximations real numbers. illustrate , add three test \nanswer 21.3It may appear strange adding 7.1 three times equal 21.3. \nstored approximate exact. \nimportant manipulating data R many statistical software.","code":"\nA <- 7.1is.double(A)\n[1] TRUE21.3 == A + A + A \n[1] FALSE"},{"path":"r-data-types.html","id":"logical","chapter":"4 R data types","heading":"4.3 Logical","text":"Logical object stored TRUE FALSE. example shows \ncreation “Z” statement asking 5 less 8. Z, therefore, \nlogical (TRUE) data type. First, assign values 5 8 X Y\nrespectively create logical data type Z asking X less \nY R.Next, determine class ZLogical objects innate values R FALSE considered \nvalue 0 TRUE value 1. output demonstrates .Apart “<” operator used example logical\noperators R. following examples illustrate use logical\noperators. begin assigning ages man wife 45 23\nyears, respectively.Next, use answer basic questions couple. First, \nask wife’s age less equal 35Next, ask husband greater equal 45 yearsThe next example combines three logical operators. ask wife \nless 25 years time, husband greater 35 yearsFinally, ask whether wife less 25 years husband greater\n50 years","code":"X <- 5\nY <- 8\nZ <- X < Y\nZ\n[1] TRUEclass(Z)\n[1] \"logical\"FALSE + FALSE\n[1] 0\nFALSE + TRUE\n[1] 1\nTRUE + TRUE\n[1] 2\nwife <- 23\nhusband <- 45wife <= 34\n[1] TRUEhusband >= 45\n[1] TRUE(wife < 25) & (husband > 35)\n[1] TRUE(wife < 25) | (husband > 50)\n[1] TRUE"},{"path":"r-data-types.html","id":"character","chapter":"4 R data types","heading":"4.4 Character","text":"Character object enclosed double quotes. called strings \nnames used mathematical calculations. Examples include “red”,\n“Male”, “1”. seen, “1” (different number 1) character \nused calculations unless converted another object form \ninteger double. illustrate creating two characters belowAnd determine class.illustrate “2” character added, doHowever, can convert B numeric variable C using function .numeric().can now use numeric variable C calculations ","code":"\nA <- \"x\"\nB <- \"2\"class(A)\n[1] \"character\"\nclass(B)\n[1] \"character\"B + B\nError in B + B: non-numeric argument to binary operatorC <- as.numeric(B)\nclass(C)\n[1] \"numeric\"C + C\n[1] 4"},{"path":"r-data-types.html","id":"factor","chapter":"4 R data types","heading":"4.5 Factor","text":"Factor R categorical variable sex (male & female). Factor\nvariables levels representing different categories. Sex naturally \ntwo levels, Male, Female. Factors can derived numeric character\nobjects using .factor(). form character variable length four\ncalled blood.grp using one used functions R c().convert factor variableNext, determine categories (levels) factor variable using \nfunction levels()Often, becomes necessary factor variables converted integer\nretaining order categories appear. instance, one may\nfactor variable levels “”, “B”, “C”, “D” wants convert\ninteger variable , B, C D represented 1, 2, 3 \n4. achieved R unclassing. Unclassing factor variable assigns\nnumbers, starting 1 levels factors order \nlevels. output shows unclassed numbers factor blood.grp2\nlevels refer . use unclass() function.useful function R generate factor variable gl(). generates\nfactor variable specified number levels (n) replications (k). \npractical example shown . Factor fac.1 created forming vector\nthree levels two replications.factor can also created labels shown","code":"blood.grp <- c(\"O\", \"AB\", \"B\", \"A\")\nclass(blood.grp)\n[1] \"character\"blood.grp2 <- as.factor(blood.grp)\nclass(blood.grp2) \n[1] \"factor\"levels(blood.grp2) \n[1] \"A\"  \"AB\" \"B\"  \"O\" unclass(blood.grp2)\n[1] 4 2 3 1\nattr(,\"levels\")\n[1] \"A\"  \"AB\" \"B\"  \"O\" fac.1 <- gl(n=3, k=2)\nfac.1\n[1] 1 1 2 2 3 3\nLevels: 1 2 3fac.2 <- gl(n=3, k=2, label=c(\"Apple\", \"Mango\",\" Pear\"))\nfac.2\n[1] Apple Apple Mango Mango  Pear  Pear\nLevels: Apple Mango  Pear"},{"path":"r-data-types.html","id":"ordered-factor","chapter":"4 R data types","heading":"4.6 Ordered Factor","text":"order factor important must declared ordered factor,\nalso known ordinal categorical variable explained earlier chapter.\ncreate character variable.Next, convert ordered factor using function ordered().ordered factor variable can also derived using gl() function introduced .","code":"size <- c(\"Medium\", \"Large\", \"Small\", \"Medium\")\nclass(size)\n[1] \"character\"ord.size <- ordered(size, levels=c(\"Small\", \"Medium\", \"Large\"))\nord.size\n[1] Medium Large  Small  Medium\nLevels: Small < Medium < Large\nclass(ord.size)\n[1] \"ordered\" \"factor\" fac.3 <- gl(n=3, k=2, ordered=TRUE, label=c(\"Small\", \"Medium\", \"Large\"))\nfac.3\n[1] Small  Small  Medium Medium Large  Large \nLevels: Small < Medium < Large"},{"path":"r-data-types.html","id":"date-and-time","chapter":"4 R data types","heading":"4.7 Date and time","text":"Date Time objects last discussed section. can \ncreated functionsas.Date(), .POSIXct(), .POSIXlt(), strptime(), ISOdatetime() ISOdate().date time data creation require use character data \n“format”. format dictates function format character\ndata recorded .e. dd/mm/yy, mm/dd/yyyy, yyyy/mm/dd hh:mm:ss etc. \nformats declared % symbol.first object create class “Date” created using \ncharacter object “01/01/1970”. first create character objectNext, convert character object Date objectNote format specified. day, month year preceded %\nsymbol. symbols separate values dates also specified\naccordingly. Next, derive object class Date time referred\nR POSIXct POSIXlt. Though referred “Date Time” \ncan Date Date Time specified formats. create \nPOSIXct Date format.Next, create Date Time variable shows date timeThe next example uses function strptime() create “POSIXlt” DateTime\nformat data.Next, use ISOdatetime() create “POSIXct” DateTime format data type.\nNote two functions ISOdatetime() ISOdate() take individual\nnumeric values combine rather convert character variables.finally function ISOdate()Mathematical manipulations can done date objects. Subtracting one date\nyields object period two. object\nclass difftime. illustration, determine time difference\ndt5 dt6It worth noting function R, difftime() specifically\ndesigned finding time differences.functions weekdays(), months() quarters() extract “character”\ndatatype days, months quarters date objects respectively.","code":"dt.str <- \"01/01/1970\"\nclass(dt.str)\n[1] \"character\"dt1<-as.Date(dt.str, format=\"%d/%m/%Y\")\ndt1\n[1] \"1970-01-01\"\nclass(dt1)\n[1] \"Date\"dt2 <- as.POSIXct(\"2003-01-23\")\ndt2\n[1] \"2003-01-23 GMT\"\nclass(dt2)\n[1] \"POSIXct\" \"POSIXt\" dt3 <- as.POSIXct(\"2003-04-23 15:34\")\ndt3\n[1] \"2003-04-23 15:34:00 GMT\"\nclass(dt3)\n[1] \"POSIXct\" \"POSIXt\" dt4<-strptime(\"02/27/92 11:30:10\", format=\"%m/%d/%y %H:%M:%S\")\ndt4\n[1] \"1992-02-27 11:30:10 GMT\"\nclass(dt4)\n[1] \"POSIXlt\" \"POSIXt\" dt5<-ISOdatetime(\n    year=2013, month=4, day=7, hour = 12, min = 33, sec = 10, tz = \"GMT\"\n    )\ndt5\n[1] \"2013-04-07 12:33:10 GMT\"\nclass(dt5)\n[1] \"POSIXct\" \"POSIXt\" dt6 <- ISOdate(year = 2013, month = 4, day = 7, tz = \"GMT\")\ndt6\n[1] \"2013-04-07 12:00:00 GMT\"\nclass(dt6)\n[1] \"POSIXct\" \"POSIXt\" dt5-dt6\nTime difference of 33.16667 minsweekdays(dt2)\n[1] \"Thursday\"\nquarters(dt2)\n[1] \"Q1\"\nmonths(dt2)\n[1] \"January\""},{"path":"r-data-types.html","id":"missing-values-in-r","chapter":"4 R data types","heading":"4.8 Missing values in R","text":"R missing values denoted NA. NaN also encountered stands \n“number”. often generated one divides instance 0 0.\noperation involving NA results NA. Examples shown belowThe function .na() produces logical indicates value missing.","code":"1 + NA\n[1] NA\nNA - 1\n[1] NA\nNA*2\n[1] NAis.na(2)\n[1] FALSE\nis.na(NA)\n[1] TRUE"},{"path":"data-structures-in-r.html","id":"data-structures-in-r","chapter":"5 Data Structures in R","heading":"5 Data Structures in R","text":"introduced R described variable types research R. Data\nstructures composite various atomic types described \npreceding chapter.Different data structures R include Vector, Matrix, Array, List, Data frame\nTime-series. data needs specific structure perform\nappropriate analysis.","code":""},{"path":"data-structures-in-r.html","id":"vectors","chapter":"5 Data Structures in R","heading":"5.1 Vectors","text":"vector simplest data structure R. made collection \nlike data types mentioned preceding chapter. numerous functions\nR capable creating vectors. c() function generic function \ncombines objects vector first converting data type.\ncreate numeric vector \nlength 10 using c() function.create integer vector sequence can use.Next, use seq() function create sequence numbers C, 0 \n20 intervals 2.vector repeated sequences atomic values can created using rep()\nfunction. repeat “B” ten times form vector length 10, \nelements “B”.R base package two functions can generate vector alphabets \neither lowercase upper cases. areA sequence dates vector can also created.Finally, vector specified length atomic type can created use\nvector() function.Mathematical operations vectors carried element \nvector. instance, dividing vector 2 means every element \nvector divided 2. First, create vector numbers 1 10And divide vector X 2 form YNext, square vector call resulting vector ZNote X divided 2 every member vector halved.\nSimilarly squaring vector meant resulting vector Z elements \nsquares X.","code":"A <- c(3, 2, 4, 5, 6, 2, 3, 1, 7, 9) \nA\n [1] 3 2 4 5 6 2 3 1 7 9B <- 1:10 \nB\n [1]  1  2  3  4  5  6  7  8  9 10C <- seq(from = 0, to = 20, by = 2) \nC\n [1]  0  2  4  6  8 10 12 14 16 18 20D <- rep(\"B\", times = 10) \nD\n [1] \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"letters[1:12] \n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\"\nLETTERS[1:12]\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\"seq(as.POSIXct(\"2003-04-23\"), by = \"month\", length = 12)\n [1] \"2003-04-23 GMT\" \"2003-05-23 GMT\" \"2003-06-23 GMT\"\n [4] \"2003-07-23 GMT\" \"2003-08-23 GMT\" \"2003-09-23 GMT\"\n [7] \"2003-10-23 GMT\" \"2003-11-23 GMT\" \"2003-12-23 GMT\"\n[10] \"2004-01-23 GMT\" \"2004-02-23 GMT\" \"2004-03-23 GMT\"E <- vector(mode = \"integer\", length = 7)\nE\n[1] 0 0 0 0 0 0 0X <- 1:10\nX\n [1]  1  2  3  4  5  6  7  8  9 10Y <- X/2\nY\n [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0Z <- X^2\nZ\n [1]   1   4   9  16  25  36  49  64  81 100"},{"path":"data-structures-in-r.html","id":"matrices","chapter":"5 Data Structures in R","heading":"5.2 Matrices","text":"Matrices vectors arranged two dimensions made rows columns\n(r,c). example 3 x 5 matrix (matrix three rows five\ncolumns)\\[\nC =\\begin{bmatrix}\n1 & 2 & 3 & 4 & 5 \\\\\n3 & 4 & 5 & 6 & 7 \\\\\n5 & 4 & 3 & 4 & 8\n\\end{bmatrix}\n\\]several ways matrices can created R. just one .\nFirst, can create vectors combine row using rbind(). begin\ncreating three vectors naming desired row names \nmatrixAnd bind together form matrixmat4 row names column names. therefore go\nahead name columns matrix using function colnames()","code":"\nRow1 <- c(1, 2, 3, 4, 5)\nRow2 <- c(3, 4, 5, 6, 7)\nRow3 <- c(5, 4, 3, 4, 8)mat4 <- rbind(Row1, Row2, Row3)\nmat4\n     [,1] [,2] [,3] [,4] [,5]\nRow1    1    2    3    4    5\nRow2    3    4    5    6    7\nRow3    5    4    3    4    8colnames(mat4) <- c(\"Col1\", \"Col2\", \"Col3\", \"Col4\", \"Col5\")\nmat4\n     Col1 Col2 Col3 Col4 Col5\nRow1    1    2    3    4    5\nRow2    3    4    5    6    7\nRow3    5    4    3    4    8"},{"path":"data-structures-in-r.html","id":"arrays","chapter":"5 Data Structures in R","heading":"5.3 Arrays","text":"array vector two dimensions. can created assigning dimensions vector using array() function. creation three-dimensional array illustrated .mathematical manipulations applicable vectors also applicable \nmatrices arrays. Matrices, however, can used matrix algebra \nfield mathematics statistics. beyond scope book \ndiscussed .","code":"Y <- array(X, \n           dim = c(2,3,2), \n           dimnames = list(\n               Sex = c(\"M\", \"F\"), \n               Color = c(\"red\", \"blue\", \"green\"), \n               Age=c(\"<30yrs\",\">=30yrs\")\n               )\n           )\nY\n, , Age = <30yrs\n\n   Color\nSex red blue green\n  M   1    3     5\n  F   2    4     6\n\n, , Age = >=30yrs\n\n   Color\nSex red blue green\n  M   7    9     1\n  F   8   10     2\nclass(Y)\n[1] \"array\""},{"path":"data-structures-in-r.html","id":"data-frame","chapter":"5 Data Structures in R","heading":"5.4 Data frame","text":"data frame important object R. ’s used numerous\nstatistical manipulations. Compared matrices arrays columns\nclass, data frames can various columns \ndifferent classes. standard statistical datasets manipulated R \ndata frames. example data frame can created R using function\ndata.frame(). first create four different vectors sex, age,\ncolour old.Next, combine four vectors data frameNext, check structure data frame function str().output indicates 4 variables 10 records. Also, describes\nclasses various variables, giving examples per variable.","code":"\nsex <- gl(n = 2, k = 5, label = c(\"Male\",\"Female\")) \nage <- c(5, 2, 5, 6, 5, 6, 7, 8, 7, 7) \ncolor <- rep(c(\"Red\", \"Blue\"), times=5) \nold <- age > 6df1 <- data.frame(sex, age, color, old) \ndf1\n      sex age color   old\n1    Male   5   Red FALSE\n2    Male   2  Blue FALSE\n3    Male   5   Red FALSE\n4    Male   6  Blue FALSE\n5    Male   5   Red FALSE\n6  Female   6  Blue FALSE\n7  Female   7   Red  TRUE\n8  Female   8  Blue  TRUE\n9  Female   7   Red  TRUE\n10 Female   7  Blue  TRUEstr(df1)\n'data.frame':   10 obs. of  4 variables:\n $ sex  : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 1 1 2 2 2 2 2\n $ age  : num  5 2 5 6 5 6 7 8 7 7\n $ color: chr  \"Red\" \"Blue\" \"Red\" \"Blue\" ...\n $ old  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ..."},{"path":"data-structures-in-r.html","id":"list","chapter":"5 Data Structures in R","heading":"5.5 List","text":"R, list set different elements objects put together. \nsimilar data frame components can made objects \nvector. include matrices, arrays, data frames etc. create \nlist called list1 made three elements using list() function.\nfirst data frame called DF. second numeric vector length\n5 called Vec last character called Color.","code":"list1 <- list(DF = df1,  Vec =1 :5, Color = \"Red\")\nlist1\n$DF\n      sex age color   old\n1    Male   5   Red FALSE\n2    Male   2  Blue FALSE\n3    Male   5   Red FALSE\n4    Male   6  Blue FALSE\n5    Male   5   Red FALSE\n6  Female   6  Blue FALSE\n7  Female   7   Red  TRUE\n8  Female   8  Blue  TRUE\n9  Female   7   Red  TRUE\n10 Female   7  Blue  TRUE\n\n$Vec\n[1] 1 2 3 4 5\n\n$Color\n[1] \"Red\""},{"path":"data-structures-in-r.html","id":"tables","chapter":"5 Data Structures in R","heading":"5.6 Tables","text":"Tables objects display frequencies. use popular \nstatistical mathematical literature. Tables can constructed many ways\nR using function table(). can also created objects\narrays matrices using .table(). simplest form table \ncount frequency occurrence elements vector.outpatient clinic hospital Accra, Ghana, 8 patients randomly\nselected sex noted. recorded vector Gender\n.determine frequencies different sexes tabulate vector\nyielding table object, table.1.result indicates five males three females selected\npatients. function .table() determines object table. Though\nknow table.1 table test belowContingency tables, cross-tabulation two variables vectors \ncommon research. vectors represent data randomly selected\n8 pupils basic school Ghana. Four pens randomly given \npupils. vector Sex recording sex Pen indicates \npupil given pen . create two vectors.Next, cross-tabulate using table() function.output indicates three females two pens.\nAlso, five males two pens given .","code":"Sex <- c(\"Male\",  \"Female\",  \"Female\",  \"Male\",  \"Male\", \"Female\", \"Male\", \"Male\")\nSex\n[1] \"Male\"   \"Female\" \"Female\" \"Male\"   \"Male\"   \"Female\"\n[7] \"Male\"   \"Male\"  table.1 <- table(Sex)\ntable.1\nSex\nFemale   Male \n     3      5 is.table(table.1)\n[1] TRUE\nSex<-c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\")\nPen<-c(\"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"No\")table(Sex, Pen)\n        Pen\nSex      No Yes\n  Female  1   2\n  Male    3   2\nPen\n[1] \"No\"  \"Yes\" \"Yes\" \"Yes\" \"Yes\" \"No\"  \"No\"  \"No\" "},{"path":"importing-data-into-r.html","id":"importing-data-into-r","chapter":"6 Importing data into R","heading":"6 Importing data into R","text":"chapter, discuss ways getting data R, either directly entering R importing another software. R, data frame data structure desirable data analysis. advent tidy data, tibble now predominant data structure used. section, reading various file formats presenting tibble.","code":""},{"path":"importing-data-into-r.html","id":"using-data-in-r-packages","chapter":"6 Importing data into R","heading":"6.1 Using data in R packages","text":"Many packages R come data can used practice. able use dataset specific package, package first installed. instance, able use data Oswego native package epiDisplay, first ensure package installed running command line :Note install package packages epiDisplay package depends .next step make data available R session belowNow data available working environment, can visualise first 6 rows belowTable 6.1:  finally visualise details data using help() function ","code":"\ninstall.packages(\"epiDisplay\")\ndata(\"Oswego\", package = \"epiDisplay\")\nOswego %>% head()\nhelp(Oswego)"},{"path":"importing-data-into-r.html","id":"direct-entry-into-r","chapter":"6 Importing data into R","heading":"6.2 Direct entry into R","text":"first use data.frame() function base package create data frame.Table 6.2:  first describe manually enter data R. aim create tibble using tibble function.Table 6.3:  ","code":"\ndata.frame(\n    name = c(\"Ama\", \"Yakubu\", \"John\"), \n    sex = c(\"Female\", \"Male\", \"Male\"),\n    age = c(12, 9, 4),\n    school = c(\"JHS\", \"Primary\", \"Creche\")\n    )\ntibble(\n    name = c(\"Ama\", \"Yakubu\", \"John\"), \n    sex = c(\"Female\", \"Male\", \"Male\"),\n    age = c(12, 9, 4),\n    school = c(\"JHS\", \"Primary\", \"Creche\")\n    )"},{"path":"importing-data-into-r.html","id":"r-data-file","chapter":"6 Importing data into R","heading":"6.3 R data file","text":"working R, frequent mode storage data .Rdata file. preserves structure environment data. read already saved .Rdata file.visualise first 4 rows single data within loaded file called data1_stataTable 6.4:  ","code":"load(file = \"./Data/data1.Rdata\")\nls()\n[1] \"data1_stata\" \"mytheme\"     \"Oswego\"     \ndata1_stata %>% head(n=4)"},{"path":"importing-data-into-r.html","id":"text-files","chapter":"6 Importing data into R","heading":"6.4 Text files","text":"first file format going read flat file text file. usually extension .txt. data files separated various delimiters. include tabs, commas, spaces, etc. section, read one tab delimiter prototype rest similar.Table 6.5:  last file read subsection comma-delimited text fileTable 6.6:  Comma-delimited files extension .csv can also imported commnandsTable 6.7:  ","code":"\nread_delim(\n    file = \"./Data/bpA.txt\", \n    delim = \"\\t\", \n    col_types = c(\"c\", \"c\", \"d\", \"d\")\n    )\nread_delim(\n    file = \"./Data/blood.txt\", \n    delim = \",\", \n    col_types = c(\"c\", \"d\", \"d\", \"d\", \"d\", \"d\", \"d\")\n    ) %>% \n    head(n=4)\nread_csv(\n    file = \"./Data/blood.txt\",\n    col_types = c(\"c\", \"d\", \"d\", \"d\", \"d\", \"d\", \"d\")) %>% \n    head(n=4)"},{"path":"importing-data-into-r.html","id":"microsoft-excel","chapter":"6 Importing data into R","heading":"6.5 Microsoft Excel","text":"Probably common format transferring data Microsoft Excel. two versions Excel extensions .xls .xlsx. reading .xlsx demonstrated using readxl package.Table 6.8:  ","code":"\nreadxl::read_xlsx(path = \"./Data/data1.xlsx\") %>% \n    head(n=4)"},{"path":"importing-data-into-r.html","id":"spss-files","chapter":"6 Importing data into R","heading":"6.6 SPSS files","text":"Files SPSS usually saved extension .sav. read SSPSS data file using haven packageTable 6.9:  ","code":"\nhaven::read_sav(file = \"./Data/data1.sav\") %>% \n    head(n=4)"},{"path":"importing-data-into-r.html","id":"stata-files","chapter":"6 Importing data into R","heading":"6.7 Stata files","text":"Stata files, similar SPSS data files can imported using haven package. illustrated belowTable 6.10:  ","code":"\nhaven::read_dta(file = \"./Data/data1.dta\") %>% \n    head(n=4)"},{"path":"importing-data-into-r.html","id":"sas-files","chapter":"6 Importing data into R","heading":"6.8 SAS files","text":"haven package also offers ability read R SAS data file. illustrated belowTable 6.11:  ","code":"\nhaven::read_sas(data_file = \"./Data/data1.sas7bdat\") %>% \n    head(n=4)"},{"path":"importing-data-into-r.html","id":"conclusion","chapter":"6 Importing data into R","heading":"6.9 Conclusion","text":"section, learned import data R various file formats programs. next section, learn export data R file formats.","code":""},{"path":"data-wrangling.html","id":"data-wrangling","chapter":"7 Data wrangling","heading":"7 Data wrangling","text":"chapter, begin delve manipulation data form \ndata frame tibble. , introduce tidyverse package\nvarious verbs (function) provides.tidyverse package just single package composite group\npackages. include among others dplyr package. function\nemploying chapter comes dplyr.begin reading blood_donors.xlsTable 7.1:  output shows 25-row 6-column tibble.","code":"\ndf_blood <- readxl::read_xls(\"./Data/blood_donors_1.xls\")\ndf_blood"},{"path":"data-wrangling.html","id":"renaming-variables","chapter":"7 Data wrangling","heading":"7.1 Renaming variables","text":"rename variables hb hemog id studyid using \nrename function, show first 5 records head function.Table 7.2:  ","code":"\ndf_blood %>% \n    rename(hemog = hb, studyid = id) %>% \n    head(5)"},{"path":"data-wrangling.html","id":"sorting-data","chapter":"7 Data wrangling","heading":"7.2 Sorting data","text":"use arrange function sort bldgrp ascending order \nhb descending order.Table 6.1:  ","code":"\ndf_blood %>% \n    arrange(bldgrp, desc(hb))"},{"path":"data-wrangling.html","id":"subsetting-data","chapter":"7 Data wrangling","heading":"7.3 Subsetting data","text":"subsection, demonstrate use filter select function\nselect specific records variables tibble. filter select\nrecords hb > 12g/dl keep id, hb sex\ncolumns.Table 7.3:  ","code":"\ndf_blood %>% \n    filter(hb > 12) %>% \n    select(id, hb, sex)"},{"path":"data-wrangling.html","id":"generating-new-variables","chapter":"7 Data wrangling","heading":"7.4 Generating new variables","text":"generate new variables use mutate function. Based knowledge\nhematocrit approximately three times haemoglobin generate\nnew variable, hb_from_hct.Table 6.2:  ","code":"\ndf_blood %>% \n    mutate(hb_from_hct = hct/3)"},{"path":"data-wrangling.html","id":"aggregating-data","chapter":"7 Data wrangling","heading":"7.5 Aggregating data","text":"Data can aggregated R using summarize function. determine\nmean standard deviation haemoglobin patient data.Table 6.3:  Grouping data “bldgrp” aggregation yields aggregated\nmeans standard deviations various blood groups.Table 7.4:  ","code":"\ndf_blood %>% \n    summarize(mean_hb = mean(hb), sd_hb = sd(hb))\ndf_blood %>% \n    group_by(bldgrp) %>% \n    summarize(mean_hb = mean(hb), sd_hb = sd(hb))"},{"path":"data-wrangling.html","id":"reshaping-data","chapter":"7 Data wrangling","heading":"7.6 Reshaping data","text":"longitudinal studies, data captured individual repeatedly.\ndata recorded either long wide formats. typical example \ndata frame long form bpB .Table 6.4:  format, visit round data taking captured new row, \nappropriate study ID period record, captured variable\nmeasure . Measurement systolic blood pressure day 1 indicated \nsbp1 measure variable. Day 2 measurements indicated sbp2.wide format data can obtained .Table 6.5:  , study participant’s record whole study one row data different measurements systolic blood pressure captured different variables. Next, convert wide back long format.Table 6.6:  ","code":"\nbp_long <- read_csv(\n    file = \"./Data/bp_long.txt\",\n    col_names = TRUE, \n    col_types = c(\"c\", \"c\", \"i\")\n    )\n\nbp_long\nbp_wide <- \n    bp_long %>% \n    pivot_wider(\n        id_cols = id, \n        names_from = measure, \n        values_from = sbp\n    )\n\nbp_wide\nbp_wide %>% \n    pivot_longer(\n        cols = c(sbp1, sbp2),\n        names_to = \"time\",\n        values_to = \"syst_bp\"\n    )"},{"path":"data-wrangling.html","id":"combining-data","chapter":"7 Data wrangling","heading":"7.7 Combining data","text":"study determine change weight athletes running marathon,\ndata athletes obtained investigators. Since marathon\nstarts town ends town B, investigators decided weigh \nathletes just starting race. took records ID \nathlete’s sid, sex, age weight start (wgtst). records five \nathletes data marathonA. end point marathon,\nanother member investigation team recorded IDs (eid), weight upon\ncompletion (wgtend) time took athletes complete marathon\n(dura).Table 6.7:  Table 6.7:  can determine weight change matching \nweight individual. merging useful. , \nmerge two data one. done .Table 6.8:  ","code":"\ndataA <- \n    read_delim(\n        file = \"./Data/marathonA.txt\",\n        col_names = TRUE,\n        delim = \"\\t\",\n        col_types = c(\"c\",\"c\",\"i\",\"d\")\n    )\n\ndataB <- \n    read_delim(\n        file = \"./Data/marathonB.txt\",\n        col_names = TRUE,\n        delim = \"\\t\",\n        col_types = c(\"c\",\"c\",\"i\",\"d\")\n    )\n\ndataA\n\ndataB\ndataA %>% \n    full_join(dataB, by = join_by(sid==eid))"},{"path":"data-cleaning.html","id":"data-cleaning","chapter":"8 Data Cleaning","heading":"8 Data Cleaning","text":"Data analysed ”cleaned” first abnormal invalid values. \ndone understanding data hand, collected\nfirst place little prejudice bias. critical\nstage analysis arbitrary deletion insertion data \nsignificantly alter conclusions., therefore, goes without saying modifications done data\ncleaning stage must sound statistical, clinical well commonsensical\nreasons . Also, whole process data cleaning well\ndocumented appropriately stored future reference. regard, \ngood practice edit data software Microsoft Excel though\nmay appear easy tempting. software keep \naudit trail.","code":""},{"path":"data-cleaning.html","id":"data-dictionary-or-codebook","chapter":"8 Data Cleaning","heading":"8.1 Data dictionary or codebook","text":"well-collected managed data, always dictionary. \ndictionary outlines every variable dataset variable name, \nmeaning variable, source variable (questionnaire, data\ncollection sheet, etc.), valid ranges codes format. \ninvaluable tool determining wrong abnormal entries. also sometimes\nreferred codebook.typical example one . data dictionary \nblood_donors_3.dta file going use.","code":""},{"path":"data-cleaning.html","id":"importing-the-data-into-r","chapter":"8 Data Cleaning","heading":"8.2 Importing the data into R","text":"first step analysis read import data data analysis\nsoftware general overview can obtained. begin importing \nblood_donors_3.dta R calling blood3.","code":"\nblood3 <- readxl::read_xls(\"./Data/blood_donors_2.xls\")"},{"path":"data-cleaning.html","id":"visualising-the-data-in-r","chapter":"8 Data Cleaning","heading":"8.3 Visualising the data in R","text":"Next, visualize dataTo visualise data can use print() View() functions. Note\nmight best relatively big data. \nuse .data.frame() function display whole data.","code":"blood3\n# A tibble: 25 × 6\n      id    hb   hct   sex bldgrp pdonor\n   <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>\n 1     1 10.5   31.8     1      3      3\n 2     2 11.9   37.2     1      4      0\n 3     3  1     26       1      1      1\n 4     4  8.90  26.8     1      1      3\n 5     5  7.80  24.2     1      1      2\n 6     6 10     30.9     1      2      1\n 7     7 10.4   33.9     1      2      0\n 8     8 11.3   35       1      3      1\n 9     9 16.4   NA       1      4      1\n10    10 14.4   43.6     1      4      1\n# ℹ 15 more rowsblood3 %>% as.data.frame()\n   id   hb  hct sex bldgrp pdonor\n1   1 10.5 31.8   1      3      3\n2   2 11.9 37.2   1      4      0\n3   3  1.0 26.0   1      1      1\n4   4  8.9 26.8   1      1      3\n5   5  7.8 24.2   1      1      2\n6   6 10.0 30.9   1      2      1\n7   7 10.4 33.9   1      2      0\n8   8 11.3 35.0   1      3      1\n9   9 16.4   NA   1      4      1\n10 10 14.4 43.6   1      4      1\n11 11 11.2 33.2   0      3     99\n12 12 11.5 35.5   0      3      1\n13 13 10.5 33.7   0      3      2\n14 14 12.2 36.8   0      4      1\n15 14 16.4 48.8   0      5      2\n16 16 12.7 99.0   0      4      0\n17 17  9.8 30.5   0      1      4\n18 18 10.9 33.8   0      3      0\n19 19 11.6 35.4   0      3      3\n20 20 10.6 34.9   0      9      2\n21 21  9.1 28.0   9      1      3\n22 22 11.9 36.1   9      4      3\n23 23 10.5 34.2   9      3      2\n24 24 12.3 38.2   9      4      2\n25 25 11.0 35.7   9      3      2"},{"path":"data-cleaning.html","id":"describing-or-summarizing-the-data","chapter":"8 Data Cleaning","heading":"8.4 Describing or summarizing the data","text":"first use glimpse() function basic view variable names variable typesWe can also use dfSummary() function summarytools package \ngive comprehensive output variable.","code":"blood3 %>% glimpse()\nRows: 25\nColumns: 6\n$ id     <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, …\n$ hb     <dbl> 10.5, 11.9, 1.0, 8.9, 7.8, 10.0, 10.4, 11.3…\n$ hct    <dbl> 31.8, 37.2, 26.0, 26.8, 24.2, 30.9, 33.9, 3…\n$ sex    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0…\n$ bldgrp <dbl> 3, 4, 1, 1, 1, 2, 2, 3, 4, 4, 3, 3, 3, 4, 5…\n$ pdonor <dbl> 3, 0, 1, 3, 2, 1, 0, 1, 1, 1, 99, 1, 2, 1, …blood3 %>% summarytools::dfSummary()\nData Frame Summary  \nblood3  \nDimensions: 25 x 6  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------------------------\nNo   Variable    Stats / Values            Freqs (% of Valid)   Graph               Valid      Missing  \n---- ----------- ------------------------- -------------------- ------------------- ---------- ---------\n1    id          Mean (sd) : 13 (7.4)      24 distinct values   : : : : :           25         0        \n     [numeric]   min < med < max:                               : : : : :           (100.0%)   (0.0%)   \n                 1 < 13 < 25                                    : : : : :                               \n                 IQR (CV) : 12 (0.6)                            : : : : :                               \n                                                                : : : : :                               \n\n2    hb          Mean (sd) : 11 (2.9)      21 distinct values             :         25         0        \n     [numeric]   min < med < max:                                         :         (100.0%)   (0.0%)   \n                 1 < 11 < 16.4                                            :                             \n                 IQR (CV) : 1.5 (0.3)                                   . :                             \n                                                                .     . : : : . :                       \n\n3    hct         Mean (sd) : 36.8 (14.3)   24 distinct values     :                 24         1        \n     [numeric]   min < med < max:                                 :                 (96.0%)    (4.0%)   \n                 24.2 < 34.6 < 99                                 :                                     \n                 IQR (CV) : 4.7 (0.4)                             :                                     \n                                                                : : .         .                         \n\n4    sex         Mean (sd) : 2.2 (3.5)     0 : 10 (40.0%)       IIIIIIII            25         0        \n     [numeric]   min < med < max:          1 : 10 (40.0%)       IIIIIIII            (100.0%)   (0.0%)   \n                 0 < 1 < 9                 9 :  5 (20.0%)       IIII                                    \n                 IQR (CV) : 1 (1.6)                                                                     \n\n5    bldgrp      Mean (sd) : 3.1 (1.7)     1 : 5 (20.0%)        IIII                25         0        \n     [numeric]   min < med < max:          2 : 2 ( 8.0%)        I                   (100.0%)   (0.0%)   \n                 1 < 3 < 9                 3 : 9 (36.0%)        IIIIIII                                 \n                 IQR (CV) : 2 (0.5)        4 : 7 (28.0%)        IIIII                                   \n                                           5 : 1 ( 4.0%)                                                \n                                           9 : 1 ( 4.0%)                                                \n\n6    pdonor      Mean (sd) : 5.6 (19.5)    0 : 4 (16.0%)        III                 25         0        \n     [numeric]   min < med < max:          1 : 7 (28.0%)        IIIII               (100.0%)   (0.0%)   \n                 0 < 2 < 99                2 : 7 (28.0%)        IIIII                                   \n                 IQR (CV) : 2 (3.5)        3 : 5 (20.0%)        IIII                                    \n                                           4 : 1 ( 4.0%)                                                \n                                           99 : 1 ( 4.0%)                                               \n--------------------------------------------------------------------------------------------------------"},{"path":"data-cleaning.html","id":"cleaning-individual-variables","chapter":"8 Data Cleaning","heading":"8.5 Cleaning individual variables","text":"note variables type “double”. sex bldgrp\nhowever, factors. done subsequently summarized .","code":"blood3 <-  \n    blood3 %>% \n    mutate(sex = factor(sex, \n                        levels = c(0,1,9),\n                        labels = c(\"Female\", \"Male\", \"Missing\")),\n           bldgrp= factor(bldgrp, \n                          levels = c(1, 2, 3, 4, 9),\n                          labels = c(\"A\", \"B\", \"O\", \"AB\", \"Missing\"))) \n\nblood3 %>% summarytools::dfSummary()\nData Frame Summary  \nblood3  \nDimensions: 25 x 6  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------------------------\nNo   Variable    Stats / Values            Freqs (% of Valid)   Graph               Valid      Missing  \n---- ----------- ------------------------- -------------------- ------------------- ---------- ---------\n1    id          Mean (sd) : 13 (7.4)      24 distinct values   : : : : :           25         0        \n     [numeric]   min < med < max:                               : : : : :           (100.0%)   (0.0%)   \n                 1 < 13 < 25                                    : : : : :                               \n                 IQR (CV) : 12 (0.6)                            : : : : :                               \n                                                                : : : : :                               \n\n2    hb          Mean (sd) : 11 (2.9)      21 distinct values             :         25         0        \n     [numeric]   min < med < max:                                         :         (100.0%)   (0.0%)   \n                 1 < 11 < 16.4                                            :                             \n                 IQR (CV) : 1.5 (0.3)                                   . :                             \n                                                                .     . : : : . :                       \n\n3    hct         Mean (sd) : 36.8 (14.3)   24 distinct values     :                 24         1        \n     [numeric]   min < med < max:                                 :                 (96.0%)    (4.0%)   \n                 24.2 < 34.6 < 99                                 :                                     \n                 IQR (CV) : 4.7 (0.4)                             :                                     \n                                                                : : .         .                         \n\n4    sex         1. Female                 10 (40.0%)           IIIIIIII            25         0        \n     [factor]    2. Male                   10 (40.0%)           IIIIIIII            (100.0%)   (0.0%)   \n                 3. Missing                 5 (20.0%)           IIII                                    \n\n5    bldgrp      1. A                      5 (20.8%)            IIII                24         1        \n     [factor]    2. B                      2 ( 8.3%)            I                   (96.0%)    (4.0%)   \n                 3. O                      9 (37.5%)            IIIIIII                                 \n                 4. AB                     7 (29.2%)            IIIII                                   \n                 5. Missing                1 ( 4.2%)                                                    \n\n6    pdonor      Mean (sd) : 5.6 (19.5)    0 : 4 (16.0%)        III                 25         0        \n     [numeric]   min < med < max:          1 : 7 (28.0%)        IIIII               (100.0%)   (0.0%)   \n                 0 < 2 < 99                2 : 7 (28.0%)        IIIII                                   \n                 IQR (CV) : 2 (3.5)        3 : 5 (20.0%)        IIII                                    \n                                           4 : 1 ( 4.0%)                                                \n                                           99 : 1 ( 4.0%)                                               \n--------------------------------------------------------------------------------------------------------"},{"path":"data-cleaning.html","id":"checking-for-duplicated-records","chapter":"8 Data Cleaning","heading":"8.6 Checking for duplicated records","text":"begin official data cleaning checking duplicate records \ndata","code":"blood3 %>% janitor::get_dupes()\nNo variable names specified - using all columns.\nNo duplicate combinations found of: id, hb, hct, sex, bldgrp, pdonor\n# A tibble: 0 × 7\n# ℹ 7 variables: id <dbl>, hb <dbl>, hct <dbl>, sex <fct>,\n#   bldgrp <fct>, pdonor <dbl>, dupe_count <int>"},{"path":"data-cleaning.html","id":"cleaning-individual-variables-1","chapter":"8 Data Cleaning","heading":"8.7 Cleaning individual variables","text":"Next, begin sort variables one one. begin study id\nvariable. begin looking duplicated study ids.Study id 14 duplicated! Next, visually inspect study idsIt looks like study ids numeric order 1 25 14 duplicated 15 \nmissing. solve writing new study id variable. Afterwards, check see \nduplicates.Next, inspect hb variable summary boxplot. observe \nsummary none haemoglobin observations missing. boxplot \nhb shown . observe 4 outliers one looks \nextreme.convert observation missing .redraw boxplot without outlier.Next, focus hct variable. note 99 represents ‘missing’. \ntherefore remove belowAnd draw boxplot belowBecause know hematocrit relationship haemoglobin, use scatter plot \nvisualise possibly pick suspicious data.Next, inspect sex variableWe convert “Missing” category NAAnd checkNext, sort bldgrp variableWe convert Missing NA visualize variableNext, sort pdonor","code":"blood3 %>% janitor::get_dupes(id)\n# A tibble: 2 × 7\n     id dupe_count    hb   hct sex    bldgrp pdonor\n  <dbl>      <int> <dbl> <dbl> <fct>  <fct>   <dbl>\n1    14          2  12.2  36.8 Female AB          1\n2    14          2  16.4  48.8 Female <NA>        2blood3$id\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 14 16 17 18\n[19] 19 20 21 22 23 24 25blood3 <- \n    blood3 %>% \n    mutate(id = 1:25) \n\nblood3 %>% janitor::get_dupes(id)\nNo duplicate combinations found of: id\n# A tibble: 0 × 7\n# ℹ 7 variables: id <int>, dupe_count <int>, hb <dbl>,\n#   hct <dbl>, sex <fct>, bldgrp <fct>, pdonor <dbl>blood3 %$% summary(hb)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   10.40   11.00   10.99   11.90   16.40 \nA <-\n    blood3 %>% \n    ggplot(aes(y = hb)) +\n    geom_boxplot(fill = \"grey\") +\n    labs(y = \"Hemoglobin (mg/dl)\",\n         title = \"Boxplot of hemoglobin of participants \n         with outliers\") +\n    theme_bw()\nblood3 <- \n    blood3 %>% \n    mutate(hb = ifelse(hb < 4, NA, hb))B <-\n    blood3 %>% \n    ggplot(aes(y = hb)) +\n    geom_boxplot(fill = \"grey\") +\n    labs(y = \"Hemoglobin (mg/dl)\",\n         title = \"Boxplot of hemoglobin of participants \n         after outlier removed\") +\n    theme_bw()\n\nA + B + plot_annotation(tag_levels = 'A')\nWarning: Removed 1 rows containing non-finite values\n(`stat_boxplot()`).\nblood3 %>% \n    drop_na() %>% \n    ggplot(aes(y = hb)) +\n    geom_boxplot(fill =\"grey\") +\n    labs(y = \"Hemoglobin (mg/dl)\",\n         title = \"Boxplot of hemoglobin of participants\") +\n    theme_bw()\nblood3 <- \n    blood3 %>% \n    mutate(hct = ifelse(hct >90, NA, hct))\nblood3 %>% \n    drop_na(hct) %>% \n    ggplot(aes(y = hct)) + \n    geom_boxplot(fill = \"grey\")+ \n    labs(y = \"Hematocrit (%)\",\n         title = \"Boxplot of hematocrit of participants\") +\n    theme_bw()\nblood3 %>% \n    drop_na(hb, hct) %>% \n    ggplot(aes(x = hct, y = hb)) + \n    geom_point(col = \"red\") + \n    labs(x = \"Hematocrit (%)\",\n         y = \"Hemoglobin (mg/dl)\",\n         title = \"Scatterplot showing the relationship \n         between the hematocrit and hemoglobin\")+\n    theme_bw()blood3 %>% \n    count(sex)\n# A tibble: 3 × 2\n  sex         n\n  <fct>   <int>\n1 Female     10\n2 Male       10\n3 Missing     5\nblood3 <- \n    blood3 %>% \n    mutate(sex = fct_recode(sex, NULL = \"Missing\"))blood3 %>% \n    count(sex)\n# A tibble: 3 × 2\n  sex        n\n  <fct>  <int>\n1 Female    10\n2 Male      10\n3 <NA>       5blood3 %>% \n    count(bldgrp)\n# A tibble: 6 × 2\n  bldgrp      n\n  <fct>   <int>\n1 A           5\n2 B           2\n3 O           9\n4 AB          7\n5 Missing     1\n6 <NA>        1blood3 <-\n    blood3 %>% \n    mutate(bldgrp = fct_recode(bldgrp, NULL = \"Missing\"))\n\nblood3 %>% count(bldgrp)\n# A tibble: 5 × 2\n  bldgrp     n\n  <fct>  <int>\n1 A          5\n2 B          2\n3 O          9\n4 AB         7\n5 <NA>       2\nblood3 <- \n    blood3 %>% \n    mutate(pdonor = ifelse(pdonor == 99, NA, pdonor))"},{"path":"data-cleaning.html","id":"visualising-the-cleaned-data","chapter":"8 Data Cleaning","heading":"8.8 Visualising the cleaned data","text":"Finally, summarize data ","code":"blood3 %>% \n    summarytools::dfSummary()\nData Frame Summary  \nblood3  \nDimensions: 25 x 6  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------------------\nNo   Variable    Stats / Values           Freqs (% of Valid)   Graph          Valid      Missing  \n---- ----------- ------------------------ -------------------- -------------- ---------- ---------\n1    id          Mean (sd) : 13 (7.4)     25 distinct values   : : : : :      25         0        \n     [integer]   min < med < max:         (Integer sequence)   : : : : :      (100.0%)   (0.0%)   \n                 1 < 13 < 25                                   : : : : :                          \n                 IQR (CV) : 12 (0.6)                           : : : : :                          \n                                                               : : : : :                          \n\n2    hb          Mean (sd) : 11.4 (2)     20 distinct values       :          24         1        \n     [numeric]   min < med < max:                                  :          (96.0%)    (4.0%)   \n                 7.8 < 11.1 < 16.4                                 :                              \n                 IQR (CV) : 1.5 (0.2)                            . :                              \n                                                               . : : : . :                        \n\n3    hct         Mean (sd) : 34.1 (5.4)   23 distinct values       :          23         2        \n     [numeric]   min < med < max:                                  : .        (92.0%)    (8.0%)   \n                 24.2 < 34.2 < 48.8                                : :                            \n                 IQR (CV) : 4.6 (0.2)                            . : :                            \n                                                               . : : : . .                        \n\n4    sex         1. Female                10 (50.0%)           IIIIIIIIII     20         5        \n     [factor]    2. Male                  10 (50.0%)           IIIIIIIIII     (80.0%)    (20.0%)  \n\n5    bldgrp      1. A                     5 (21.7%)            IIII           23         2        \n     [factor]    2. B                     2 ( 8.7%)            I              (92.0%)    (8.0%)   \n                 3. O                     9 (39.1%)            IIIIIII                            \n                 4. AB                    7 (30.4%)            IIIIII                             \n\n6    pdonor      Mean (sd) : 1.7 (1.1)    0 : 4 (16.7%)        III            24         1        \n     [numeric]   min < med < max:         1 : 7 (29.2%)        IIIII          (96.0%)    (4.0%)   \n                 0 < 2 < 4                2 : 7 (29.2%)        IIIII                              \n                 IQR (CV) : 1.2 (0.7)     3 : 5 (20.8%)        IIII                               \n                                          4 : 1 ( 4.2%)                                           \n--------------------------------------------------------------------------------------------------"},{"path":"data-cleaning.html","id":"generating-new-variables-1","chapter":"8 Data Cleaning","heading":"8.9 Generating new variables","text":"Often cleaning individual variables data analyst required \ngenerate new variables old ones. put practice \ngenerating presence Anemia hb less 11g/dl.","code":"blood3 <-\n    blood3 %>% \n    mutate(anemia = case_when(hb < 11 ~ \"Yes\", hb >= 11 ~ \"No\") %>% factor())\n\nsummarytools::dfSummary(blood3)\nData Frame Summary  \nblood3  \nDimensions: 25 x 7  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------------------\nNo   Variable    Stats / Values           Freqs (% of Valid)   Graph          Valid      Missing  \n---- ----------- ------------------------ -------------------- -------------- ---------- ---------\n1    id          Mean (sd) : 13 (7.4)     25 distinct values   : : : : :      25         0        \n     [integer]   min < med < max:         (Integer sequence)   : : : : :      (100.0%)   (0.0%)   \n                 1 < 13 < 25                                   : : : : :                          \n                 IQR (CV) : 12 (0.6)                           : : : : :                          \n                                                               : : : : :                          \n\n2    hb          Mean (sd) : 11.4 (2)     20 distinct values       :          24         1        \n     [numeric]   min < med < max:                                  :          (96.0%)    (4.0%)   \n                 7.8 < 11.1 < 16.4                                 :                              \n                 IQR (CV) : 1.5 (0.2)                            . :                              \n                                                               . : : : . :                        \n\n3    hct         Mean (sd) : 34.1 (5.4)   23 distinct values       :          23         2        \n     [numeric]   min < med < max:                                  : .        (92.0%)    (8.0%)   \n                 24.2 < 34.2 < 48.8                                : :                            \n                 IQR (CV) : 4.6 (0.2)                            . : :                            \n                                                               . : : : . .                        \n\n4    sex         1. Female                10 (50.0%)           IIIIIIIIII     20         5        \n     [factor]    2. Male                  10 (50.0%)           IIIIIIIIII     (80.0%)    (20.0%)  \n\n5    bldgrp      1. A                     5 (21.7%)            IIII           23         2        \n     [factor]    2. B                     2 ( 8.7%)            I              (92.0%)    (8.0%)   \n                 3. O                     9 (39.1%)            IIIIIII                            \n                 4. AB                    7 (30.4%)            IIIIII                             \n\n6    pdonor      Mean (sd) : 1.7 (1.1)    0 : 4 (16.7%)        III            24         1        \n     [numeric]   min < med < max:         1 : 7 (29.2%)        IIIII          (96.0%)    (4.0%)   \n                 0 < 2 < 4                2 : 7 (29.2%)        IIIII                              \n                 IQR (CV) : 1.2 (0.7)     3 : 5 (20.8%)        IIII                               \n                                          4 : 1 ( 4.2%)                                           \n\n7    anemia      1. No                    13 (54.2%)           IIIIIIIIII     24         1        \n     [factor]    2. Yes                   11 (45.8%)           IIIIIIIII      (96.0%)    (4.0%)   \n--------------------------------------------------------------------------------------------------"},{"path":"descriptive-statistics-categorical.html","id":"descriptive-statistics-categorical","chapter":"9 Descriptive Statistics: Categorical","heading":"9 Descriptive Statistics: Categorical","text":"section use Titanic data setWe visualize first 6 rows dataTable 6.1:  summarize entire ","code":"\ntitanic2 <-  \n    haven::read_dta(\"./Data/titanic2.dta\") %>% \n    mutate(sex  = haven::as_factor(sex),\n           died = haven::as_factor(died),\n           age  = haven::as_factor(age),\n           class = haven::as_factor(class)) %>% \n    haven::zap_labels()\ntitanic2 %>% head()titanic2 %>% summary()\n    class        age           sex        died     \n first :325   child: 109   female: 470   No : 711  \n second:285   adult:2092   male  :1731   Yes:1490  \n third :706                                        \n crew  :885                                        "},{"path":"descriptive-statistics-categorical.html","id":"single-categorical-variable","chapter":"9 Descriptive Statistics: Categorical","heading":"9.0.1 Single Categorical Variable","text":"","code":""},{"path":"descriptive-statistics-categorical.html","id":"frequencies-proportions","chapter":"9 Descriptive Statistics: Categorical","heading":"9.0.1.1 Frequencies & Proportions","text":"","code":"\ntitanic2 %>% \n    gtsummary::tbl_summary(\n        include = class,\n        digits = class ~ c(0,1)\n    ) %>% \n    gtsummary::bold_labels()"},{"path":"descriptive-statistics-categorical.html","id":"graph---barchart","chapter":"9 Descriptive Statistics: Categorical","heading":"9.0.1.2 Graph - Barchart","text":"first summarize dataTable 6.3:  plot barplot","code":"\nbar_data <-\n    titanic2 %>% \n    drop_na(class) %>% \n    count(class) %>% \n    mutate(perc = `n` / sum(`n`)) %>% \n    arrange(perc) %>%\n    mutate(labels = paste(n, \" (\", scales::percent(perc), \")\", sep=\"\"))\n\nbar_data\nbar_data %>% \n    ggplot() +\n    geom_bar(stat = \"identity\", \n             aes(y = n, x = class, fill = class), \n             col = \"black\", \n             show.legend = F) +\n    geom_label(aes(y = n, label = labels, x = class), \n               vjust = 1.2,\n               show.legend = FALSE, size=3.5) +\n    labs(x = NULL, \n         y = \"Count\", \n         title = \"Distribution of Class of passenger\") +\n    theme_bw()"},{"path":"descriptive-statistics-categorical.html","id":"pie-chart","chapter":"9 Descriptive Statistics: Categorical","heading":"9.0.1.3 Pie Chart","text":"use previously summarized data. draw customised Pie Chart","code":"\nbar_data %>% \n    ggplot(aes(x = \"\", y = perc, fill = class)) +\n    geom_col() +\n    geom_label(aes(label = labels),\n               position = position_stack(vjust = 0.5),\n               show.legend = FALSE, size =3) +\n    coord_polar(theta = \"y\", start=0) +\n    labs(title = \"Distribution of Blood Groups of study participants\",\n         fill = \"Blood Group\") +\n    theme_void()"},{"path":"descriptive-statistics-categorical.html","id":"two-categorical-variables","chapter":"9 Descriptive Statistics: Categorical","heading":"9.0.2 Two categorical Variables","text":"","code":""},{"path":"descriptive-statistics-categorical.html","id":"frequencies-proportions-1","chapter":"9 Descriptive Statistics: Categorical","heading":"9.0.2.1 Frequencies & Proportions","text":"","code":"\ntitanic2 %>% \n    tbl_cross(row = sex, col = died) %>% \n    bold_labels()"},{"path":"descriptive-statistics-categorical.html","id":"row-percentages","chapter":"9 Descriptive Statistics: Categorical","heading":"9.0.2.2 Row percentages","text":"","code":"\ntitanic2 %>% \n    tbl_cross(row = sex, col = died, percent = \"row\", digits = c(0,1)) %>% \n    bold_labels()"},{"path":"descriptive-statistics-categorical.html","id":"column-percentages","chapter":"9 Descriptive Statistics: Categorical","heading":"9.0.2.3 Column percentages","text":"","code":"\ntitanic2 %>% \n    tbl_cross(row = sex, col = died, percent = \"col\", digits = c(0,1)) %>% \n    bold_labels()"},{"path":"descriptive-statistics-categorical.html","id":"table-total-percentages","chapter":"9 Descriptive Statistics: Categorical","heading":"9.0.2.4 Table Total Percentages","text":"","code":"\ntitanic2 %>% \n    tbl_cross(\n        row = sex, \n        col = died, \n        percent = c(\"cell\"), \n        digits = c(0,1)) %>% \n    bold_labels()"},{"path":"descriptive-statistics-categorical.html","id":"bar-chart","chapter":"9 Descriptive Statistics: Categorical","heading":"9.0.2.5 Bar Chart","text":"","code":"\ntitanic2 %>% \n    ggplot(aes(x = class, fill = died)) +\n    geom_bar(position = position_dodge(), col = \"black\") +\n    labs(y = \"Count\", x = \"Class\", fill = \"Died\",\n          title = \"Bar plot of outcome of passengers for each class\") +\n    theme_bw()"},{"path":"descriptive-statistics-numeric.html","id":"descriptive-statistics-numeric","chapter":"10 Descriptive Statistics: Numeric","heading":"10 Descriptive Statistics: Numeric","text":"initial analysis numeric data usually description data hand without making inference population data drawn. gives data analyst general overview data hand, best describe analysis best suits . descriptive analysis numeric data basic determine :Measure Central Tendency: description center data. measures include mean, median mode.Measure Dispersion: measure widespread data . include standard deviation, variance, interquartile range range.section, use NewDrug_clean.dta dataset","code":"newdrug <-  \n    haven::read_dta(\"./Data/NewDrug_clean.dta\") %>% \n    mutate(sex  = haven::as_factor(sex), treat = haven::as_factor(treat)) %>% \n    haven::zap_labels() \n\nnewdrug %>% summary()\n      id                treat         age        sex   \n Length:50          Control:22   Min.   :45.00   F:26  \n Class :character   Newdrug:28   1st Qu.:57.25   M:24  \n Mode  :character                Median :63.00         \n                                 Mean   :61.48         \n                                 3rd Qu.:65.00         \n                                 Max.   :75.00         \n      bp1              bp2            bpdiff      \n Min.   : 87.50   Min.   :78.00   Min.   : 0.500  \n 1st Qu.: 95.62   1st Qu.:85.22   1st Qu.: 4.800  \n Median : 97.70   Median :88.15   Median : 8.250  \n Mean   : 98.30   Mean   :88.60   Mean   : 9.704  \n 3rd Qu.: 99.40   3rd Qu.:92.10   3rd Qu.:13.700  \n Max.   :111.70   Max.   :99.70   Max.   :26.300  "},{"path":"descriptive-statistics-numeric.html","id":"single-continuous-variable","chapter":"10 Descriptive Statistics: Numeric","heading":"10.1 Single continuous variable","text":"","code":""},{"path":"descriptive-statistics-numeric.html","id":"measures-of-central-tendency-dispersion","chapter":"10 Descriptive Statistics: Numeric","heading":"10.1.1 Measures of Central Tendency & Dispersion","text":"determine mean, median, standard deviation, range (minimum, maximum) interquartile range initial blood pressureTable 7.2:  Alternatively, psych package gives measures details. output includes measure Kurtosis Skewness, describing shape data.Table 6.1:  show interquartile range following.Table 7.3:  ","code":"\nnewdrug %>% \n    summarise(\n        Mean = mean(bp1), \n        Median = median(bp1), \n        Standard_Dev = sd(bp1), \n        Minimum = min(bp1), \n        Maximum = max(bp1),\n        IQR = IQR(bp1)\n    ) \nnewdrug %$% \n    psych::describe(bp1)\nnewdrug %$% \n    psych::describe(bp1, IQR = TRUE,quant = c(.25, .75))"},{"path":"descriptive-statistics-numeric.html","id":"graphs---histogram","chapter":"10 Descriptive Statistics: Numeric","heading":"10.1.2 Graphs - Histogram","text":"","code":"\nnewdrug %>% \n    ggplot(aes(x = bp1)) + \n    geom_histogram(bins = 7, col=\"black\", alpha = .5, fill = \"red\") +\n    labs(title = \"Histogram of Blood Pressure before  intervention\",\n         x= \"BP1\")+\n    theme_light()"},{"path":"descriptive-statistics-numeric.html","id":"graphs---boxplot-and-violin-plot","chapter":"10 Descriptive Statistics: Numeric","heading":"10.1.3 Graphs - Boxplot and violin plot","text":"","code":"\nnewdrug %>% \n    ggplot(aes(y = bp1)) + \n    geom_boxplot(col=\"black\",  \n                 alpha = .2, \n                 fill = \"blue\", \n                 outlier.fill = \"black\",\n                 outlier.shape = 22) +\n    labs(title = \"Boxplot of Blood Pressure before  intervention\",\n         y = \"BP1\")+\n    theme_light()"},{"path":"descriptive-statistics-numeric.html","id":"graphs---density-plot","chapter":"10 Descriptive Statistics: Numeric","heading":"10.1.3.1 Graphs - Density plot","text":"","code":"\nnewdrug %>% \n    ggplot(aes(y = bp1)) + \n    geom_density(col=\"black\", fill = \"yellow\", alpha=.6) +\n    labs(title = \"Density Plot of Blood Pressure before  intervention\",\n         y = \"Blood Pressure before  intervention\")+\n    coord_flip() +\n    theme_light()"},{"path":"descriptive-statistics-numeric.html","id":"graphs---cumulative-frequency-plot","chapter":"10 Descriptive Statistics: Numeric","heading":"10.1.3.2 Graphs - Cumulative Frequency plot","text":"","code":"\nnewdrug %>% \n    group_by(bp1) %>% \n    summarize(n = n()) %>% \n    ungroup() %>% \n    mutate(cum = cumsum(n)/sum(n)*100) %>% \n    ggplot(aes(y = cum, x = bp1)) +\n    geom_line(col=3, linewidth=1.2)+\n    labs(\n        title = \"Cumulative Frequency Plot of Blood Pressure before  intervention\",\n        x = \"BP1\",\n        y = \"Cumulative Frequency\")+\n    theme_light() "},{"path":"descriptive-statistics-numeric.html","id":"multiple-continuous-variables","chapter":"10 Descriptive Statistics: Numeric","heading":"10.1.4 Multiple Continuous variables","text":"","code":""},{"path":"descriptive-statistics-numeric.html","id":"measures-of-central-tendency-dispersion-1","chapter":"10 Descriptive Statistics: Numeric","heading":"10.1.4.1 Measures of Central tendency & Dispersion","text":"Table 6.5:  illustrate graphing multiple continuous variables use 2 bp variablesNext, create multiple density plots","code":"\nnewdrug %>% \n    select(where(is.numeric)) %>% \n    psych::describe()\nbps <- \n    newdrug %>%\n    select(bp1, bp2) %>% \n    pivot_longer(\n        cols = c(bp1, bp2),\n        names_to = \"measure\", \n        values_to = \"bp\") %>% \n    mutate(\n        measure = fct_recode(\n            measure, \"Pre-Treatment\" = \"bp1\", \"Post-Treatment\" = \"bp2\"\n            )\n        )bps %>% \n    ggplot(aes(y = measure, x = bp, fill = measure)) +\n    ggridges::geom_density_ridges2( col=\"black\", alpha = .5, scale=1, \n                                    show.legend = F) +\n    labs(x = \"Blood pressure (mmHg)\", \n         y = \"Density\", \n         fill = \"Blood Pressure\") +\n    theme_bw()\nPicking joint bandwidth of 1.52\nbps %>% \n    ggplot(aes(y = measure, x = bp, fill = measure))+\n    geom_boxplot(show.legend = FALSE) +\n    labs(y = NULL, \n         x = \"Blood Pressure\", \n         fill = \"Blood Pressure\") +\n    coord_flip()+\n    theme_light() \nbps %>% \n    ggplot(aes(y = measure, x = bp, fill = measure))+\n    geom_violin(show.legend = FALSE) +\n    coord_flip()+\n    theme_light() "},{"path":"descriptive-statistics-numeric.html","id":"continuous-by-a-single-categorical-variable","chapter":"10 Descriptive Statistics: Numeric","heading":"10.2 Continuous by a single categorical variable","text":"","code":""},{"path":"descriptive-statistics-numeric.html","id":"summary","chapter":"10 Descriptive Statistics: Numeric","heading":"10.2.1 Summary","text":"one variable.Table 6.10:  ","code":"\nnewdrug %>% \n    group_by(treat) %>% \n    summarize(mean.bp1 = mean(bp1),\n              sd.bp1 = sd(bp1),\n              var.bp1 = var(bp1),\n              se.mean.bp1 = sd(bp1)/sqrt(n()),\n              median.bp1 = median(bp1),\n              min.bp1 = min(bp1),\n              max.bp1 = max(bp1)) %>% \n    ungroup()"},{"path":"descriptive-statistics-numeric.html","id":"graph---histogram-boxplot-density-plot-and-cumulative-frequency","chapter":"10 Descriptive Statistics: Numeric","heading":"10.2.2 Graph - Histogram, Boxplot, Density plot and cumulative frequency","text":"graphs similar skip .","code":""},{"path":"descriptive-statistics-numeric.html","id":"continuous-by-multiple-categorical-variables","chapter":"10 Descriptive Statistics: Numeric","heading":"10.3 Continuous by multiple categorical variables","text":"","code":""},{"path":"descriptive-statistics-numeric.html","id":"summary-1","chapter":"10 Descriptive Statistics: Numeric","heading":"10.3.1 Summary","text":"can done .Table 6.11:  can presented boxplot ","code":"\nnewdrug %>% \n    group_by(treat, sex) %>% \n    summarize(mean.bp1 = mean(bp1),\n              sd.bp1 = sd(bp1),\n              var.bp1 = var(bp1),\n              se.mean.bp1 = sd(bp1)/sqrt(n()),\n              median.bp1 = median(bp1),\n              min.bp1 = min(bp1),\n              max.bp1 = max(bp1),\n              .groups = \"drop\") \nnewdrug %>% \n    ggplot(aes(y = bp1, x = sex, fill = treat)) +\n    geom_boxplot()+\n    labs(\n        y = \"Blood Pressure (mmHg)\",\n        x =  \"Sex\",\n        fill = 'Treatment') +\n    theme_bw()"},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"11 Hypothesis-Testing","heading":"11 Hypothesis-Testing","text":"","code":""},{"path":"hypothesis-testing.html","id":"population-and-sample","chapter":"11 Hypothesis-Testing","heading":"11.1 Population and sample","text":"","code":""},{"path":"hypothesis-testing.html","id":"population","chapter":"11 Hypothesis-Testing","heading":"11.1.1 Population","text":"population statistical sense different general sense. population collection items, people, places etc. investigator generally interested wants study. population tends large necessitating investigator pick just representative sample \nstudy particular property. example:determine proportion Ghanaians males may pick representative sample study population “Ghanaians”.determine proportion defective items produced factory sample items actual population involves items produced possibly yet produced.Therefore, statistical definition population include items, places persons even born non-existent.","code":""},{"path":"hypothesis-testing.html","id":"sample","chapter":"11 Hypothesis-Testing","heading":"11.1.2 Sample","text":"part population selected whatever method, usually whole population large unavailable studied. many ways select sample population. sample thus usually smaller population.","code":""},{"path":"hypothesis-testing.html","id":"descriptive-versus-inferential-statistics","chapter":"11 Hypothesis-Testing","heading":"11.2 Descriptive versus Inferential Statistics","text":"almost every research idea determine specific parameter population. instance, determine proportion children age 18 years specific flight (e.g. British Airways (BA) London Johannesburg) start randomly selecting sample BA flights \ntwo destinations. Next, determine ages selected flights finally come proportion ages less 18 years. Bear mind population includes future flights population parameter can rarely obtained. However, sample proportion (statistic) determined good estimate population parameter.Descriptive statistics involve statistical manipulations done specific sample whereas inferential statistics manipulation used estimate population parameter sample statistic. Using example , determining proportion -18-year-old persons chosen flights falls descriptive statistics whereas estimating population proportion -18-year-old persons fly BA along route sample statistic use inferential statistics.","code":""},{"path":"hypothesis-testing.html","id":"sample-variation","chapter":"11 Hypothesis-Testing","heading":"11.3 Sample variation","text":"Mid-upper arm circumference children five measure thin child quick way determining /nutritional status. population 2000 children, mean median mid-upper arm circumferences determined independently group 10 students. decided take random sample 10 children estimate parameters. column students shows measurements made student /sample chosen. first read data.visualise itTable 7.2:  Next, determine mean median values obtained studentTable 6.1:  obvious despite data coming population means obtained different time. effect despite using population, since choosing sample random process descriptive statistic(s) obtained time sample chosen population vary. differences (variation) statistics obtained (mean median MUAC) instance, every sample described sample variation. statistics vary can determine population parameter? inferential statistics , estimating population parameter sample statistic.","code":"\ndf_students <- read.delim(\"./Data/students.txt\", sep = \" \")\ndf_students\ndf_students %>% \n    summarize(across(X1:X10, ~mean(.)))"},{"path":"hypothesis-testing.html","id":"hypothesis-testing-1","chapter":"11 Hypothesis-Testing","heading":"11.4 Hypothesis testing","text":"collection scientific data usually preceded idea one wants prove disprove. humans, often preconceived ideas opinions expected results study. subconscious thinking brought light formally setting hypothesis testing . section deals formalizing steps involved process affects study design, data collection, analysis, presentation results.","code":""},{"path":"hypothesis-testing.html","id":"stating-the-hypothesis.","chapter":"11 Hypothesis-Testing","heading":"11.4.1 Stating the hypothesis.","text":"Standing window one morning Mr Osei wondered () number women using services bank next door men. vibrant market nearby mainly women trading wares. proximity market bank may given impression. Now decided investigate . Subconsciously wondering :proportion men using services bank women.Conversely, also wondering :proportion men using banking services different women.Bear mind second line thinking includes men using services compared women vice versa. competing ideas give rise following hypothesis:Null hypothesis (H0): proportion men using services bank next door different women.alternativelyAlternate Hypothesis (Ha): difference proportion men women using banking services.two hypotheses describe Mr. Osei’s idea opposing manner. null alternate hypotheses can tested well-designed study. statistical technical reasons, null hypothesis often preferred regard.","code":""},{"path":"hypothesis-testing.html","id":"testing-the-hypothesis","chapter":"11 Hypothesis-Testing","heading":"11.4.2 Testing the Hypothesis","text":"hypothesis stated next objective collect evidence (data) either prove disprove . obvious customers (study population) include future ones assertion can never completely determined customers enumerated. Hence Mr Osei decides pick sample customers. sample, needs determine enough evidence disprove null hypothesis. thinks can conclude “enough evidence say proportion men women using bank’s services ”.words, different proportions. hand, come significant evidence null hypothesis can conclude “insufficient evidence conclude proportion men using bank’s services women”.","code":""},{"path":"hypothesis-testing.html","id":"type-i-error","chapter":"11 Hypothesis-Testing","heading":"11.4.3 Type I error","text":"recollect earlier chapter since many ways choosing sample population results tend differ sample sample. called sample variability. Due sample variability, sample statistics usually differ population parameter.example involving Mr. Osei bank customers, proceeded collect sexes 250 systematically selected samples customers came following results. 112(44.8%) males 138(55.2%) females sample. note sample inference, another sample give entirely different result (sample variability)., question think enough evidence reject H0? think difference proportion significant evidence notion proportions ? can never entirely sure rejection H0 right wrong. However, can conclude smaller \nproportion males obtained sample higher chance null hypothesis (proportions ) false. hypothesis testing type error said made null hypothesis rejected fact true. can therefore say type error made H0 wrongly rejected.example Mr Osei making type error concludes based data obtained proportions men women fact population.","code":""},{"path":"hypothesis-testing.html","id":"the-significance-level","chapter":"11 Hypothesis-Testing","heading":"11.4.4 The Significance Level","text":"significance level, usually denoted alpha (\\(\\alpha\\)) probability making type error. usually set investigator direct bearing sample size study.","code":""},{"path":"hypothesis-testing.html","id":"type-ii-error","chapter":"11 Hypothesis-Testing","heading":"11.4.5 Type II error","text":"Conversely, type II error said made researcher fails reject null hypothesis fact false. Applied situation type II error committed Mr Osei based stated result decides insufficient evidence conclude proportion two sexes differ fact differ population bank users.two types errors mentioned always increase expense . type error rises type II error falls vice versa.","code":""},{"path":"hypothesis-testing.html","id":"power","chapter":"11 Hypothesis-Testing","heading":"11.4.6 Power","text":"probability committing type II error called beta (\\(\\beta\\)). probability committing type II error called Power test.\nisp(Type II error) = \\(\\beta\\) \\(Power = 1 - \\beta\\)power statistical test, therefore, measures ability reject null hypothesis false hence make right decision.","code":""},{"path":"hypothesis-testing.html","id":"critical-value","chapter":"11 Hypothesis-Testing","heading":"11.4.7 Critical value","text":"Another way stating null hypothesis put forward Mr Osei isH0: difference proportion men women using services bank \nzero.proportions 50% absolute difference (difference disregarding whether negative positive) proportions zero. can thus seen possible values absolute difference proportion can 0% (equal numbers males females sample) 100% (either females males). diagrammatically shown Figure 21. diagram, percentage men women x-axis absolute difference y-axis. V-shaped graph lowest (0%) percentages equal men women. hand either males 100% (far right) females 100% (far left) absolute value rises 100%. results obtained Mr Osei, absolute difference percentage men women \\[55.2\\% - 44.8\\% = 10.4\\%\\]\nAlso, intuitively difference proportion sexes (.e. 50% ) sample (randomly selected) difference proportion likely near 0 100. nearer 0 less likely reject null hypothesis. nearer 100 likely reject null hypothesis.next issue arises! cut-value one decide enough evidence reject H0? hypothetical value, often set investigator called critical value. Mr. Osei set critical value difference proportion 10% (green horizontal line graph) rejected null hypothesis since 10.4% falls line. hand, set critical value 12.0% failed reject H0 value\ngreater observed statistic 10.4% hence falling green line.","code":""},{"path":"hypothesis-testing.html","id":"critical-region","chapter":"11 Hypothesis-Testing","heading":"11.4.8 Critical region","text":"critical value divide range test statistic two possible regions. region values obtained lead rejection H0 (green line) value obtained lead rejection H0 (green line). critical region latter. former often called acceptance region.Mr. Osei’s example, using critical value 10% can divide possible values test statistic two regions. 0% less 10% 10% 100%. region 10% 100% critical region study. comes value region automatically reject null hypothesis.","code":""},{"path":"hypothesis-testing.html","id":"p-value","chapter":"11 Hypothesis-Testing","heading":"11.4.9 P-value","text":"P-values well-known research. often misinterpreted overemphasized. however pivotal place research statistical inference. probability value (p-value) probability statistic extreme one observed sample null hypothesis true. determines strength support null hypothesis. Thus nearer p-value 1 better data hand test statistics support null value. hand nearer p-value 0 less statistic data supports null hypothesis. data analysis, p-value often compared significance level (\\(\\alpha\\)). less significance level result said statistically significant. case Mr Osei, chose significance level 0.05 however p-value (analysis using software) determined 0.1137. probability obtaining proportions (112(44.8%) males 138(55.2%) females) observed null hypothesis (50% vs 50%) true 0.1137.therefore conclude based significance level 0.05 results statistically significant.","code":""},{"path":"hypothesis-testing.html","id":"steps-in-hypothesis-testing","chapter":"11 Hypothesis-Testing","heading":"11.4.10 Steps in Hypothesis testing","text":"going terms now ready outline hypothesis testing done statistics.4 basic steps. :first step hypothesis testing state null hypothesis (H0).Next decide significance level (\\(\\alpha\\)). Typically use \\(\\alpha\\) 0.05 0.1 0.01 also sometimes used.Next compute probability value (p-value). explained aboveFinally, compare p-value significance level. p-value lower \\(\\alpha\\) reject null hypothesis refuse reject null hypothesis.Generally, lower p-value one confident rejecting H0. Note failure reject H0 mean H0 true. simply means don’t enough evidence reject .","code":""},{"path":"hypothesis-testing.html","id":"estimation","chapter":"11 Hypothesis-Testing","heading":"11.5 Estimation","text":"","code":""},{"path":"hypothesis-testing.html","id":"point-and-interval-estimates","chapter":"11 Hypothesis-Testing","heading":"11.5.1 Point and interval estimates","text":"Previously came across statistics mean proportion used estimates population parameter. called point estimates usually give single value population estimate. However, another way determining population parameter provide interval rather just statistic. referred interval estimate.Another way Mr. Osei express result estimate proportion men 38.5% 51.2%. Thus indicating based data available thinks population estimate proportion men 38.5% 51.2%.","code":""},{"path":"hypothesis-testing.html","id":"confidence-interval","chapter":"11 Hypothesis-Testing","heading":"11.5.2 Confidence Interval","text":"one ever read research journal article may come across phrase confidence interval. used express uncertainty precision results obtained sampling method.study determine average age approximately 200,000 factory workers Ghana, investigator decided choose sample 200 determine mean age. course, sample mean likely differ population mean sample variation. Based data investigator can estimate interval population mean likely fall. estimate confidence interval. can imagine instead just one study study done hundred persons choosing 200 workers randomly coming similar confidence intervals based samples. sampling variation, study likely come different values confidence intervals. intervals generated hundred persons 100 studies plotted Figure 22. green horizontal line population mean age.95% confidence interval confidence interval likely contain population mean 95% time. estimate provided Figure 22 95% confidence interval individual studies. can seen 5 100 randomly sampled workers yielded confidence interval including population mean. Similarly, 99% confidence interval repeated studies going contain population mean 99 % cases. obvious interpreting confidence interval terms one study can problematic. However, intuitive way understanding confidence interval : 95% chance 95% confidence interval calculated contains true population mean. words, always 5% chance 95% confidence interval generated contain population mean.determines width confidence interval?first determined size confidence interval. 99% confidence interval wider 95% confidence interval turn wider 90% confidence interval.Secondly smaller sample size wider confidence interval. just ection uncertainty estimate.Finally variation data obtained reacts confidence intervals. Populations elements similar yield smaller confidence interval compared one observations variable. instance, confidence interval mean weight babies much narrower confidence interval whole\npopulation. weights babies likely near (less variability) weight whole population babies, children, adolescents adults.common mistake population parameter said instance 95% probability lying within 95% confidence interval. instance interpreting 95% confidence interval 38.5% 51.2% probability proportion men using bank lies within range wrong. population parameter fixed number probability.","code":""},{"path":"normality-of-data.html","id":"normality-of-data","chapter":"12 Normality of data","heading":"12 Normality of data","text":"Many test statistical tests, specifically parametric tests done\npremise numeric data normally distributed. Unfortunately, \nalways . chapter, look normally distributed data\ncan tell data normally distributed. section, \nuse hb variable mps.dta data.","code":""},{"path":"normality-of-data.html","id":"the-normal-distribution","chapter":"12 Normality of data","heading":"12.1 The normal distribution","text":"normal distribution, also called Gaussian Distribution bell curve,\ndefined two main statistics. mean standard deviation.\nwider standard deviation, broader curve. example \nnormal distribution shown :features normal distribution:symmetrical.mean, median mode .Approximately 68% data falls within one standard deviation mean.Approximately 95% data falls within two standard deviations meanApproximately 99.7% data fall within three standard deviations mean.normal distribution mean 1 standard deviation 1 called\nstandard normal distribution.","code":"\ndf_temp <- data.frame(x = rnorm(2000)) \n\ndf_temp %>% \n    ggplot(aes(x = x))+\n    geom_histogram(\n        aes(y=after_stat(density)), \n        bins=10, fill = \"snow\", col = \"red\") +\n    stat_function(\n        fun = dnorm, \n        args = list(\n            mean = mean(df_temp$x, na.rm=T), \n            sd = sd(df_temp$x)), col = \"blue\",\n            linewidth = 1.5) +\n    labs(x = NULL, y = NULL) +\n    scale_x_continuous(labels = NULL)+\n    scale_y_continuous(labels = NULL)+\n    theme_minimal()"},{"path":"normality-of-data.html","id":"evaluating-normality","chapter":"12 Normality of data","heading":"12.2 Evaluating normality","text":"two main modalities evaluating normality. graphical \nformal hypothesis testing.","code":""},{"path":"normality-of-data.html","id":"graphical-evaluation","chapter":"12 Normality of data","heading":"12.2.1 Graphical evaluation","text":"Histogram: Probably well know modality histogram.\nfirst read data keep hb variable:draw histogram hb.near symmetry slightly heavier left tail.Boxplot: next graphical modality boxplot drawn .conclusion good symmetry slightly heavier lower tail seen\n.Q-Q plot: Finally, Q-Q plot line. graphical modality plots\nactual values data theoretical normal distribution. Thus,\ndots straight line along line drawn \nideal normal distribution. therefore use principle determine \ndata instance heave tails, indicating skewness. Q-Q plot\ndata done :seen apart points mainly right tail rest\npretty much follow line.","code":"\ndf_mps <- haven::read_dta(\"./Data/mps.dta\")\ndf_mps %>% \n    drop_na(hb) %>% \n    ggplot(aes(x = hb)) +\n    geom_histogram(bins = 10, fill = 'white', col = \"black\") +\n    labs(title = \"Histogram of HB\", y = \"Frequency\", x = 'Hb (g/dL') +\n    theme_bw()\ndf_mps %>% \n    drop_na(hb) %>% \n    ggplot(aes(x = hb)) +\n    geom_boxplot(fill = 'white', col = \"black\") +\n    labs(title = \"Boxplot of HB\", y = \"Frequency\", x = 'Hb (g/dL') +\n    theme_bw()\ndf_mps %>%\n    drop_na(hb) %>% \n    ggpubr::ggqqplot(x = \"hb\",title = \"Q-Q plot of the HB\", conf.int = FALSE)"},{"path":"normality-of-data.html","id":"statistical-tests-for-normality","chapter":"12 Normality of data","heading":"12.2.2 Statistical tests for normality","text":"Formal statistical tests available testing.\nH0: data sampled normally distributed population.\nHa: data sampled normally distributed population\ntests concentrating Shapiro-Wilk\ntests. never advisable different tests together\nuse different algorithms may produce different results \nconclusions.Shapiro-Wilk test: perform Shapiro-wilk test normality.Table 6.3:  p-value greater 0.05 indicates reject Null hypothesis thus\nconclude data comes normally distributed population.","code":"\ndf_mps %$% \n    shapiro.test(hb) %>% \n    broom::tidy()"},{"path":"normality-of-data.html","id":"conclusion-1","chapter":"12 Normality of data","heading":"12.3 Conclusion","text":"conclusion, can seen graphical presentations well \nformal test data coming normally distributed\npopulation.various test can give contradictory results recommend evaluating\nnormality population, one first plot histogram, Q-Q plot,\nperform one formal test, combine results making \njudgement numeric data normally distributed.","code":""},{"path":"analysis-of-numeric-data.html","id":"analysis-of-numeric-data","chapter":"13 Analysis of numeric data","heading":"13 Analysis of numeric data","text":"far, dealt descriptive statistics analysis sample\ndata collected. However, bane statistical analysis make\ninferences population whole. section, mainly \ninferential analysis continuous variables.","code":""},{"path":"analysis-of-numeric-data.html","id":"confidence-interval-of-a-mean","chapter":"13 Analysis of numeric data","heading":"13.1 Confidence interval of a mean","text":"determine confidence interval mean numeric variable R, \nuse One Sample Student’s T-test. assumptions validity \ntest :sample randomly chosenThe population distribution variable normal. can \nassumed present \ndistribution population known normally distributed\npopulation distribution one mode, symmetric, without\noutliers sample size 15 less\npopulation distribution moderately skewed, without outliers,\none mode sample size 16 40\nsample size 40 data outliers.\ndistribution population known normally distributedThe population distribution one mode, symmetric, without\noutliers sample size 15 lessThe population distribution moderately skewed, without outliers,\none mode sample size 16 40The sample size 40 data outliers.sample considered randomly selected sample size 140, \napply One-sample T-test .first import dataAnd summarize belowTable 6.2:  Table 6.3:  sex stratified confidence intervals haveTable 7.4:  ","code":"\ndf_data1 <- \n    read_delim(\n        file = \"./Data/data1.txt\", \n        delim = \"\\t\",\n        col_types = c(\"c\", \"f\", \"i\",\"i\")\n    ) %>% \n    mutate(sex = factor(sex))df_data1 %>% \n    summarytools::dfSummary(graph.col = F)\nData Frame Summary  \ndf_data1  \nDimensions: 140 x 4  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------\nNo   Variable      Stats / Values            Freqs (% of Valid)   Valid      Missing  \n---- ------------- ------------------------- -------------------- ---------- ---------\n1    id            1. 1                        1 ( 0.7%)          140        0        \n     [character]   2. 10                       1 ( 0.7%)          (100.0%)   (0.0%)   \n                   3. 100                      1 ( 0.7%)                              \n                   4. 101                      1 ( 0.7%)                              \n                   5. 102                      1 ( 0.7%)                              \n                   6. 103                      1 ( 0.7%)                              \n                   7. 104                      1 ( 0.7%)                              \n                   8. 105                      1 ( 0.7%)                              \n                   9. 106                      1 ( 0.7%)                              \n                   10. 107                     1 ( 0.7%)                              \n                   [ 130 others ]            130 (92.9%)                              \n\n2    sex           1. Female                 64 (45.7%)           140        0        \n     [factor]      2. Male                   76 (54.3%)           (100.0%)   (0.0%)   \n\n3    weight        Mean (sd) : 12.2 (6.3)    28 distinct values   140        0        \n     [numeric]     min < med < max:                               (100.0%)   (0.0%)   \n                   2 < 11 < 33                                                        \n                   IQR (CV) : 7 (0.5)                                                 \n\n4    height        Mean (sd) : 90.9 (21.3)   70 distinct values   139        1        \n     [numeric]     min < med < max:                               (99.3%)    (0.7%)   \n                   49 < 88 < 137                                                      \n                   IQR (CV) : 33 (0.2)                                                \n--------------------------------------------------------------------------------------\ndf_data1 %>% \n    meantables::mean_table(height) \ndf_data1 %>% \n    group_by(sex) %>% \n    meantables::mean_table(height)"},{"path":"analysis-of-numeric-data.html","id":"comparing-the-mean-to-a-hypothesised-value","chapter":"13 Analysis of numeric data","heading":"13.2 Comparing the mean to a hypothesised value","text":"Assuming objective data collected determine average\nweight population similar population known mean weight \n14kgs.null hypothesis :H0: difference mean weight population \npopulation mean weight 14kgs.test hypothesis use One sample t-test satisfied\nassumptions use met.Table 6.4:  p-value 0.001 probability sample come \npopulation mean weight 14kgs. Since small reject \nnull H0 5% significance level conclude population mean weight\nsignificantly different 14kgs.\nconfidence interval generated sample mean. \nhypothesized value \n14kgs outside confidence interval mean conclude \ninsuficient evidence suggest mean weight population 14kgs.","code":"\ndf_data1 %$% \n    t.test(weight, mu=14) %>% \n    broom::tidy()"},{"path":"analysis-of-numeric-data.html","id":"comparing-mean-of-two-independent-groups","chapter":"13 Analysis of numeric data","heading":"13.3 Comparing mean of two independent groups","text":"possibly common use t-test. compare mean weights\nmales female study come withH0: difference weight males females \npopulation.test assertion first determine sample fits assumption \nuse Two sample t-test. :sample randomly chosenThe two samples completely independentEach population least 20 times larger respective sample.population distribution variable normal. can assumed \npresent \ndistribution population known normal\npopulation distribution one mode, symmetric, without outliers \nsample size 15 less\npopulation distribution moderately skewed, without outliers, \none mode sample size 16 40\nsample size 40 data outliers.\ndistribution population known normalThe population distribution one mode, symmetric, without outliers \nsample size 15 lessThe population distribution moderately skewed, without outliers, \none mode sample size 16 40The sample size 40 data outliers.data fulfils criteria hence apply testTable 6.5:  relatively high p-value conclude insufficient evidence\nrefute null hypothesis. words insufficient evidence \nconclude mean weights males females differ study population.\nNote sample however females appear heavier males shown \nlast two lines output .confidence interval determined (-0.86 3.47) actually \nmean sample difference females males. Since confidence\ninterval contains null value H0 .e. 0, conclude \nisn’t enough evidence difference mean weight two sexes.\nHence confidence interval p-value come similar conclusions.","code":"\ndf_data1 %$% \n    t.test(formula = weight ~ sex) %>% \n    broom::tidy()"},{"path":"analysis-of-numeric-data.html","id":"comparing-means-of-paired-observations","chapter":"13 Analysis of numeric data","heading":"13.4 Comparing means of paired observations","text":"section use bread.txt data weight grams \nbaking loaves bread. description variables \ncontained data file. Paired observations occur circumstances \nrepeated measurement done object data collected \ncharacteristics common. bread data bread weighed \nbaking. Determining significant difference \ntwo measurements requires use Paired t-test. always begin \nimporting dataNext determine structure data frame df2And summarize itOur next task compare weight loaves bread \nbaking. begin looking mean standard deviations two\nweights.Table 6.10:  obvious mean weight bread baking much higher\nhowever standard deviations appear similar. formal test \ndetermine difference means use paired t-test. \nstate assumptions paired t-testThe sample randomly chosenThe two samples independent (related)population least 20 times larger respective sample.population distribution difference two variables\nnormal.\ncan assumed present \ndistribution population known normal\npopulation distribution one mode, symmetric, without\noutliers sample size 15 less\npopulation distribution moderately skewed, without outliers,\none mode sample size 16 40\nsample size 40 data outliers.\ndistribution population known normalThe population distribution one mode, symmetric, without\noutliers sample size 15 lessThe population distribution moderately skewed, without outliers,\none mode sample size 16 40The sample size 40 data outliers.new assumption need evaluate distribution \ndifference weights . belowAlternatively, can perform Shapiro-Wilk’s test normality. H0\ndeviating normal distribution. done belowTable 13.1:  output graphical representation shows difference weight \nliterally normally distributed. therefore go ahead determine \ndifference mean weights. First state hypothesisH0: change weight loaves bread bakingAnd perform test converting data long formatTable 13.2:  average 162.5g reduction weight loaves bread \nbaking. reduction 95% confidence interval 159.1g 165.9g \nsignificantly different 0 (p-value<0.001).","code":"\ndf_bread <- \n    read.table(\"./Data/bread.txt\", sep=\"\\t\", header=T) %>% \n    mutate(oven = factor(oven), type = factor(type))df_bread %>% summarytools::dfSummary(graph.col = F)\nData Frame Summary  \ndf_bread  \nDimensions: 399 x 5  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------\nNo   Variable    Stats / Values             Freqs (% of Valid)    Valid      Missing  \n---- ----------- -------------------------- --------------------- ---------- ---------\n1    sid         Mean (sd) : 200 (115.3)    399 distinct values   399        0        \n     [integer]   min < med < max:           (Integer sequence)    (100.0%)   (0.0%)   \n                 1 < 200 < 399                                                        \n                 IQR (CV) : 199 (0.6)                                                 \n\n2    type        1. maize                   212 (53.1%)           399        0        \n     [factor]    2. wheat                   187 (46.9%)           (100.0%)   (0.0%)   \n\n3    before      Mean (sd) : 379.5 (28.1)   120 distinct values   399        0        \n     [integer]   min < med < max:                                 (100.0%)   (0.0%)   \n                 304 < 379 < 479                                                      \n                 IQR (CV) : 40 (0.1)                                                  \n\n4    after       Mean (sd) : 217 (29.8)     121 distinct values   399        0        \n     [integer]   min < med < max:                                 (100.0%)   (0.0%)   \n                 140 < 215 < 295                                                      \n                 IQR (CV) : 48 (0.1)                                                  \n\n5    oven        1. Firewood                199 (49.9%)           399        0        \n     [factor]    2. Gas                     200 (50.1%)           (100.0%)   (0.0%)   \n--------------------------------------------------------------------------------------\noptions(huxtable.knit_print_df = TRUE)\ndf_bread %>% \n    select(before, after) %>% \n    rstatix::get_summary_stats(type= \"mean_sd\")\ndf_bread %>% \n    mutate(diff_in_wgt = after - before) %>% \n    ggplot(aes(x = diff_in_wgt)) +\n    geom_histogram(bins = 10, col = \"white\") + \n    labs(x = \"Difference in weight\") +\n    theme_bw()\ndf_bread %>% \n    mutate(diff_in_wgt = after - before) %>%\n    rstatix::shapiro_test(vars = \"diff_in_wgt\")\ndf_bread %>%\n    pivot_longer(\n        cols = c(before, after), names_to = \"time\",values_to = \"weight\"\n        ) %>% \n    rstatix::t_test(formula = weight~time, paired = TRUE, detailed = TRUE)"},{"path":"analysis-of-numeric-data.html","id":"test-for-equality-of-variances","chapter":"13 Analysis of numeric data","heading":"13.5 Test for equality of variances","text":"using Student’s T-test determine difference means two\nindependent groups need mindful variances group. \ncomputations done independent groups t-test different \nvariances groups similar different. Therefore determine\nmean weight significantly differ males females need \ndetermine compare variances. function var.test() R compares\nvariances two independent groups can used \ndetermination.apply F-test compare variances weight\ntwo sexes. First determine variances.Table 13.3:  seem big difference variance weights \ntwo sexes. females almost 1.7 times males. \ndetermine chance finding apply formal statistical test.\nourH0: difference variance weights males \nfemales populationThe F-test actually tests ratio variances difference. \nregard null value 1.Table 13.4:  significant p-value (significance level 0.05) confidence\ninterval containing 1 (null value) implies little evidence\nvariance two groups (words \ndiffer significantly). case conclusion previous analysis \nvalid R assumes variances unequal default t.test()\nfunction used.Next apply principle determine mean heights \nsimilar males females population. first determine \nvariances significantly different.Table 13.5:  results variance females look much higher (1.3 times)\nmales however apply test formally determine .Table 13.6:  p-value confidence interval conclude insufficient evidence \nsay two variances different. use t.test() function determine\npossibility mean height differ males females specify\nvariance equal .Table 13.7:  ","code":"\ndf_data1 %>% \n    group_by(sex) %>%\n    summarise(across(weight, list(var = var, meam = mean)))df_data1 %>% \n    var.test(formula = weight~sex, data = .) %>% \n    broom::tidy() \nMultiple parameters; naming those columns num.df, den.df\ndf_data1 %>% \n    group_by(sex) %>%\n    summarise(\n        across(height, list(var = ~var(., na.rm=T), meam = ~mean(., na.rm=T)))\n    )df_data1 %>% \n    var.test(formula = height~sex, data = .) %>% \n    broom::tidy() \nMultiple parameters; naming those columns num.df, den.df\ndf_data1 %>%\n    rstatix::t_test(formula = height~sex, var.equal = TRUE, detailed = TRUE)"},{"path":"analysis-of-categorical-data.html","id":"analysis-of-categorical-data","chapter":"14 Analysis of categorical data","heading":"14 Analysis of categorical data","text":"section discuss analyse categorical data, aggregated non-aggregated.","code":""},{"path":"analysis-of-categorical-data.html","id":"one-sample-binomial-test","chapter":"14 Analysis of categorical data","heading":"14.1 One-sample binomial test","text":"","code":""},{"path":"analysis-of-categorical-data.html","id":"one-sample-proportion-confidence-interval","chapter":"14 Analysis of categorical data","heading":"14.1.1 One sample proportion: Confidence interval","text":"study determine prevalence hypertension certain adult\npopulation, random sample taken revealed 23 67 hypertension. \nproportion hypertensive patients sample rather straightforward.\nApproximately 0.34 (34.3%) sample hypertensives. however\nextrapolate estimate proportion population estimating \nconfidence interval. single proportion estimation use \nbinom.test() function fulfilled conditions required \nuse. :sample obtained simple random samplingThere just two possible outcomes data, hypertension (successes) \nhypertension (failures).sample includes least 10 successes 10 failuresThe population least 20 times larger sample size.sample violating conditions, go ahead use one-sample\nbinomial test :Table 7.1:  binomial exact confidence interval proportion hypertension \nstudy population, therefore, 23.2% 46.9%.","code":"\nbinom.test(23, 67) %>% \n    broom::tidy() %>% \n    select(-method)"},{"path":"analysis-of-categorical-data.html","id":"one-sample-proportion-hypothesis-testing","chapter":"14 Analysis of categorical data","heading":"14.1.2 One sample proportion: Hypothesis testing","text":"data hypertension collected, investigator\nhypothesized 50% population hypertensive. Next, test \nhypothesis. usual, start setting null hypothesis.H0: population proportion hypertensive population 50%Table 7.2:  realised p-value identical last\ncomputation. binom.test() default tests proportion\n50:50 proportion population. 5% significance level, \nreject H0 conclude proportion hypertensive population\nsignificantly differs 50%., investigators hypothesized prevalence population\n40%? null hypothesis :H0: population proportion hypertensive 40%Next, test hypothesis.Table 6.1:  significance level 0.05, fail reject null hypothesis,\nconcluding insufficient evidence conclude population\nprevalence 40%.Finally, investigators hypothesized population\nprevalence hypertension least 47%? Remember one-sided test\nnull hypothesis beH0: population proportion hypertension greater equal 47%test hypothesisTable 7.3:  small p-value leads us reject H0 conclude insufficient\nevidence support assertion prevalence hypertension population least 47%.","code":"\nbinom.test(x=23, n=67, p=.5) %>% \n    broom::tidy()%>% \n    select(-method)\nbinom.test(x=23, n=67, p=.4) %>% \n    broom::tidy()%>% \n    select(-method)\nbinom.test(23, 67, p=.47, alternative = \"less\") %>% \n    broom::tidy()%>% \n    select(-method)"},{"path":"analysis-of-categorical-data.html","id":"two-or-more-sample-binomial-test","chapter":"14 Analysis of categorical data","heading":"14.2 Two or more sample binomial test","text":"","code":""},{"path":"analysis-of-categorical-data.html","id":"two-sample-proportion-hypothesis-testing","chapter":"14 Analysis of categorical data","heading":"14.2.1 Two sample proportion: Hypothesis testing","text":"Taking decide select different population sample 100\npersons determine proportion hypertension. \npopulation, came 52 hypertension patients hundred. aim compare significant difference proportion \nhypertension patients two populations. use \nprop.test() function R.First, state null hypothesis:\n>H0: population proportion hypertension populations .Next, test hypothesis. first create two vectors\nrepresenting number persons hypertension total samples\nchosen.Next, apply testTable 6.3:  p-value 0.0365 less regular significance level 0.05 \nreject null hypothesis say enough evidence conclude \nprevalence hypertension two populations.95% confidence interval generated difference \ntwo proportions, 0.177. confidence interval 0.014 0.339\ninclude null value 0 conclude difference \nprevalence hypertension 2 populations.","code":"\nhpt <-c(52, 23) # Vector of numbers with hypertension\nn <- c(100, 67) # Vector of numbers in the sample\nprop.test(x = hpt, n = n) %>% \n    broom::tidy()%>% \n    select(-method)"},{"path":"analysis-of-categorical-data.html","id":"chi-squared-test","chapter":"14 Analysis of categorical data","heading":"14.3 Chi-squared test","text":"Pearson’s chi-square test, also known chi-square goodness--fit test \nchi-square test independence used determine observed frequencies \ndata consistent expected. instance, Asia, \npercentages ABO blood groups population 38%, 10%, 3% 49% \ngroups , B, AB, O respectively. random sample 600 persons Kumasi,\nGhana 226, 82, 21 271 blood groups , B, AB, O respectively.\nchi-squared test can help us determine proportion blood groups\nfound Kumasi consistent seen Asia.","code":""},{"path":"analysis-of-categorical-data.html","id":"chi-squared-goodness-of-fit-test","chapter":"14 Analysis of categorical data","heading":"14.3.1 Chi-squared goodness of fit test","text":"blood group example investigators may want know \nproportions blood groups Kumasi consistent seen Asia.\nuse Chi-squared goodness fit test. always, begin \nmaking sure test can appropriately used condition. \nassumptions test :data obtained randomly populationThe variable study categoricalEach observed values category least 5With none conditions violated go ahead state null hypothesis asH0: distribution blood groups Kumasi different Asia.Next, perform test first create data frame \nexpected observed frequencies Asia Kumasi respectively.Table 7.4:  Next, illustrate proportions using barplot plotting blood\ngroups different regions side side.observe similarities proportions Kumasi Asia. \ninstance, blood groups regions show decreasing frequency O,\n, B AB. However, also observe adjacent bars exactly \nheight. Blood groups O B instance approximately 4%-point\ndifference two populations. Next, perform actual test \ndifference proportions.Table 6.5:  p-value 0.592, fail reject H0 conclude evidence\nproportions blood groups Kumasi different observed\nproportions Asia.","code":"\nbld_grp <- c(\"A\", \"B\", \"AB\", \"O\")\nAsia <- c(38.0, 10.0, 3.0, 49.9)\nKumasi  <- c(226, 82, 21, 271)\nKumasi <- round(Kumasi/sum(Kumasi)*100, 1)\ndf_temp <- data.frame(bld_grp, Asia, Kumasi)\ndf_temp\ndf_temp %>% \n    pivot_longer(\n        cols = c(Asia, Kumasi), \n        names_to = \"Place\", \n        values_to = \"Perc\") %>%\n    mutate(\n        labels = paste(\n            format(round(Perc,1), nsmall=1), \"%\", sep=\"\")) %>% \n    ggplot(aes(x = bld_grp, y = Perc, fill = Place)) +\n    geom_bar(stat=\"identity\", position= position_dodge()) +\n    geom_text(\n        aes(label=labels), vjust=1.5, color=\"black\", \n        size=3.5, position = position_dodge(0.9)) +\n    scale_fill_brewer(palette=\"Blues\") + \n    labs(\n        title=\"Comparative distribution of Blood Groups\", \n        x = \"Blood Group\", \n        y = \"Frequency\")+\n    theme_bw()chisq.test(x = Kumasi, p = Asia/sum(Asia)) %>% \n    broom::tidy()\nWarning in chisq.test(x = Kumasi, p = Asia/sum(Asia)):\nChi-squared approximation may be incorrect"},{"path":"analysis-of-categorical-data.html","id":"chi-squared-test-for-independent-data","chapter":"14 Analysis of categorical data","heading":"14.3.2 Chi-squared test for independent data","text":"subsection, use ANCdata epicalc package. can also\nobtained list data comes book. ANCdata\ncontains records high-risk pregnant women trial compare new \nold method antenatal care (anc) two clinics (clinic). outcome \nperinatal mortality, death baby within first week life (death).begin loading ANCdataOur objective section determine significant\nrelationship anc type perinatal death. words, \nperinatal mortality differ population depending anc method used?begin cross-tabulate two variables.cell proportions uniform. proportion deaths used\nold anc methods 11.0% 5.9% respectively. enough\nevidence conclude new better old population? \nquestion answer using formal statistical test.Test independence tabular data often entails use Chi-squared\ntest /Fisher’s exact test. Independence simply means one \none variable one predict variable.Next, apply chi-squared test verified data \nviolate assumption required use. .data obtained randomly populationThe variables study categoricalEach observation fits one cell tableThe expected values cell tabular data least 5Our data violate go ahead state null\nhypothesis.H0: difference proportion perinatal deaths mothers used new old ANC methods.perform testTable 6.11:  test yields relatively small p-value compared significance\nlevel 0.05, indicating null hypothesis independence cell\nproportions unlikely. words, cell proportions differ\nsignificantly, old method can said result significantly higher\nperinatal deaths compared new method.","code":"df_anc <- \n    read.delim(\"./Data/ANCData.txt\") %>% \n    mutate(\n        death = factor(death, levels = c(\"no\",\"yes\"), labels = c(\"No\", \"Yes\")),\n        clinic = factor(clinic),\n        anc = factor(anc, levels = c(\"old\", \"new\"), labels = c(\"Old\", \"New\")))\n\ndf_anc %>% summarytools::dfSummary(graph.col = FALSE)\nData Frame Summary  \ndf_anc  \nDimensions: 755 x 3  \nDuplicates: 747  \n\n--------------------------------------------------------------------------\nNo   Variable   Stats / Values   Freqs (% of Valid)   Valid      Missing  \n---- ---------- ---------------- -------------------- ---------- ---------\n1    death      1. No            689 (91.3%)          755        0        \n     [factor]   2. Yes            66 ( 8.7%)          (100.0%)   (0.0%)   \n\n2    anc        1. Old           419 (55.5%)          755        0        \n     [factor]   2. New           336 (44.5%)          (100.0%)   (0.0%)   \n\n3    clinic     1. A             497 (65.8%)          755        0        \n     [factor]   2. B             258 (34.2%)          (100.0%)   (0.0%)   \n--------------------------------------------------------------------------\ndf_anc %>% \n    tbl_cross(percent = \"col\",\n        row = death, \n        col = anc, \n        label = list(death ~ \"Death\", anc = \"ANC\")) %>% \n    bold_labels()\ndf_anc %>% \n    group_by(death, anc) %>% \n    count() %>% \n    ggplot(aes(fill = death, y = n, x = anc)) + \n    geom_bar(position = \"fill\", stat = \"identity\", col = \"black\") +\n    scale_fill_discrete(name = \"Death\", type = c(\"white\",\"red\"))+\n    labs(\n        y = \"Proportion\", \n        x = \"ANC Type\",\n        title = \"Distribution of the ANC method used and perinatal deaths\") +\n    theme_bw()\ndf_anc %$% \n    table(anc, death) %>% \n    chisq.test() %>% \n    broom::tidy()"},{"path":"analysis-of-categorical-data.html","id":"chi-squared-test-for-trend","chapter":"14 Analysis of categorical data","heading":"14.3.3 Chi-squared test for trend","text":"Often arise situations categorical data analysis objective \njust see difference proportions Chi-squared test independence\nFisher’s test determine trend proportions seen. \nChi-squared test trend often employed.example, study determine proportion persons eye changes\nfollowing data obtained: 4 42 persons aged less 45yrs, 7 \n43 aged 46yrs 55yrs, 12 46 aged 56yrs 65yrs \n15 44 aged 65yrs. First, input R calculate percentages eye changes.Next, determine percentage eye changes age groupNext, form matrix showing number persons eye changes, number\npersons studied percentage persons eye changes age\ngroup.analysis can said proportions persons eye\nchanges increases age. However, determine apparent rise \nchance finding apply Chi-Squared test confirm data \nviolate assumptions use. condition use like \nchi-squared test independent data addition least one \nvariables must ordered. Now satisfied data satisfies \nconditions state null hypothesis.H0: trend eye changes increasing ageNext, put formal testTable 13.4:  p-value less 0.05, reject H0 conclude \nsignificant trend (upward know ) developing eye changes \nincreasing age.","code":"\nNo.eye <- c(4,7,12,15)\nNo.studied <- c(42, 43, 46, 44)\nPerc.eye<-round(No.eye/No.studied * 100, 1)names(No.eye)<-c(\"<=45yrs\",\"46-55yrs\",\"56-65yrs\",\">65yrs\")\ncbind(No.eye, No.studied, Perc.eye)\n         No.eye No.studied Perc.eye\n<=45yrs       4         42      9.5\n46-55yrs      7         43     16.3\n56-65yrs     12         46     26.1\n>65yrs       15         44     34.1\nprop.trend.test(No.eye, No.studied) %>% \n    broom::tidy()"},{"path":"analysis-of-categorical-data.html","id":"fishers-exact-test","chapter":"14 Analysis of categorical data","heading":"14.4 Fisher’s Exact test","text":"valid conclusion use chi-squared test can guaranteed counts cells table question equal greater 5. Whenever count value cells 5, Fisher’s exact test must used instead. use interpretation similar chi-squared test demonstrated ANCdata .Table 13.5:  p-value quite like obtained chi-squared test \n. However, conclusion remains . added advantage using\nfisher.test() provision odds ratio 95% confidence\ninterval. odds ratio explained subsequent sections.","code":"\ndf_anc %$% \n    table(anc, death) %>% \n    fisher.test() %>% \n    broom::tidy()"},{"path":"risk-and-odds.html","id":"risk-and-odds","chapter":"15 Risk and Odds","heading":"15 Risk and Odds","text":"analysis effects depends mainly p-values confidence\nintervals difference proportions. common often better\nway expressing using Risk Odds. chapter, use \nANCData.txt data illustrate . First, read dataAnd summarize data","code":"\ndf_anc <- \n    read.delim(\"./Data/ANCData.txt\") %>% \n    mutate(\n        death = factor(death, levels = c(\"no\",\"yes\"), labels = c(\"No\", \"Yes\")),\n        clinic = factor(clinic),\n        anc = factor(anc, levels = c(\"old\", \"new\"), labels = c(\"Old\", \"New\")))df_anc %>% \n    summarytools::dfSummary(graph.col = FALSE)\nData Frame Summary  \ndf_anc  \nDimensions: 755 x 3  \nDuplicates: 747  \n\n--------------------------------------------------------------------------\nNo   Variable   Stats / Values   Freqs (% of Valid)   Valid      Missing  \n---- ---------- ---------------- -------------------- ---------- ---------\n1    death      1. No            689 (91.3%)          755        0        \n     [factor]   2. Yes            66 ( 8.7%)          (100.0%)   (0.0%)   \n\n2    anc        1. Old           419 (55.5%)          755        0        \n     [factor]   2. New           336 (44.5%)          (100.0%)   (0.0%)   \n\n3    clinic     1. A             497 (65.8%)          755        0        \n     [factor]   2. B             258 (34.2%)          (100.0%)   (0.0%)   \n--------------------------------------------------------------------------"},{"path":"risk-and-odds.html","id":"risk","chapter":"15 Risk and Odds","heading":"15.1 Risk","text":"Risk defined probability outcome. Therefore, \npopulation 100, 35 develop diabetes mellitus specified period \nfollow-, risk developing diabetes population \\[\\frac{35}{100} = 0.35\\]\nTabulation ANC method occurrence death , can conclude\nrisk perinatal mortality one uses old method 0.11 (11.0%)\nnew method 0.06 (5.9%).can written \n\\[Re = 0:06 \\text{ } Rne = 0:11\\]\\(Re\\) risk exposed group (new anc method) \\(Rne\\) \nrisk non-exposed (old anc method).","code":"\ndf_anc %>% \n    tbl_cross(percent = \"col\",\n        row = death, \n        col = anc, \n        label = list(death ~ \"Death\", anc = \"ANC\"),\n        digits = c(0,2)) %>% \n    bold_labels()"},{"path":"risk-and-odds.html","id":"risk-ratio","chapter":"15 Risk and Odds","heading":"15.2 Risk Ratio","text":"comparative way expressing risks two groups use \nRisk Ratio Relative Risk (RR).\n\n\\[RR = \\frac{Re}{Rne}\\]Note inference \\(Re\\) \\(Rne\\) \\(RR = 1\\). \\(RR\\)\nperinatal mortality new compared old method \\[RR = \\frac{5.952381}{10.978520} = 0.5421843 \\approx 0.54\\]epiDisplay package function cs() automatically calculates\nRR relevant stats confidence intervals. \napplied ANCdata .output first tabulates two variables producing contingency table\nmarginal totals. shows previously calculated parameters, Re\nRne. Rt (Risk total) risk exposed unexposed put\ntogether, .\\[Rt =\\frac{20 + 46}{419 + 336} = \\frac{66}{755} \\approx 0.09\\]next section output shows risk difference (difference \nrisks two groups), risk ratio, protective efficacy number\nneeded treat (NNT) together confidence intervals.Interpreting analysis far, conclude risk perinatal death\nusing new anc method significantly less using old method.\nsignificantly reduces risk death (Risk difference) 0.05 (5%) \nhalves chances death (RR = 0.54, 95%CI: 0.32 0.91). 20 (95%CI:\n11 95) pregnant women need treated new anc method prevent\none perinatal death (NNT).","code":"df_anc %$% epiDisplay::cs(outcome = death, exposure = anc, )\n\n          Exposure\nOutcome    Non-exposed Exposed Total\n  Negative 373         316     689  \n  Positive 46          20      66   \n  Total    419         336     755  \n                                    \n           Rne         Re      Rt   \n  Risk     0.11        0.06    0.09 \n                                         Estimate Lower95ci\n Risk difference (Re - Rne)              -0.05    -0.09    \n Risk ratio                              0.54     0.32     \n Protective efficacy =(Rne-Re)/Rne*100   45.8     8.71     \n   or percent of risk reduced                              \n Number needed to treat (NNT)            19.9     10.79    \n   or -1/(risk difference)                                 \n Upper95ci\n -0.01    \n 0.91     \n 68.06    \n          \n 94.2     \n          "},{"path":"risk-and-odds.html","id":"odds","chapter":"15 Risk and Odds","heading":"15.3 Odds","text":"Another way expressing risk outcome using Odds. Statistically odds\ndefined \\[Odds = \\frac{p}{1-p}\\]p probability outcome occurring. Using ANCdata probability \ndeath exposed .\\[pe = \\frac{20}{336} = 0.05952381\\]odds death exposed can determined \\[Oddse = \\frac{0.05952381}{1-0.05952381} = 0.06329114\\]Similarly, probability death non-exposed (old anc type) \\[pne = \\frac{46}{419} = 0.1097852\\]odds death non-exposed \\[Oddsne = \\frac{0.1097852}{1-0.1097852} = 0.1233244\\]","code":""},{"path":"risk-and-odds.html","id":"odds-ratio","chapter":"15 Risk and Odds","heading":"15.4 Odds ratio","text":"comparative way comparing two odds Odds Ratio (). \ndetermined \\[= \\frac{Oddse}{Oddsne} = 0.5132086 \\approx 0.51\\]fortunately go tedious procedure \ntime need calculate . cc() function epiDisplay\npackage well. apply analysis just done.output shows table variables question, 95%\nconfidence interval p-values determine chi-squared test \nFisher’s test. confidence interval odds ratio containing \nnull value 1, small p-values methods can concluded \nodds death mothers used new ANC method half (0.5) \nused old method probability obtaining values\nnull true, low (p-value = 0.019). Therefore, use new\nanc method associated significantly better perinatal outcomes compared\nold.Odds ratios important regression analysis dealt \ndetail subsequent chapters.","code":"df_anc %$% epiDisplay::cc(outcome=death, exposure=anc, graph = FALSE)\n\n       anc\ndeath   Old New Total\n  No    373 316   689\n  Yes    46  20    66\n  Total 419 336   755\n\nOR =  0.51 \n95% CI =  0.3, 0.89  \nChi-squared = 5.9, 1 d.f., P value = 0.015\nFisher's exact test (2-sided) P value = 0.019 "},{"path":"confounding-and-interaction.html","id":"confounding-and-interaction","chapter":"16 Confounding and Interaction","heading":"16 Confounding and Interaction","text":"","code":""},{"path":"confounding-and-interaction.html","id":"introduction-1","chapter":"16 Confounding and Interaction","heading":"16.1 Introduction","text":"new waiter employed bar serve ice glasses Accra made \nobservation seem irrational. realized customers served\nice often ended drunk. first accept make sense \nstrongly believe right. approached friend researcher \ndecided investigate . friend decided take data 510\nrandomly selected customers. data recorded drinks.txt.variables collected include id sequentially allocated study id, sex,\nsex customer, liquor whether customer took alcoholic liquor\n(spirits, brandy, etc), ice whether customer served ice glass,\ndrunk whether customer ended drunk, food whether customer\nserved food age, age years customer. task now \ndetermine data ingesting ice associated drunk.","code":""},{"path":"confounding-and-interaction.html","id":"effect-and-effect-size","chapter":"16 Confounding and Interaction","heading":"16.2 Effect and effect size","text":"","code":""},{"path":"confounding-and-interaction.html","id":"effect","chapter":"16 Confounding and Interaction","heading":"16.2.1 Effect","text":"begin discuss problem first look \nstatistical terms meant effect. effect defined change\noccurs consequence action. statistical terms, effect \nusually change one variable another. instance, effect \nmean change blood pressure (effect) taking drug (action). Two \ncommon ways expressing effect categorical data analysis \nodds ratio () Relative Risk (RR). two usually determine effect \ncomparing odds risk two groups expressing effect \nratios.","code":""},{"path":"confounding-and-interaction.html","id":"effect-size","chapter":"16 Confounding and Interaction","heading":"16.2.2 Effect size","text":"suffice report effect also size. People \nwant know intervention makes difference also much\ndifference makes. Effect size can opposite direction. instance, \ndifference blood pressure taking medication may positive\n(pressure taking drug taking) negative\n(pressure becomes less drug taken). hand, effects\ncan side difference magnitude. 5.1 much\nhigher 1.5 despite indicating increased odds outcome.\ngoing apply notion determining confounders effect\nmodifiers.Now back waiter’s issue. First, read data.summarize dataIn section, make use two main functions cc() mhor() \nepicalc package. First, determine effect ice concerning\ndrunk using odds ratio.appears waiter right! Taking ice associated significantly\nhigher odds getting drunk (: 1.73, 95% CI: 1.17 2.55, p<.001). \nagree defies logic. question : arise?","code":"\ndf_drinks <- \n    read.table(\"./Data/drinks.txt\", header=T, sep=\"\\t\") %>% \n    mutate(\n        across(\n            c(liquor, ice, drunk, food), \n            ~factor(.x, levels = c(0,1), labels = c(\"No\",\"Yes\"))\n        ),\n        sex = factor(sex)\n    )options(huxtable.knit_print_df = FALSE)\ndf_drinks %>% \n    summarytools::dfSummary(graph.col = F)\nData Frame Summary  \ndf_drinks  \nDimensions: 510 x 7  \nDuplicates: 0  \n\n---------------------------------------------------------------------------------------\nNo   Variable    Stats / Values              Freqs (% of Valid)    Valid      Missing  \n---- ----------- --------------------------- --------------------- ---------- ---------\n1    id          Mean (sd) : 255.5 (147.4)   510 distinct values   510        0        \n     [integer]   min < med < max:            (Integer sequence)    (100.0%)   (0.0%)   \n                 1 < 255.5 < 510                                                       \n                 IQR (CV) : 254.5 (0.6)                                                \n\n2    sex         1. Female                   256 (50.2%)           510        0        \n     [factor]    2. Male                     254 (49.8%)           (100.0%)   (0.0%)   \n\n3    liquor      1. No                       155 (30.4%)           510        0        \n     [factor]    2. Yes                      355 (69.6%)           (100.0%)   (0.0%)   \n\n4    ice         1. No                       191 (37.5%)           510        0        \n     [factor]    2. Yes                      319 (62.5%)           (100.0%)   (0.0%)   \n\n5    drunk       1. No                       186 (36.5%)           510        0        \n     [factor]    2. Yes                      324 (63.5%)           (100.0%)   (0.0%)   \n\n6    age         Mean (sd) : 35.8 (5.1)      31 distinct values    510        0        \n     [integer]   min < med < max:                                  (100.0%)   (0.0%)   \n                 19 < 36 < 51                                                          \n                 IQR (CV) : 7 (0.1)                                                    \n\n7    food        1. No                       288 (56.5%)           510        0        \n     [factor]    2. Yes                      222 (43.5%)           (100.0%)   (0.0%)   \n---------------------------------------------------------------------------------------\n\noptions(huxtable.knit_print_df = TRUE)\ndf_drinks %$% epiDisplay::cc(drunk, ice)\n       ice\ndrunk    No Yes Total\n  No     85 101   186\n  Yes   106 218   324\n  Total 191 319   510\n\nOR =  1.73 \n95% CI =  1.2, 2.51  \nChi-squared = 8.5, 1 d.f., P value = 0.004\nFisher's exact test (2-sided) P value = 0.004 "},{"path":"confounding-and-interaction.html","id":"confounding","chapter":"16 Confounding and Interaction","heading":"16.2.3 Confounding","text":"research often arises situation observed effect \nvariable tends depend another. instance, case, effect ice\nmaking customers drunk associated another item.\ncustomers took ice also took hard liquor time \nmay make seem taking ice associated drunk. \ntypical case confounding liquor ingestion\nconfounding effect ice.variable confounder must meet basic propertiesThe confounder must related exposure (ice) variableThe confounder must related outcome (drunk) variableThe confounder causal pathway exposure(ice)\noutcome (drunk).first two conditions easily tested statistically last can \ntested prior knowledge, often coming epidemiological scientific\nfacts. investigator bar study suspects liquor confounder \neffect ice sets determine . First, looks \nrelationship suspected confounder outcomeHaving liquor strongly associated drunk \n= 2.41 (95%CI: 1.6, 3.62, p<.001). Next, test association \nexposure possible confounder.strong association ice taking hard liquor\n(: 13.71, 95% CI: 8.48 22.33, p<.001). relationship established\nfirst two conditions stated fulfilled.next task determine consumption liquor confounder \neffect ice. determining adjusted comparing \ncrude (unadjusted ). recall crude 1.73\n(95% CI = 1.17 2.55, p=0.004). Mantel-Haenszel odd ratios determined \nmhor() function adjusted possible confounder. shown\nbelowThe function first stratifies odds getting drunk taking ice \nindividual groups possible confounding variable. determines\nlevel confounder, 1.14(95%CI= 0.621 2.05, p=0.666) \ntook liquor 1.13 (95%CI: 0.497 2.58, p=0.848) \n. reports MH combined (adjusted) 1.14 (95%CI: 0.726 \n1.78, p=0.574). Thus stratification adjustment reduced effect \nice significant 1.7 non-significant relationship 1.1. effect \ndisappeared ice longer significantly associated getting\ndrunk adjustment. shows ice cause people \ndrunk observed effect just issue confounding \nconsumption liquor.shown relationship three variables involved \ndemonstrating confounding effect liquor effect taking ice. can\nask: ice confounder relationship drunk \nconsumption liquor? Remember crude effect 2.41 (95%CI:\n1.6, 3.62, p<.001). answer question determine adjusted .output shows adjusted (=2.24, 95%CI: 1.41 3.56, p<.001) \nbarely different crude also retains significant effect. \nconsumption ice therefore confounder effect hard liquor.summary determine variable confounder need determine \nassociation exposure outcome variable. association\ndetermine crude effect adjusted effect. two \nsignificantly different can conclude presence confounder.However, definition “significant effect” always difficult determine.\nsay change least 10% crude effect can considered enough.\nWhenever possible confounder adjusted effect reported.\nAlso, worth noting effect change making effect higher \nlower. confounder instance results crude 2.4 \nadjusted 1.5 positive confounder one crude \n1.5 adjusted 2.5 shows negative confounding.","code":"\ndf_drinks %$% epiDisplay::cc(drunk, liquor)\n       liquor\ndrunk    No Yes Total\n  No     79 107   186\n  Yes    76 248   324\n  Total 155 355   510\n\nOR =  2.41 \n95% CI =  1.63, 3.55  \nChi-squared = 20.2, 1 d.f., P value = 0\nFisher's exact test (2-sided) P value = 0 df_drinks %$% epiDisplay::cc(ice, liquor, graph = F)\n\n       liquor\nice      No Yes Total\n  No    120  71   191\n  Yes    35 284   319\n  Total 155 355   510\n\nOR =  13.71 \n95% CI =  8.68, 21.67  \nChi-squared = 151.85, 1 d.f., P value = 0\nFisher's exact test (2-sided) P value = 0 df_drinks %$% epiDisplay::mhor(drunk, ice, liquor)\n\nStratified analysis by  liquor \n               OR lower lim. upper lim. P value\nliquor No    1.13      0.497       2.58   0.848\nliquor Yes   1.14      0.621       2.05   0.666\nM-H combined 1.14      0.726       1.78   0.574\n\nM-H Chi2(1) = 0.32 , P value = 0.574 \nHomogeneity test, chi-squared 1 d.f. = 0 , P value = 0.986 df_drinks %$% epiDisplay::mhor(drunk, liquor, ice)\n\nStratified analysis by  ice \n               OR lower lim. upper lim.  P value\nice No       2.22       1.16       4.33 0.010714\nice Yes      2.24       1.03       4.86 0.032828\nM-H combined 2.24       1.41       3.56 0.000589\n\nM-H Chi2(1) = 11.81 , P value = 0.001 \nHomogeneity test, chi-squared 1 d.f. = 0 , P value = 0.981 "},{"path":"confounding-and-interaction.html","id":"interaction-or-effect-modification","chapter":"16 Confounding and Interaction","heading":"16.3 Interaction or Effect modification","text":"common knowledge among drink alcoholic beverages filling\ntummy food often delays getting drunk one consumes alcoholic\nbeverages. investigator bar data now wants determine \n. notion right getting drunk taking hard liquor\ndependent whether customer also ordered ate food \nwell. typical example effect modification (term often used \nepidemiologists) interaction (used statisticians). two essentially\nmeans . definition, effect modification occurs effect size \nexposure (liquor) outcome (drunk) differs depending level \nthird variable (eating food). occurs just computing reporting \noverall effect misleading.begin investigation food causing interaction effect \ndrinking hard liquor determining getting drunk one took \nfood one didn’t.begin ate food.statistically insignificant odds getting drunk taking hard\nliquor one eats well 1.41 (95% CI: 0.8 2.51, p=0.208).\nNext, eatThe getting drunk one takes liquor empty stomach much\nhigher statistically significant (: 3.38, 95% CI: 1.82 6.26, p<.001).\nseem quite substantial difference effect drinking liquor\ndepending food intake . sure food intake significant\neffect modifier subject formal test using mhor() function.stratified analysis output shows prior calculated stratified ORs\nwell combined Mantel-Haenszel . Homogeneity test tests \nnull hypothesis:H0: difference two stratified Odds ratios.p-value 0.031, therefore, indicates statistically significant difference\neffects () ate . , therefore, conclude\nsignificant interaction food intake getting drunk\none takes hard liquor.section, dealt detecting confounder interaction variable\ncategorical data analysis. just scratching surface. subsequent\nchapters, dealing lot using regression analysis.","code":"\ndf_drinks %>% \n    filter(food == \"Yes\") %$% \n    epiDisplay::cc(drunk, liquor)\n       liquor\ndrunk    No Yes Total\n  No     47  60   107\n  Yes    41  74   115\n  Total  88 134   222\n\nOR =  1.41 \n95% CI =  0.82, 2.43  \nChi-squared = 1.59, 1 d.f., P value = 0.208\nFisher's exact test (2-sided) P value = 0.219 \ndf_drinks %>% \n    filter(food == \"No\") %$% \n    epiDisplay::cc(drunk, liquor)\n       liquor\ndrunk    No Yes Total\n  No     32  47    79\n  Yes    35 174   209\n  Total  67 221   288\n\nOR =  3.38 \n95% CI =  1.9, 6.03  \nChi-squared = 18.13, 1 d.f., P value = 0\nFisher's exact test (2-sided) P value = 0 df_drinks %$% epiDisplay::mhor(drunk, liquor, food, graph = F)\n\nStratified analysis by  food \n               OR lower lim. upper lim.  P value\nfood No      3.37      1.817       6.26 4.16e-05\nfood Yes     1.41      0.795       2.51 2.19e-01\nM-H combined 2.08      1.406       3.09 1.78e-04\n\nM-H Chi2(1) = 14.05 , P value = 0 \nHomogeneity test, chi-squared 1 d.f. = 4.65 , P value = 0.031 "},{"path":"diagnostic-tests.html","id":"diagnostic-tests","chapter":"17 Diagnostic Tests","heading":"17 Diagnostic Tests","text":"Scientific testing presence various disease conditions processes\ncommon everyday life. range complex testing \npresence strange diseases newly manufactured electrical gadgets \ndefects. often Gold Standard test, one deemed \nperfectly determine presence absence condition. However, \nalways search alternative tests often cheaper easier\nuse compared Gold standard.study diagnose malaria children attending outpatient clinic Ghana, children clinical suspicion malaria tested using three\nmethods. First, blood film reported count malaria parasites\n(Gold standard) done. Two rapid diagnostic kits, called RDT.1 RDT.2\nalso done concurrently reported positive (1) negative (0). \ndone 100 patients recorded malaria.csv. task \nevaluate RDT.1’s ability accurately reliably diagnose malaria.First, read dataThe summary data shown belowAnd tabulate rdt.1 Gold Standard asThe table decomposes test results 4 distinct categories.RDT.1 gold standard positive (True positive) 50.group RDT.1 Gold standard negative (True Negative) 44.group showed positive RDT.1 results negative Gold standard (False positive) 2.Finally last group, whose RDT.1 results negative positive\njudging Gold standard (False negative) 4.operationalise extracting relevant portions table ","code":"\ndf_malaria <- \n    read_csv(\"./Data/malaria.txt\") %>% \n    mutate(\n        gold = ifelse(mps == 0, 0, 1) %>% \n            factor(levels = c(1,0),\n                   labels = c(\"Positive\", \"Negative\")),\n        across(\n            c(rdt.1, rdt.2), \n            ~factor(\n                .x, \n                levels = c(1,0),\n                labels = c(\"Positive\", \"Negative\"),\n            )\n        )\n    )df_malaria %>% summarytools::dfSummary(graph.col = F)\nData Frame Summary  \ndf_malaria  \nDimensions: 100 x 4  \nDuplicates: 44  \n\n-----------------------------------------------------------------------------------------\nNo   Variable    Stats / Values                 Freqs (% of Valid)   Valid      Missing  \n---- ----------- ------------------------------ -------------------- ---------- ---------\n1    mps         Mean (sd) : 3365.2 (23683.3)   53 distinct values   100        0        \n     [numeric]   min < med < max:                                    (100.0%)   (0.0%)   \n                 0 < 62.5 < 236155                                                       \n                 IQR (CV) : 413.8 (7)                                                    \n\n2    rdt.1       1. Positive                    52 (52.0%)           100        0        \n     [factor]    2. Negative                    48 (48.0%)           (100.0%)   (0.0%)   \n\n3    rdt.2       1. Positive                    51 (51.0%)           100        0        \n     [factor]    2. Negative                    49 (49.0%)           (100.0%)   (0.0%)   \n\n4    gold        1. Positive                    54 (54.0%)           100        0        \n     [factor]    2. Negative                    46 (46.0%)           (100.0%)   (0.0%)   \n-----------------------------------------------------------------------------------------\ndf_malaria %>% \n    gtsummary::tbl_cross(\n        col = gold,\n        row = rdt.1,\n        label = list(\n            gold ~ \"Gold Standard\",\n            rdt.1 ~ \"First RDT\"\n        )\n    ) %>% \n    gtsummary::bold_labels()\ntp <- 50\ntn <- 44\nfp <- 2\nfn <- 4"},{"path":"diagnostic-tests.html","id":"true-prevalence-of-the-disease","chapter":"17 Diagnostic Tests","heading":"17.1 True prevalence of the disease","text":"true prevalence disease proportion diseased individuals\nobserved study population determined gold standard. \nmathematically given \n\\[True~prevalence = \\frac{tp + fn}{tp + tn + fp + fn}\\]determined data ","code":"true.prevalence <- (tp+fn)/(tp+tn+fp+fn)\ntrue.prevalence\n[1] 0.54"},{"path":"diagnostic-tests.html","id":"apparent-prevalence-of-the-disease","chapter":"17 Diagnostic Tests","heading":"17.2 Apparent prevalence of the disease","text":"apparent prevalence disease proportion diseased\nindividuals observed study population determined RDT.1 test.\nmathematically given \n\\[Apparent~prevalence = \\frac{tp + fp}{tp + tn + fp + fn}\\]\ndetermined data ","code":"apparent.prevalence<-(tp+fp)/(tp+tn+fp+fn)\napparent.prevalence\n[1] 0.52"},{"path":"diagnostic-tests.html","id":"sensitivity-of-a-test","chapter":"17 Diagnostic Tests","heading":"17.3 Sensitivity of a test","text":"sensitivity test defines proportion individuals \ndisease correctly identified test applied. ranges 0, completely useless test 1, perfect test. Mathematically defined \\[Sensitivity = \\frac{tp}{tp + fn}\\]\ndetermined ","code":"sensitivity <- tp/(tp+fn)\nsensitivity\n[1] 0.9259259"},{"path":"diagnostic-tests.html","id":"specificity-of-a-test","chapter":"17 Diagnostic Tests","heading":"17.4 Specificity of a test","text":"specificity test defined proportion individuals without \ndisease correctly identified test used. ranges 0, \ncompletely useless test 1, perfect test. Mathematically defined \n\\[Specificity = \\frac{tn}{tn + fp}\\]determine ","code":"specificity<-tn/(tn+fp)\nspecificity\n[1] 0.9565217"},{"path":"diagnostic-tests.html","id":"predictive-value-of-a-test","chapter":"17 Diagnostic Tests","heading":"17.5 Predictive value of a test","text":"","code":""},{"path":"diagnostic-tests.html","id":"positive-predictive-value-of-a-test","chapter":"17 Diagnostic Tests","heading":"17.5.1 Positive predictive value of a test","text":"positive predictive value (PPV) test defined proportion \nindividuals positive test result disease. \nuseful measure compared sensitivity specificity indicates\nmuch weight one put positive test result confronted \none. Mathematically defined :\\[PPV = \\frac{tp}{tp + fp}\\]","code":"ppv <- tp/(tp+fp)\nppv\n[1] 0.9615385"},{"path":"diagnostic-tests.html","id":"negative-predictive-value-of-a-test","chapter":"17 Diagnostic Tests","heading":"17.5.2 Negative predictive value of a test","text":"negative predictive value (npv) test defined proportion \nindividuals negative test result disease. \nppv useful measure compared sensitivity \nspecificity indicates much weight one put negative test\nresult confronted one. Mathematically defined :\n\\[NPV = \\frac{tn}{tn + fn}\\]\ndetermined ","code":"npv <- tn/(tn+fn)\nnpv\n[1] 0.9166667"},{"path":"diagnostic-tests.html","id":"likelihood-ratio-of-a-test","chapter":"17 Diagnostic Tests","heading":"17.6 Likelihood ratio of a test","text":"likelihood ratio test another way expressing usefulness.\nUnlike previous statistics tests, likelihood ratios stretch beyond\n0 1. likelihood ratio 1 indicates useless (non-discriminatory) test.","code":""},{"path":"diagnostic-tests.html","id":"the-positive-likelihood-ratio-lr","chapter":"17 Diagnostic Tests","heading":"17.6.1 The Positive likelihood ratio (LR+)","text":"ratio chance positive result patient \ndisease chance positive result disease. \nhigher positive likelihood better test.mathematically equivalent \\[LR+ = \\frac{Sensitivity}{1-Specificity}\\]Applying data far ","code":"pLR <- sensitivity/(1-specificity)\npLR\n[1] 21.2963"},{"path":"diagnostic-tests.html","id":"negative-liklihood-ratio-lr-","chapter":"17 Diagnostic Tests","heading":"17.6.2 Negative liklihood ratio (LR-)","text":"negative likelihood ratio (LR-) hand ratio \nchance person negative result disease chance \nnegative result person disease. lower negative\nlikelihood better test.Computationally equivalent \n\\[LR+ = \\frac{1-Sensitivity}{Specificity}\\]\nApplying data far ","code":"nLR<-(1-sensitivity)/specificity\nnLR\n[1] 0.07744108"},{"path":"diagnostic-tests.html","id":"summary-2","chapter":"17 Diagnostic Tests","heading":"17.7 Summary","text":"Fortunately, can obtained one go using epi.tests() function\nepiRpackage. function however requires table formatted \nspecific way. create tableAnd evaluate testConclusion: high (0.9) Sensitivity, Specificity, PPV \nNPV, test appears good one. confirmed relatively\nhigh LR+ low LR-.","code":"table.test <- \n    df_malaria %$%\n    table(rdt.1, gold)\n\ntable.test\n          gold\nrdt.1      Positive Negative\n  Positive       50        2\n  Negative        4       44table.test %>% epiR::epi.tests()\n          Outcome +    Outcome -      Total\nTest +           50            2         52\nTest -            4           44         48\nTotal            54           46        100\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.52 (0.42, 0.62)\nTrue prevalence *                      0.54 (0.44, 0.64)\nSensitivity *                          0.93 (0.82, 0.98)\nSpecificity *                          0.96 (0.85, 0.99)\nPositive predictive value *            0.96 (0.87, 1.00)\nNegative predictive value *            0.92 (0.80, 0.98)\nPositive likelihood ratio              21.30 (5.48, 82.77)\nNegative likelihood ratio              0.08 (0.03, 0.20)\nFalse T+ proportion for true D- *      0.04 (0.01, 0.15)\nFalse T- proportion for true D+ *      0.07 (0.02, 0.18)\nFalse T+ proportion for T+ *           0.04 (0.00, 0.13)\nFalse T- proportion for T- *           0.08 (0.02, 0.20)\nCorrectly classified proportion *      0.94 (0.87, 0.98)\n--------------------------------------------------------------\n* Exact CIs"},{"path":"agreement.html","id":"agreement","chapter":"18 Agreement","heading":"18 Agreement","text":"","code":""},{"path":"agreement.html","id":"introduction-2","chapter":"18 Agreement","heading":"18.1 Introduction","text":"chapter deals methods evaluating agreement two different\nmeasurements, either done two different methods method done \ntwo different times. understand need define terms :Accuracy:Precision:Measurement variabilityMeasurement errorReliability:","code":""},{"path":"receiver-operating-characteristic.html","id":"receiver-operating-characteristic","chapter":"19 Receiver Operating Characteristic","heading":"19 Receiver Operating Characteristic","text":"now, dealt tests categorized either positive \nnegative. However, many tests quantitative rather qualitative. \ninstance, blood urea nitrogen, serum cholesterol serum protein among\nmany others measured continuous scale often ranging 0 infinity.\ntests pose different challenges often need cut-point \ndetermine range values can considered “normal” “abnormal”. \nlearned sensitivity specificity test often used \ndefine good . tests continuous scale sensitivity \nspecificity change depending cut-provided measure.section, use lbw.csv data collected study conducted \ncohort 350 newborns Ghana. identification babies born weight\nless 2.5 kg important special needs require. \nmany underdeveloped countries, however, unavailability reliable weighing\nscale makes challenge. prompted search surrogate\nmeasures easily available rural areas determining baby low birth\nweight (<2.5kgs). study aimed determine well length \nchest circumference baby used surrogate indicator low\nbirth weight newborns. turn good tests can easily \ndeployed rural area instrument needed \nmeasuring tape. variables collected include study ID (sid), birth\nweight (bweight), sex (gender), chest circumference (chc) length\nbaby (lgth).First, read data clearing workspaceAnd summarize itNext, introduce pROC package. function roc() package \nused extensively section.","code":"\ndf_lbw <- \n    read_csv(\"./Data/lbw.csv\") %>% \n    mutate(gender = factor(gender)) %>% \n    mutate(bwcat = ifelse(bweight < 2.5, 1, 0) %>% \n               factor(levels = c(0,1), labels = c(\"Normal\",\"Low\")))df_lbw %>% dfSummary(graph.col = F)\nData Frame Summary  \ndf_lbw  \nDimensions: 350 x 6  \nDuplicates: 0  \n\n---------------------------------------------------------------------------------------\nNo   Variable    Stats / Values              Freqs (% of Valid)    Valid      Missing  \n---- ----------- --------------------------- --------------------- ---------- ---------\n1    sid         Mean (sd) : 175.5 (101.2)   350 distinct values   350        0        \n     [numeric]   min < med < max:                                  (100.0%)   (0.0%)   \n                 1 < 175.5 < 350                                                       \n                 IQR (CV) : 174.5 (0.6)                                                \n\n2    bweight     Mean (sd) : 2.9 (0.6)       33 distinct values    350        0        \n     [numeric]   min < med < max:                                  (100.0%)   (0.0%)   \n                 1 < 2.9 < 4.5                                                         \n                 IQR (CV) : 0.7 (0.2)                                                  \n\n3    gender      1. Female                   166 (47.4%)           350        0        \n     [factor]    2. Male                     184 (52.6%)           (100.0%)   (0.0%)   \n\n4    chc         Mean (sd) : 31.7 (3.2)      115 distinct values   350        0        \n     [numeric]   min < med < max:                                  (100.0%)   (0.0%)   \n                 20 < 32 < 42.1                                                        \n                 IQR (CV) : 4 (0.1)                                                    \n\n5    lgth        Mean (sd) : 46.8 (4.3)      109 distinct values   350        0        \n     [numeric]   min < med < max:                                  (100.0%)   (0.0%)   \n                 28 < 47 < 60.3                                                        \n                 IQR (CV) : 5.5 (0.1)                                                  \n\n6    bwcat       1. Normal                   268 (76.6%)           350        0        \n     [factor]    2. Low                       82 (23.4%)           (100.0%)   (0.0%)   \n---------------------------------------------------------------------------------------"},{"path":"receiver-operating-characteristic.html","id":"sensitivity-specificity-and-cut-offs","chapter":"19 Receiver Operating Characteristic","heading":"19.1 Sensitivity, specificity and cut-offs","text":"mentioned one gold standard bivariate (indicating\npresence absence) quantitative test, sensitivity specificity \ntest depends cut-chosen. lbw.csv data gold standard\nlow birth weight baby birth weight categorised low birth\nweight (LBW) normal birth weight (NBW). Two continuous measures, chest\ncircumference length baby used tests.illustrate relationship various cut-offs sensitivity \nspecificity generate arbitrary cut-offs.cut-offs, generate categories length babies\ntabulate resultant categorical variable.count various groupsNext, determine sensitivities specificities various cut-offs \nusing roc() function pROC package. Since package requires\nordered categorical variable convert lgthcat ordered factor\nvariable. go ahead impute relevant information roc()\nfunction","code":"\ncut.off <- c(28, 42, 44, 46, 47, 49, 50, 51, 61)\ndf_lbw <- \n    df_lbw %>% \n    mutate(lgthcat = cut(lgth, br=cut.off, include.lowest=T))\ndf_lbw %>% \n    gtsummary::tbl_summary(\n        include = lgthcat,\n        digits = lgthcat ~ c(0,1)\n    ) %>% \n    gtsummary::bold_labels()"},{"path":"linear-regression.html","id":"linear-regression","chapter":"20 Linear Regression","heading":"20 Linear Regression","text":"","code":""},{"path":"linear-regression.html","id":"introduction-3","chapter":"20 Linear Regression","heading":"20.1 Introduction","text":"Regression plays key part modern-day statistics. R advanced regression\nfunctions includes; lm() (fitting linear models), glm() (fitting\ngeneralized linear models), nls() (fitting nonlinear models) coxph()\n(fitting Cox proportional hazards regression model).statistical analysis, often arises relationship two\ncontinuous variables expressed rate rise per unit .\ninstance, one can ask:average, many units rise blood hematocrit occurs unit rise blood haemoglobin content?Readers conversant may guess answer approximately 3.","code":""},{"path":"linear-regression.html","id":"regression-with-single-continuous-independent-variable","chapter":"20 Linear Regression","heading":"20.2 Regression with single continuous independent variable","text":"basic idea behind linear regression formula dependent\nvariable can derived independent variable(s). Simply put \ngiven one’s hb can predict person’s hct? come \nformula form\n\\[Y = aX + b\\]\n\\(Y\\) dependent variable (hct), \\(\\) slope straight line\ndraw points, \\(X\\) independent variable (hb) \\(b\\) \nintercept value \\(Y\\) \\(\\) 0.first logical step regression analysis plot scatter diagram \nvisualize relationship two variables. plot hct versus\nhb shown importing dataAnd summarize data belowWe now derive constants lm() function .Table 6.3:  table gives us values intercept (b) 1.625 slope\n() 2.778. can thus say based data increase \napproximately 2.788 hematocrit unit rise Hb. Also, hct \nperson Hb 0 (anything like ) 1.625. However, \nnever good idea extrapolate formula beyond range data \nhand. instance, predicting hct anyone hb 200 \nstatistically common sensically right.Conclusion: unit rise blood hb hematocrit rises 2.8%\nsample. Extrapolating onto general population 95% certain\nrate rise fall 2.6% 2.9%. observed slope \nsignificantly different slope 0. evident small p-value\n(<.001).","code":"\ndf_blood <- read_csv(\"./Data/blood.csv\")df_blood %>% summarytools::dfSummary(graph.col = F)\nData Frame Summary  \ndf_blood  \nDimensions: 50 x 7  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------\nNo   Variable    Stats / Values              Freqs (% of Valid)   Valid      Missing  \n---- ----------- --------------------------- -------------------- ---------- ---------\n1    stno        Mean (sd) : 1025.5 (14.6)   50 distinct values   50         0        \n     [numeric]   min < med < max:                                 (100.0%)   (0.0%)   \n                 1001 < 1025.5 < 1050                                                 \n                 IQR (CV) : 24.5 (0)                                                  \n\n2    age         Mean (sd) : 122.4 (30.7)    19 distinct values   50         0        \n     [numeric]   min < med < max:                                 (100.0%)   (0.0%)   \n                 72 < 132 < 180                                                       \n                 IQR (CV) : 48 (0.3)                                                  \n\n3    wgt         Mean (sd) : 27.4 (7.7)      42 distinct values   50         0        \n     [numeric]   min < med < max:                                 (100.0%)   (0.0%)   \n                 17 < 26.3 < 59.5                                                     \n                 IQR (CV) : 7.9 (0.3)                                                 \n\n4    hgt         Mean (sd) : 130.5 (11.4)    27 distinct values   50         0        \n     [numeric]   min < med < max:                                 (100.0%)   (0.0%)   \n                 107 < 130.5 < 155                                                    \n                 IQR (CV) : 15 (0.1)                                                  \n\n5    hb          Mean (sd) : 8.2 (1.8)       36 distinct values   50         0        \n     [numeric]   min < med < max:                                 (100.0%)   (0.0%)   \n                 5.3 < 7.7 < 12                                                       \n                 IQR (CV) : 2.6 (0.2)                                                 \n\n6    wbc         Mean (sd) : 13.9 (8.8)      44 distinct values   50         0        \n     [numeric]   min < med < max:                                 (100.0%)   (0.0%)   \n                 4.8 < 11.4 < 62.6                                                    \n                 IQR (CV) : 5.7 (0.6)                                                 \n\n7    hct         Mean (sd) : 24.4 (5.1)      45 distinct values   50         0        \n     [numeric]   min < med < max:                                 (100.0%)   (0.0%)   \n                 15.7 < 23 < 35                                                       \n                 IQR (CV) : 7.4 (0.2)                                                 \n--------------------------------------------------------------------------------------\ndf_blood %>% \n    ggplot(aes(x = hb, y = hct)) +\n    geom_point(color = \"red\",)+\n    geom_smooth(method = \"lm\", formula = y~x, color = \"skyblue\", se = F)+\n    labs(x = \"Hemoglobin (mg/dl)\", y = \"Hematocrit (%)\") +\n    theme_bw()\ndf_blood %>% \n    lm(hct ~ hb, data=.) %>% \n    broom::tidy(conf.int=T)"},{"path":"linear-regression.html","id":"regression-with-single-categorical-independent-variable","chapter":"20 Linear Regression","heading":"20.3 Regression with single categorical independent variable","text":"Previously implemented linear regression one continuous independent\nvariable. implement linear regression categorical independent\nvariable. blood data set, categorical variable \ngenerate one categorising high wbc (>11.0 x 109/ml) “High” others “Low”.Table 7.4:  Next, implement linear regressionTable 6.4:  interpretation slightly different single continuous\nvariable.estimate (Intercept) mean hct lower level \nwbc.cat .e. wbc categorised “Low”. mean hct \n“Low” wbc therefore 27.055.coefficient wbc.catHigh difference mean hct \n“High” “Low” wbc.cat categories. Therefore difference mean hct\n“High” “Low” wbcs -4.705.mean hct “High” wbc.cat therefore\n(Intercept) plus coefficient wbc.catHigh. \n\\(27.055 + (-4:405) = 22:35\\).comparative purposes, perform analysis t.test() function :Table 6.5:  close look shows two analyses produce identical results. t-statistic,\np-value, 95% confidence interval difference sample estimates \nidentical. question arises: can easily derive \nt.test() function go hassle fitting linear regression\nmodel? reason regression opens whole new world statistics without\nmany manipulations difficult impossible achieve.Conclusion: mean hct “Low” wbc.cat 27.0 95%\nconfidence interval (25.1 29.0). difference hct \n“High” “Low” wbc -4.7 95% confidence interval (-7.3 -2.1).\ndifference two means significantly different 0 \np-value = 0.001.","code":"\ndf_blood <-\n    df_blood %>% \n    mutate(wbc.cat = case_when(wbc > 11 ~ \"High\", TRUE ~ \"Low\") %>% \n               factor(levels=c(\"Low\",\"High\")))\n\ndf_blood %>% head()\ndf_blood %>% \n    lm(hct ~ wbc.cat, data=.) %>% \n    broom::tidy(conf.int=T)\ndf_blood %$% \n    t.test(hct ~ wbc.cat, var.equal=TRUE) %>% \n    broom::tidy()"},{"path":"linear-regression.html","id":"regression-with-two-continuous-independent-variables","chapter":"20 Linear Regression","heading":"20.4 Regression with two continuous independent variables","text":"Previously dealt identifying reporting confounder \nconfounding categorical data analysis. effect used odds\nratio. subsection, deal confounding linear regression using\nregression coefficient often represented \\(\\beta\\) effect.Previously identified significant linear relationship \nblood hb hct. illustrative purposes, present show just\ncoefficients, confidence interval p-values.Table 6.6:  output shows significant relationship hb hct\n(\\(\\beta\\) = 2.778, 95%CI: 2.644 2.932, p=<.001). Previously, learned \nconfounder related outcome exposure variable. \npreliminary analysis, decided see wbc also related hct.\ndone belowTable 6.7:  Interestingly appears . seems significant reduction wbc\ncount increasing hct. question arises:effect wbc confounded hb?begin investigate must first check wbc relationship \nhb well. done belowTable 6.8:  result indicates significant reduction wbc increasing hb.\nresult, demonstrated relationship three variables\nnecessary confounding exist.Next, determine adjusted coefficients comparison crude ones.\nput variables independent variables linear regression\nformula belowTable 6.9:  coefficients generated (hb wbc) adjusted . First\ncompare crude effect hb (\\(\\beta\\) = 2.778, 95%CI: 2.644 2.932, p<.001)\nadjusted effect (\\(\\beta\\) = 2.783, 95%CI: 2.621 2.946, p<.001). \nobvious literally confounding relationship hb hct\nwbc coefficient remains literally .Next, compare crude relationship wbc hct (\\(\\beta\\) = -0.255,\n95%CI: -0.405 -0.104, p=0.001) adjusted relationship (\\(\\beta\\) = -0.002,\n95%CI: -0.035 0.031, p=0.895). observe significant change\nsignificant negative crude relationship literally relationship\n. Thus conclude hb confounder relationship\nhct wbc.","code":"\ndf_blood %>% lm(hct ~ hb, data=.) %>% broom::tidy(conf.int=T) \ndf_blood %>% lm(hct ~ wbc, data=.) %>% broom::tidy(conf.int=T) \ndf_blood %>% lm(wbc ~ hb, data=.) %>% broom::tidy(conf.int=T) \ndf_blood %>% lm(hct ~ hb + wbc, data=.) %>% broom::tidy(conf.int=T) "},{"path":"linear-regression.html","id":"regression-with-continuous-and-categorical-independent-variables","chapter":"20 Linear Regression","heading":"20.4.1 Regression with continuous and categorical independent variables","text":"section perform linear regression involving three variables; hct,\nhb wbc.cat. type linear regression done answer questions like:rate change hct unit rise hb \n“High” “Low” wbc.cat?answer question perform linear regression :Table 6.10:  regression output three coefficients. First (intercept), \napparent value hct “Low” wbc Hb 0. determine \nvalue intercept persons High wbc add intercept term\ncoefficient wbc.catHigh.lines, slope coefficient hb. Thus intercept \n“Low” wbc 2.300622, High wbc \\(2.300622 + -0.435617 = 1.865005\\)\ncommon slope lines 2.735240. represented graphically\nbelowConclusion: data, hct persons High WBC count \n0.4% lower Low WBC assuming two rise rate \ngroups. difference however statistically significantly different\n0 (p = 0.128). Thus enough evidence \ndifference levels hct depending level one’s WBC.","code":"\ndf_blood %>% lm(hct ~ hb + wbc.cat, data=.) %>% broom::tidy(conf.int=T) \ndf_blood %>% \n    ggplot() +\n    geom_point(aes(x = hb, y = hct, col = wbc.cat))+\n    geom_abline(\n        aes(intercept = 2.300622, slope = 2.735240, col = \"Low\"), \n        show.legend = T)+\n    geom_abline(\n        aes(intercept = 2.300622 + -0.435617, slope = 2.735240, col = \"High\"), \n        show.legend = T) +\n    labs(x = \"Hemoglobin (mg/dl)\", y = \"Hematocrit (%)\")+\n    scale_color_manual(\n        name = \"WBC Category\", \n        values = c(\"Low\" = \"red\", \"High\" = \"blue\")) +\n    theme_bw()"},{"path":"linear-regression.html","id":"regression-with-continuous-and-categorical-independent-variables-with-interaction","chapter":"20 Linear Regression","heading":"20.5 Regression with continuous and categorical independent variables with interaction","text":"analysis assumed slopes two lines .\nHowever, usually case. determine individual slopes well\nintercepts need linear regression interaction term.\nanalysis, seek answer question:rate rise hct every rise hb significantly different \npersons High compared Low WBC?.Fitting linear regression done R .Table 13.1:  Four coefficients generated. (Intercept) intercept “Low”\nwbc group. hb coefficient represents slope line representing\n“Low” wbc. intercept “High” WBC sum (Intercept)\nwbc.catHigh (\\(1.93925961 + 0.19624236 = 2.135502\\)). Finally, slope \nregression line “High” wbc sum coefficients hb \nhb:wbc.catHigh (\\(2.77516971 + -0.07604741 = 2.699122\\)).graphically shown belowIt obvious slopes (rate rise hct concerninga unit rise hb) different two groups. High WBC\ngroup lower. However, decrease rate rise real effect just\nchance observation? answer question need use inferential\nstatistics.now concern line hb:wbc.catHigh indicates \ndifference slopes two. appears slope “High” wbc\ngroup -0.076 (95%CI: -0.394 0.242, p=0.633) less “Low” wbc\ngroup. difference however statistically significant.Conclusion: confidence interval contains null value 0 \nprobability difference arisen chance relatively high\n(0.633), conclude significant evidence difference \nslopes “High” “Low” WBCs.","code":"\ndf_blood %>% lm(hct ~ hb*wbc.cat, data=.) %>% broom::tidy(conf.int=T) df_blood %>% \n    ggplot(aes(x = hb, y = hct, color = wbc.cat))+\n    geom_point()+\n    geom_smooth(formula = y~x, se=FALSE, method = lm, size = .5)+\n    theme_bw()+\n    labs(x = \"Hemoglobin (mg/dl)\", y = \"Hematocrit (%)\")+\n    scale_color_manual(\n        name = \"WBC Category\", \n        values = c(\"Low\" = \"red\", \"High\" = \"blue\")) \nWarning: Using `size` aesthetic for lines was deprecated in ggplot2\n3.4.0.\nℹ Please use `linewidth` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where\nthis warning was generated."},{"path":"linear-regression.html","id":"assumptions","chapter":"20 Linear Regression","heading":"20.6 Assumptions","text":"","code":""},{"path":"logistic-regression.html","id":"logistic-regression","chapter":"21 Logistic Regression","heading":"21 Logistic Regression","text":"now, dealt linear regression requires continuous\ndependent variable. However research, especially medical research, lots \noutcome variables binary disease present absent, death \nsurvival cured cured. Modelling binary outcome data usually requires\nlogistic regression done R using glm() function \nfamily specified binomial.section, go back ANCdata used previously.summarize ","code":"\ndf_anc <- \n    readstata13::read.dta13(\".\\\\Data\\\\ANCdata.dta\")df_anc %>% summarytools::dfSummary(graph.col = F)\nData Frame Summary  \ndf_anc  \nDimensions: 755 x 3  \nDuplicates: 747  \n\n--------------------------------------------------------------------------\nNo   Variable   Stats / Values   Freqs (% of Valid)   Valid      Missing  \n---- ---------- ---------------- -------------------- ---------- ---------\n1    death      1. no            689 (91.3%)          755        0        \n     [factor]   2. yes            66 ( 8.7%)          (100.0%)   (0.0%)   \n\n2    anc        1. old           419 (55.5%)          755        0        \n     [factor]   2. new           336 (44.5%)          (100.0%)   (0.0%)   \n\n3    clinic     1. A             497 (65.8%)          755        0        \n     [factor]   2. B             258 (34.2%)          (100.0%)   (0.0%)   \n--------------------------------------------------------------------------"},{"path":"logistic-regression.html","id":"logistic-regression-with-a-single-binary-predictor","chapter":"21 Logistic Regression","heading":"21.1 Logistic regression with a single binary predictor","text":"mission determine relationship anc (anc) type used \nmanaging pregnant women outcome pregnancy (death). answer \nquestion run logistic regression model simplest form belowTable 6.2:  object results glm() model class glm lm. lm also\nused linear modelling using lm() function.","code":"\ndf_anc %>% \n    glm(death ~ anc, family=binomial, data=.) %>% \n    broom::tidy()"},{"path":"analysis-of-variance.html","id":"analysis-of-variance","chapter":"22 Analysis of Variance","heading":"22 Analysis of Variance","text":"","code":""},{"path":"analysis-of-variance.html","id":"introduction-4","chapter":"22 Analysis of Variance","heading":"22.1 Introduction","text":"Hypertension, elevated blood pressure beyond normal, common medical\ncondition, especially adults. treat condition, persons hypertension\noften given blood pressure-lowering drugs. many types drug,\nvarying blood pressure-lowering effects taken. study identify\nblood pressure-lowering effect, four drugs, called , B, C D\nrandomly administered persons hypertension.\nsystolic blood pressure (SBP) measured administration\ndrugs.drop SBP recorded. data systolic.txt captured \nstudy. variables data include drug, drug administered 1, 2,\n3, 4 representing drugs , B, C D, respectively. variables \ndata disease representing diseases present patient 1,\n2, 3 representing Asthma, Diabetes Mellitus (DM) Obesity, systolic\nrepresenting drop systolic blood pressure week starting drug.first read data summarise .data analyst, investigator asks determine data \ndrug highest blood pressure-lowering effect best worst\nperforming drugs. Recollect two drugs \nlikely performing Student t-test .However, four categories answer questions \nt-test. Analysis Variance (ANOVA) simplest form\nkicks . First, visualize distribution SBP-lowering effect \nfour drugs boxplot :Summarizing change SBP drugs administered haveTable 6.2:  figure summary obvious drug greatest\nlowering effect appears , followed B, D C order. \nknow can say sure B better , turn better\nD etc? answer question perform ANOVA.","code":"\ndf_syst <- \n    read.table(\".\\\\Data\\\\systolic.txt\") %>% \n    mutate(\n        drug = factor(drug, levels = 1:4, labels = c(\"A\", \"B\", \"C\", \"D\")),\n        disease = factor(\n            disease, levels = 1:3, \n            labels = c(\"Asthma\", \"DM\", \"Obesity\")\n        )\n    )df_syst %>% summarytools::dfSummary(graph.col = F)\nData Frame Summary  \ndf_syst  \nDimensions: 58 x 3  \nDuplicates: 3  \n\n------------------------------------------------------------------------------------\nNo   Variable    Stats / Values            Freqs (% of Valid)   Valid      Missing  \n---- ----------- ------------------------- -------------------- ---------- ---------\n1    drug        1. A                      15 (25.9%)           58         0        \n     [factor]    2. B                      15 (25.9%)           (100.0%)   (0.0%)   \n                 3. C                      12 (20.7%)                               \n                 4. D                      16 (27.6%)                               \n\n2    disease     1. Asthma                 19 (32.8%)           58         0        \n     [factor]    2. DM                     19 (32.8%)           (100.0%)   (0.0%)   \n                 3. Obesity                20 (34.5%)                               \n\n3    systolic    Mean (sd) : 18.9 (12.8)   32 distinct values   58         0        \n     [integer]   min < med < max:                               (100.0%)   (0.0%)   \n                 -6 < 21 < 44                                                       \n                 IQR (CV) : 19 (0.7)                                                \n------------------------------------------------------------------------------------\ndf_syst %>% \n    ggplot(aes(x = drug, y = systolic, color = drug)) +\n    geom_boxplot()+\n    labs(title = \"Decrease in systolic BP for the various drugs\",\n         x = \"Drug\", color = \"Drug\", \n         y = \"Change in SBP (mmHg)\")+\n    theme_bw()\ndf_syst %>% \n    group_by(drug) %>% \n    rstatix::get_summary_stats(type = \"common\")"},{"path":"analysis-of-variance.html","id":"one-way-analysis-of-variance","chapter":"22 Analysis of Variance","heading":"22.2 One-way analysis of variance","text":"analysis variance parametric test used compare means \ntwo groups. tests null hypothesisH0: population means groups .\nHa: least one population means differ rest.proper ANOVA summarise change SBP different drugs,\npresenting data confidence interval mean.apparent confidence intervals mean change systolic\nblood pressures effect drug quite similar B.\nAlso, B quite different C D. C D hand\nsimilar effects. preliminary finding back minds, \nfit linear regression model .Table 6.3:  ANOVA output p-value order 0.000057! \nsmall indicating least one means significantly differs \nrest.","code":"\ndf_syst %>% \n    aov(systolic ~ drug, data = .) %>% \n    broom::tidy()"},{"path":"analysis-of-variance.html","id":"postestimation-pairwise-comparison","chapter":"22 Analysis of Variance","heading":"22.2.1 Postestimation pairwise comparison","text":"made point ANOVA tell us least one means \nsignificantly different others. pick drug’s mean effect(s)\ndiffer others use post-estimation tests. example \nPairwise t-test.don’t just multiple T-tests present results ? \nt-test designed . significance level \nset single comparisons multiple meaning p-values obtained \ncomparison invalid. Many authors come ways \ncorrecting p-values common one implemented R listed . \nfollowing extract help page functionp.adjust().“adjustment methods include Bonferroni correction (”Bonferroni”) \np-values multiplied number comparisons. Less conservative cor-\nreactions also included Holm (1979) (“Holm”), Hochberg (1988) (“Hochberg”),\nHommel (1988) (“hommel”), Benjamini & Hochberg (1995) (“BH” alias “fdr”),\nBenjamini & Yekutieli (2001) (“”), respectively. pass-option\n(“none”) also included.”determine p-values multiple comparisons various drugs\nusing “holm” methodTable 7.4:  analysis output generated p-values pairwise comparison \nmeans decrease SBP various drugs. can inferred \ndifference reduction B significant (p-value: 0.89214).\nwords, reduction obtained two drugs can said \ncomparable. However, significant difference C, \nD, B C, B D. Also isn’t significant difference C\nD (p-value: 0.50216). arguably informative way showing \npairwise differences can obtained R use Tukey’s\npost-estimation test.Table 6.4:  output shows pairwise differences, 95%CI differences\nTukey’s adjusted p-values. Comparing two methods, Holms Tukey\nobserve multiple comparison p-values differ slightly. However, \nconclusions remain .Conclusion: drugs highest SBP lowering effect B \nevidence one better . C D hand \nsignificantly smaller SBP lowering effects compared B. Drugs C D\ndon’t perform significantly differently .","code":"\ndf_syst %$% \n    pairwise.t.test(\n        x = systolic, g = drug, data = ., p.adjust.methods = \"holm\"\n        ) %>% \n    broom::tidy()\ndf_syst %>% \n    aov(systolic ~ drug, data = .) %>% \n    TukeyHSD() %>% \n    broom::tidy()"},{"path":"analysis-of-variance.html","id":"two-way-analysis-of-variance","chapter":"22 Analysis of Variance","heading":"22.3 Two-Way Analysis of Variance","text":"data, two possible independent variables may explain BP-lowering effect obtained. disease drug administered. Two-way analysis variance, primarily come three hypotheses:H0: population means took various drugs .H0: population means various diseases .H0: significant interaction drug taken disease study participant determining mean BP change.evaluate hypothesis, perform two-way analysis variance interaction.Table 6.5:  analysis output indicates significant difference means various drugs various diseases present study subjects. Also, significant interaction drug disease type. visual look effect, plot means :plot indicates systolic blood pressure among groups disease conditions seem vary significantly drug given. reflected high p-value interaction drug disease.","code":"\ndf_syst %>% \n    aov(systolic ~ drug*disease, data = .) %>% \n    broom::tidy()\ndf_syst %>% \n    aov(systolic ~ drug*disease, data = .) %>% \n    ggeffects::ggeffect(terms = c(\"drug\",\"disease\")) %>% \n    plot()"},{"path":"analysis-of-variance.html","id":"repeated-measure-analysis-of-variance","chapter":"22 Analysis of Variance","heading":"22.4 Repeated measure analysis of variance","text":"Often research, investigator desires determine change outcome repeated measures, often specific time interval. involves measuring outcome study participant multiple times. Since measurements independent, methodology analysis described . two measurements made, paired T-test suffices use . however, 2 measurements study participant. repeated measure used.subsection, use data involves set hypertension patients, whose blood pressures measured every 2 months year.Next, select required variablesAnd view dataNext, need convert data long format view first 6 records.Table 13.1:  first summarize blood pressures four periods measurement.Table 13.2:  observe differences four means. , show graphically.may insignificant drop BP period. Next, perform repeated measure ANOVA determine differences means various review periods.Table 13.4:  ","code":"\nset.seed(7)\ndf_sbp <- \n    readxl::read_xlsx(\"./Data/sbp_longitudinal.xlsx\") %>% \n    sample_n(50)\ndf_sbp_selected <-\n    df_sbp %>%  \n    select(sid, sbp0, sbp4, sbp8, sbp12)df_sbp_selected %>% summarytools::dfSummary()\nData Frame Summary  \ndf_sbp_selected  \nDimensions: 50 x 5  \nDuplicates: 0  \n\n-----------------------------------------------------------------------------------------------------------\nNo   Variable      Stats / Values             Freqs (% of Valid)   Graph               Valid      Missing  \n---- ------------- -------------------------- -------------------- ------------------- ---------- ---------\n1    sid           1. AA0991KAB                1 ( 2.0%)                               50         0        \n     [character]   2. AB1053KAA                1 ( 2.0%)                               (100.0%)   (0.0%)   \n                   3. AC0162AGB                1 ( 2.0%)                                                   \n                   4. AD0510KAB                1 ( 2.0%)                                                   \n                   5. AD1208KAB                1 ( 2.0%)                                                   \n                   6. AF0129APA                1 ( 2.0%)                                                   \n                   7. AF0217KAB                1 ( 2.0%)                                                   \n                   8. AF0347TTB                1 ( 2.0%)                                                   \n                   9. AH0084TTB                1 ( 2.0%)                                                   \n                   10. AJ0478APB               1 ( 2.0%)                                                   \n                   [ 40 others ]              40 (80.0%)           IIIIIIIIIIIIIIII                        \n\n2    sbp0          Mean (sd) : 141.3 (20)     28 distinct values       :               50         0        \n     [numeric]     min < med < max:                                    :   :           (100.0%)   (0.0%)   \n                   106 < 139 < 189                                     : . :                               \n                   IQR (CV) : 25.2 (0.1)                             : : : : . :                           \n                                                                   : : : : : : : : :                       \n\n3    sbp4          Mean (sd) : 141.3 (27.1)   35 distinct values     :                 50         0        \n     [numeric]     min < med < max:                                : : .               (100.0%)   (0.0%)   \n                   100 < 136.5 < 215                               : : :                                   \n                   IQR (CV) : 34 (0.2)                             : : : :                                 \n                                                                   : : : : . :                             \n\n4    sbp8          Mean (sd) : 145.2 (32.1)   37 distinct values     :                 50         0        \n     [numeric]     min < med < max:                                . :                 (100.0%)   (0.0%)   \n                   101 < 138.5 < 277                               : : :                                   \n                   IQR (CV) : 38.5 (0.2)                           : : : .                                 \n                                                                   : : : : :   .   .                       \n\n5    sbp12         Mean (sd) : 138.5 (17.9)   37 distinct values     :                 50         0        \n     [numeric]     min < med < max:                                . : : .             (100.0%)   (0.0%)   \n                   110 < 136 < 179                                 : : : :                                 \n                   IQR (CV) : 23.5 (0.1)                           : : : : . :                             \n                                                                   : : : : : : :                           \n-----------------------------------------------------------------------------------------------------------\ndf_sbp_long <- \n    df_sbp_selected %>% \n    pivot_longer(\n        cols = sbp0:sbp12, values_to = \"bp\", names_to = \"period\") %>% \n    mutate(\n        period = factor(period, levels = c(\"sbp0\", \"sbp4\", \"sbp8\", \"sbp12\")))\n\ndf_sbp_long %>% head()\ndf_sbp_long %>% \n    group_by(period) %>% \n    summarize(Mean = mean(bp), SD = sd(bp))\ndf_sbp_long %>% \nggplot(aes(x = period, y = bp, group = sid)) +\n    geom_line(alpha =.1, col = \"red\") +\n    stat_summary(\n        fun.data = mean_sdl, \n        geom=\"line\", \n        colour = \"black\", \n        linewidth = .8, \n        group=1, linetype = 2) +\n    labs(x = \"Time Blood pressure taken\",\n         y = \"Blood pressure (mmHg)\",\n         title = \"Variation of blood pressure over the review periods\",\n         subtitle = \"Sampling done on all patients\",\n         caption = \"Source: 2010 data\")+\n    theme_bw()\naov(bp ~ period + Error(sid/period), data = df_sbp_long) %>% broom::tidy()"},{"path":"analysis-of-variance.html","id":"kruskal-wallis-test","chapter":"22 Analysis of Variance","heading":"22.5 Kruskal Wallis Test","text":"beginning section, noted ANOVA parametric test. Therefore requires approximately normally distributed dependent data workmwith. data deviates significantly normality assumptions ANOVA can used? conditions use Kruskal-Wallis test, non-parametric equivalent one-way ANOVA. tests null hypothesisH0: population distribution (centre) groups .Ha: least one population distributions (centre) differs \nrest.use systolic blood pressure drug used beforeTable 13.5:  small p-value, conclude centre least one effects\ndrug significantly different others. conclusion \ndifferent derived analysis variance. least centre\none systolic blood pressure significantly different another","code":"\ndf_syst %$% kruskal.test(systolic, drug) %>% broom::tidy()"},{"path":"analysis-of-variance.html","id":"assumptions-1","chapter":"22 Analysis of Variance","heading":"22.6 Assumptions","text":"","code":""},{"path":"ancova-manova.html","id":"ancova-manova","chapter":"23 ANCOVA & MANOVA","heading":"23 ANCOVA & MANOVA","text":"","code":""},{"path":"ancova-manova.html","id":"analysis-of-covariance-ancova","chapter":"23 ANCOVA & MANOVA","heading":"23.1 Analysis of Covariance (ANCOVA)","text":"","code":""},{"path":"ancova-manova.html","id":"multivariate-analysis-of-variance-manova","chapter":"23 ANCOVA & MANOVA","heading":"23.2 Multivariate Analysis of Variance (MANOVA)","text":"","code":""},{"path":"ancova-manova.html","id":"multivariate-analysis-of-variance-mancova","chapter":"23 ANCOVA & MANOVA","heading":"23.3 Multivariate Analysis of Variance (MANCOVA)","text":"","code":""},{"path":"ordinal-logistic-regression.html","id":"ordinal-logistic-regression","chapter":"24 Ordinal logistic regression","heading":"24 Ordinal logistic regression","text":"","code":""},{"path":"ordinal-logistic-regression.html","id":"importing-data","chapter":"24 Ordinal logistic regression","heading":"24.1 Importing data","text":"presentation, analyse dataset using ordinal logistic regression.\nbegin reading data selecting desired subset.view summary dataNote anemia_cat variable ordered factor variable.\ncompleteness single missing observation variable hosp_adm\nrecoded .","code":"\ndataF <- \n    dget(\"./Data/anemia_data\") %>% \n    select(sid, anemia_cat, community, fever, sex,\n           famsize, moccup2, foccup2, hosp_visit)dataF %>% glimpse()\nRows: 476\nColumns: 9\n$ sid        <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, …\n$ anemia_cat <fct> Mild, Moderate, Normal, Severe, Mild, M…\n$ community  <fct> Kasei, Kasei, Kasei, Kasei, Kasei, Kase…\n$ fever      <fct> Yes, Yes, Yes, Yes, Yes, No, No, Yes, Y…\n$ sex        <fct> Female, Female, Female, Male, Male, Fem…\n$ famsize    <dbl> 4, 4, 2, 3, 5, 4, 9, 4, 4, 10, 3, 4, 3,…\n$ moccup2    <fct> Farmer, Farmer, Other, Other, Farmer, F…\n$ foccup2    <fct> Farmer, Farmer, Other, Farmer, Farmer, …\n$ hosp_visit <fct> No, No, No, Yes, Yes, No, No, No, No, Y…dataF <- dataF %>% \n    mutate(hosp_visit = forcats::fct_explicit_na(hosp_visit, na_level = \"No\"))\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `hosp_visit =\n  forcats::fct_explicit_na(hosp_visit, na_level = \"No\")`.\nCaused by warning:\n! `fct_explicit_na()` was deprecated in forcats 1.0.0.\nℹ Please use `fct_na_value_to_level()` instead.\nsummary(dataF)\n      sid           anemia_cat       community   fever    \n Min.   :  1.0   Normal  : 92   Asuogya   : 61   No :160  \n 1st Qu.:119.8   Mild    :143   Sunkwae   : 62   Yes:316  \n Median :240.5   Moderate:226   Dromankoma:229            \n Mean   :240.2   Severe  : 15   Kasei     :124            \n 3rd Qu.:360.2                                            \n Max.   :501.0                                            \n     sex         famsize         moccup2      foccup2   \n Male  :252   Min.   : 0.000   Farmer:314   Farmer:355  \n Female:224   1st Qu.: 4.000   Other :162   Other :121  \n              Median : 5.000                            \n              Mean   : 5.151                            \n              3rd Qu.: 6.000                            \n              Max.   :11.000                            \n hosp_visit\n No :320   \n Yes:156   \n           \n           \n           \n           "},{"path":"ordinal-logistic-regression.html","id":"model-specification","chapter":"24 Ordinal logistic regression","heading":"24.2 Model specification","text":"Now begin ordinal regression fixing first model, Null model.Subsequently, introduce fever variable independent express \nresults 95%CItermestimatestd.errorstatisticp.valueconf.lowconf.highcoef.typecharacternumericnumericnumericnumericnumericnumericcharacterNormal|Mild0.3060.163-7.2680.000interceptMild|Moderate1.2570.1521.5050.132interceptModerate|Severe40.1110.29312.6010.000interceptfeverYes1.4610.1812.0900.0371.0242.086locationResults indicate significant association fever degree anaemia (=1.46, 95%CI: 1.02 2.09). Performing ANOVA test see exists difference 2 models.Table 6.3:  results indicate adding fever Null model significantly improves null model.Next, add community variabletermestimatestd.errorstatisticp.valueconf.lowconf.highcoef.typecharacternumericnumericnumericnumericnumericnumericcharacterNormal|Mild0.1910.286-5.7890.000interceptMild|Moderate0.8080.274-0.7750.438interceptModerate|Severe27.0580.3678.9850.000interceptfeverYes1.3730.1831.7280.0840.9581.966locationcommunitySunkwae1.1790.3430.4790.6320.6022.314locationcommunityDromankoma0.6260.268-1.7470.0810.3681.054locationcommunityKasei0.4630.289-2.6590.0080.2610.813location","code":"Model_0 <- ordinal::clm(anemia_cat ~ 1, data = dataF, link = \"logit\")\nsummary(Model_0)\nformula: anemia_cat ~ 1\ndata:    dataF\n\n link  threshold nobs logLik  AIC     niter max.grad\n logit flexible  476  -543.39 1092.77 7(0)  2.15e-13\n cond.H \n 1.4e+01\n\nThreshold coefficients:\n                Estimate Std. Error z value\nNormal|Mild     -1.42885    0.11608 -12.310\nMild|Moderate   -0.02521    0.09168  -0.275\nModerate|Severe  3.42535    0.26237  13.056\nModel_1 <- ordinal::clm(anemia_cat ~ fever, data = dataF, link = \"logit\")\n\nbroom::tidy(Model_1, conf.int = TRUE, exponentiate = TRUE)%>% \n    flextable::as_flextable() %>% \n    flextable::colformat_double(\n        j = c(\"estimate\", \"std.error\", \"statistic\", \"p.value\", \n              \"conf.low\", \"conf.high\"), \n        digits = 3)\nanova(Model_0, Model_1)\nModel_2 <- \n     ordinal::clm(anemia_cat ~ fever + community, data = dataF, link = \"logit\")\n\nbroom::tidy(Model_2, conf.int = TRUE, exponentiate = TRUE)%>% \n    flextable::as_flextable() %>% \n    flextable::colformat_double(\n        j = c(\"estimate\", \"std.error\", \"statistic\", \"p.value\", \n              \"conf.low\", \"conf.high\"), \n        digits = 3)"},{"path":"ordinal-logistic-regression.html","id":"checking-proportional-odds-assumption-for-the-model","chapter":"24 Ordinal logistic regression","heading":"24.3 Checking proportional odds assumption for the model","text":"check proportional odd assumption second modelTable 6.4:  significant p-value community variable indicates breach proportional odd assumption","code":"\nordinal::nominal_test(Model_2)"},{"path":"ordinal-logistic-regression.html","id":"prediction","chapter":"24 Ordinal logistic regression","heading":"24.4 Prediction","text":"section, use model created predict observation specific anaemia severity group. First, begin forming prediction data call newData.Table 6.5:  now predict probability specific predictor combination falls within specific outcome category (anaemia category)better visualisation, bind original data predictions\nTable 6.7: Probabilities\n","code":"\nNewData <- expand.grid(community = levels(dataF$community),\n                       fever = levels(dataF$fever))\nNewData(preds <- predict(Model_2, newdata = NewData, type = \"prob\"))\n$fit\n     Normal      Mild  Moderate     Severe\n1 0.1601782 0.2868320 0.5173493 0.03564053\n2 0.1392894 0.2675468 0.5514246 0.04173921\n3 0.2335081 0.3300308 0.4138463 0.02261481\n4 0.2915487 0.3440404 0.3475709 0.01684010\n5 0.1220012 0.2486393 0.5810802 0.04827929\n6 0.1054658 0.2277287 0.6103913 0.05641416\n7 0.1816335 0.3030775 0.4845071 0.03078186\n8 0.2306604 0.3289444 0.4174245 0.02297070\nbind_cols(NewData, preds$fit) %>% \n    kableExtra::kbl(caption = \"Probabilities\", booktabs = T, digits = 3) %>%\n    kableExtra::kable_classic(full_width = F, html_font = \"Cambria\") %>% \n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))"},{"path":"ordinal-logistic-regression.html","id":"visualising-the-model","chapter":"24 Ordinal logistic regression","heading":"24.5 Visualising the model","text":"visualize model using MASS effects packages. begin fitting model polr function.visualise probability various forms anaemia giving one belonging various groups.","code":"\npol_model.1 <- MASS::polr(anemia_cat ~ community, data = dataF)\npol_model.2 <- MASS::polr(anemia_cat ~ fever*community, data = dataF)M1 <- effects::Effect(focal.predictors = \"community\", mod=pol_model.1)\n\nRe-fitting to get Hessian\nM2 <- effects::Effect(focal.predictors = c(\"community\", \"fever\"), mod=pol_model.2)\n\nRe-fitting to get Hessian\nplot(M1)\nplot(M2)"},{"path":"survival-analysis.html","id":"survival-analysis","chapter":"25 Survival analysis","heading":"25 Survival analysis","text":"aforementioned analysis deals presence absence outcome. take consideration long took outcome occur. Survival analysis designed purpose, analysis time events. instance, analysing long takes car develop faulty engine, person develop heart attack divorce marriage require survival analysis.","code":""},{"path":"survival-analysis.html","id":"terminologies","chapter":"25 Survival analysis","heading":"25.1 Terminologies","text":"survival analysis, subjects usually followed period time main interest duration takes event occur. understand survival analysis need familiarise terminologies. Consider Figure 42. beginning year 2001, study initiated determine survival women treated breast cancer. six horizontal lines represent six patients recruited study indicate time recruitment study, long study outcome.Survival: generally termed probability individual subject survives beginning say study observation (t0 = 0) specific time (t) without experiencing event. study one determining survival breast cancer patients, five-year survival probability patient alive five years \ndiagnosis observation. also called survival function often represented S(t).Hazard: hazard hand probability individual observation time t0 event time t. Relating breast cancer example \nhazard five years probability patient dead five years. hazard function often represented \\(h(t)\\) \\(\\lambda(t)\\). Note survivor function deals event (death) occurring hazard function deals event (death) occurring.Event: event interested death. example, death event interest divorce, relapse cancer, etc. depending outcome interest \nstudied. diagram, event (death) coloured red. Patient recruited 2001 died 2003 E recruited study 2003 died 2005. Hence patients\nstudy 2 years experienced event death.Censor: Patients experience event usually said censored, literally meaning time develop event determined within study period.\nPatient B instance recruited beginning study lost follow-(green dot) 2004. Patient D recruited study 2002 also lost follow \n2005. Patients C F stayed study till end without experiencing event. patients B, C, D, F idea end follow-period events \noccur. , outcome termed censored. six lines give us characteristic scenarios may occur right censored data.Median Survival: time half study subjects expected experienced event. words, chance surviving (experiencing event) beyond \ntime 50% population disease exposure studied.section, use lung data survival package. First, look details lung data usingTable 7.2:  good understanding lung data readers advised read help page revealed help() function . Note time duration participant followed \nstudy. status hand indicates patient experienced outcome, case, death followed censored, indicating death never occurred follow-period.","code":"\nlibrary(survival)\nlung %>% head()"},{"path":"survival-analysis.html","id":"the-survival-object","chapter":"25 Survival analysis","heading":"25.2 The survival object","text":"initial step survival analysis R (matter many statistical software) creation survival object. achieved R function Surv() survival package. function requires least two arguments supplied. include time, indicating duration subject study status indicating whether event occurred . demonstrated using “lung” dataset survival package.Surv()function time supplied numeric variable event either expressed 0 (event censored) 1 (event occurring) FALSE (event censoring) TRUE (occurrence event). command used logical version (TRUE/FALSE) created vector TRUE (status 2) FALSE (status otherwise).survival object essentially conversion time object incorporates time status. combine time status lung dataset survival object. can seen time variable survival object patient died patients censored outcomes, plus (+) sign attached.Table 7.3:  ","code":"Surv.1 <- Surv(time=lung$time, event=lung$status==2)\nSurv.1\n [1]  306   455  1010+  210   883  1022+  310   361   218   166   170   654 \n[13]  728    71   567   144   613   707    61    88   301    81   624   371 \n[25]  394   520   574   118   390    12   473    26   533   107    53   122 \n[37]  814   965+   93   731   460   153   433   145   583    95   303   519 \n[49]  643   765   735   189    53   246   689    65     5   132   687   345 \n[61]  444   223   175    60   163    65   208   821+  428   230   840+  305 \n[73]   11   132   226   426   705   363    11   176   791    95   196+  167 \n[85]  806+  284   641   147   740+  163 \n [ reached getOption(\"max.print\") -- omitted 138 entries ]cbind(lung[,2:3], Surv.1) %>% head()\nWarning in value[[jj]][ri] <- if (is.factor(xij)) as.vector(xij) else xij:\nnumber of items to replace is not a multiple of replacement length"},{"path":"survival-analysis.html","id":"the-survival-fit-object","chapter":"25 Survival analysis","heading":"25.3 The survival fit object","text":"Next, survival object created accordance analysis intended done. non-stratified analysis currently fitted 1 right side formula shown \nstratified survival done stratifying variable right side formula.analysis try determine median survival time persons included studyThe survival fit object sfit.1 like linear regression logistic regression object little important information first hand. shows number records used, median survival time (time expected half patients event) confidence interval. analysis indicates 310 days half patients died! output simple can revealed attributes. displayed\n","code":"sfit.1 <- survfit(Surv.1 ~ 1)\nsfit.1\nCall: survfit(formula = Surv.1 ~ 1)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363attributes(sfit.1)\n$names\n [1] \"n\"         \"time\"      \"n.risk\"    \"n.event\"   \"n.censor\"  \"surv\"     \n [7] \"std.err\"   \"cumhaz\"    \"std.chaz\"  \"type\"      \"logse\"     \"conf.int\" \n[13] \"conf.type\" \"lower\"     \"upper\"     \"call\"     \n\n$class\n[1] \"survfit\""},{"path":"survival-analysis.html","id":"kaplan-meier-life-table","chapter":"25 Survival analysis","heading":"25.4 Kaplan-Meier life table","text":"compact view attributes, summarise survfit object. displayed belowThe long table produced output called Kaplan-Meier life table. contains individual survival probabilities 95% confidence intervals every point time study. instance, proportion survived end day 30 0.9561 (95%CI: 0.9299 0.983). rather long life table can significantly compacted show specific times. instance, decide show survival life table every 90 days can specify times ","code":"summary(sfit.1)\nCall: survfit(formula = Surv.1 ~ 1)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    5    228       1   0.9956 0.00438       0.9871        1.000\n   11    227       3   0.9825 0.00869       0.9656        1.000\n   12    224       1   0.9781 0.00970       0.9592        0.997\n   13    223       2   0.9693 0.01142       0.9472        0.992\n   15    221       1   0.9649 0.01219       0.9413        0.989\n   26    220       1   0.9605 0.01290       0.9356        0.986\n   30    219       1   0.9561 0.01356       0.9299        0.983\n   31    218       1   0.9518 0.01419       0.9243        0.980\n   53    217       2   0.9430 0.01536       0.9134        0.974\n   54    215       1   0.9386 0.01590       0.9079        0.970\n   59    214       1   0.9342 0.01642       0.9026        0.967\n   60    213       2   0.9254 0.01740       0.8920        0.960\n [ reached getOption(\"max.print\") -- omitted 127 rows ]summary(sfit.1, times=seq(0,900,90))\nCall: survfit(formula = Surv.1 ~ 1)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    0    228       0   1.0000  0.0000       1.0000        1.000\n   90    201      27   0.8816  0.0214       0.8406        0.925\n  180    160      36   0.7217  0.0298       0.6655        0.783\n  270    108      30   0.5753  0.0338       0.5128        0.645\n  360     70      24   0.4340  0.0358       0.3693        0.510\n  450     48      16   0.3287  0.0356       0.2659        0.406\n  540     33      10   0.2554  0.0344       0.1962        0.333\n  630     22       7   0.1958  0.0330       0.1407        0.272\n  720     14       8   0.1246  0.0290       0.0789        0.197\n  810      7       5   0.0783  0.0246       0.0423        0.145\n  900      3       2   0.0503  0.0228       0.0207        0.123"},{"path":"survival-analysis.html","id":"kaplan-meier-curve","chapter":"25 Survival analysis","heading":"25.5 Kaplan-Meier curve","text":"Kaplan-Meier life table can plotted give graphical representation called Kaplan-Meier curve. resulting curve plot Survival probability time. example shown figure plotted withThe green dotted line plot cuts curve median survival time. Also, plot confidence interval superimposed (dotted red lines).","code":"\nsurvminer::ggsurvplot(\n    sfit.1, \n    data = lung,\n    palette= 'blue',\n    surv.median.line = \"hv\",\n    censor =F,\n    title = \"Kaplan-Meier Curve with 95% CI\",\n    ggtheme = theme_bw())"},{"path":"survival-analysis.html","id":"survival-for-subgroups","chapter":"25 Survival analysis","heading":"25.6 Survival for subgroups","text":"determine survival separately subgroups need fit survival object specifying requisite group. instance, determine survival among different sexes fit object . First, make sure sex variable right format, factor.fit appropriate survfit object. Note right-hand side formula time sex 1.presented survfit object displaying records, number events, median survival confidence intervals. statistics however separate \nsex. instance, realise number deaths males twice females median survival time males 270 days lesser females. , complete life table, summarise survfit object.rather long life table can best appreciated KM curve. go ahead plot Kaplan-Meier curve two sexes shown Figure 44 plot withThe figure shows KM survival curve separately males females. can seen two plots completely separate, sign survival sexes. steeper curve males indicates relatively worse outcome time females. median survival time instance (indicated green dotted line) intersects males’ curve much earlier females indicating much lower median survival time males compared females.","code":"\nlung <- \n    lung %>% \n    mutate(sex = factor(sex, levels = 1:2, labels = c(\"Male\", \"Female\")))sfit.2 <- survfit(Surv.1 ~ lung$sex)\nsfit.2\nCall: survfit(formula = Surv.1 ~ lung$sex)\n\n                  n events median 0.95LCL 0.95UCL\nlung$sex=Male   138    112    270     212     310\nlung$sex=Female  90     53    426     348     550summary(sfit.2)\nCall: survfit(formula = Surv.1 ~ lung$sex)\n\n                lung$sex=Male \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n   11    138       3   0.9783  0.0124       0.9542        1.000\n   12    135       1   0.9710  0.0143       0.9434        0.999\n   13    134       2   0.9565  0.0174       0.9231        0.991\n   15    132       1   0.9493  0.0187       0.9134        0.987\n   26    131       1   0.9420  0.0199       0.9038        0.982\n   30    130       1   0.9348  0.0210       0.8945        0.977\n   31    129       1   0.9275  0.0221       0.8853        0.972\n   53    128       2   0.9130  0.0240       0.8672        0.961\n   54    126       1   0.9058  0.0249       0.8583        0.956\n   59    125       1   0.8986  0.0257       0.8496        0.950\n   60    124       1   0.8913  0.0265       0.8409        0.945\n   65    123       2   0.8768  0.0280       0.8237        0.933\n [ reached getOption(\"max.print\") -- omitted 87 rows ]\n\n                lung$sex=Female \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    5     90       1   0.9889  0.0110       0.9675        1.000\n   60     89       1   0.9778  0.0155       0.9478        1.000\n   61     88       1   0.9667  0.0189       0.9303        1.000\n   62     87       1   0.9556  0.0217       0.9139        0.999\n   79     86       1   0.9444  0.0241       0.8983        0.993\n   81     85       1   0.9333  0.0263       0.8832        0.986\n   95     83       1   0.9221  0.0283       0.8683        0.979\n  107     81       1   0.9107  0.0301       0.8535        0.972\n  122     80       1   0.8993  0.0318       0.8390        0.964\n  145     79       2   0.8766  0.0349       0.8108        0.948\n  153     77       1   0.8652  0.0362       0.7970        0.939\n  166     76       1   0.8538  0.0375       0.7834        0.931\n [ reached getOption(\"max.print\") -- omitted 39 rows ]\nplot(\n    sfit.2, \n    col=1:2, \n    lty=1:2, \n    mark.time=F,\n    main=\"K-M curve by sex\",\n    xlab = \"Days\", ylab=\"Survival\")\nlegend(\"topright\", c(\"Male\",\"Female\"), lty=1:2, col=1:2)\nabline(h=0.5, lty=2, col=3)\nsurvminer::ggsurvplot(\n    sfit.2, \n    data = lung,\n    surv.median.line = \"hv\",\n    censor = F,\n    title = \"Kaplan-Meier Curve by sex (with 95% CI)\",\n    ggtheme = theme_bw(),\n    conf.int = T,\n    xlab = \"Days\")"},{"path":"survival-analysis.html","id":"cumulative-incidence-hazard","chapter":"25 Survival analysis","heading":"25.7 Cumulative Incidence (Hazard)","text":"Another way expressing relationship time outcome outcome Cumulative Incidence (CI). CI point time measure frequency outcome period time. can determined R . First, fit survival object.survival object similar done previously time fitting used argument type=\"mstate\" specify preference cumulative incidence. use summary() function survfit object generate CI table.Table 13.1:  Similar survival cumulative incidence table can drawn . obvious addition presence prevalence column, indicating cumulative prevalence outcome time point.pictorial overview cumulative incidence can also obtained using commands shown figure .","code":"sfit.3 <- \n    survfit(\n        Surv(\n            time = time, \n            event = status==2, \n            type = \"mstate\") ~ 1, \n        data=lung)\nsfit.3\nCall: survfit(formula = Surv(time = time, event = status == 2, type = \"mstate\") ~ \n    1, data = lung)\n\n       n nevent   rmean*\n(s0) 228      0 376.2747\nTRUE 228    165 645.7253\n   *restricted mean time in state (max time = 1022 )\nsfit.3 %>% broom::tidy() %>% head(10)"},{"path":"survival-analysis.html","id":"comparing-survival-between-two-or-more-groups","chapter":"25 Survival analysis","heading":"25.8 Comparing survival between two or more groups","text":"often arises survival hazard two groups compared. instance randomised controlled trial determine survival patients chronic disease 3 drugs can given three randomly selected groups. period follow-, one want compare group survived best. various non-parametric tests available help. Standing tall among log-rank test (Peto et al, 1977) going apply.","code":""},{"path":"survival-analysis.html","id":"the-log-rank-test","chapter":"25 Survival analysis","heading":"25.8.1 The Log-Rank test","text":"idea behind log-rank test test proportional hazards among groups compared. test null hypothesis hazard equal populations consideration. p-value generated probability hazard equal populations consideration. Hence small p-value indicates differences exist least two members group. use log-rank test determine hazards two sexes significantly different lung cancer patients. Remember pictorially comparison median survival already know females seem better males put formal test. done R using survdiff() initial survival object created.small p-value (p= 0.00131) leads us conclusion hazards within two groups significantly differ.","code":"survdiff(Surv.1 ~ lung$sex)\nCall:\nsurvdiff(formula = Surv.1 ~ lung$sex)\n\n                  N Observed Expected (O-E)^2/E (O-E)^2/V\nlung$sex=Male   138      112     91.6      4.55      10.3\nlung$sex=Female  90       53     73.4      5.68      10.3\n\n Chisq= 10.3  on 1 degrees of freedom, p= 0.001 "},{"path":"survival-analysis.html","id":"stratified-log-rank-test","chapter":"25 Survival analysis","heading":"25.8.2 Stratified Log-Rank test","text":"trying figure females tend better males lung cancer ask questions. One : difference ages two sexes took part study? begin evaluate assertion compare ages two sexes.Table 13.3:  Well, appears males average older females. reason males worse? remove possibility age reason males may worse perform stratified log-rank test belowThe analysis asks question: Assuming ages sexes equal among sexes, survival still depend sex patient? analysis, appears age difference reason difference hazards observed. significant p-value hardly affected stratify age.","code":"\nlung %>% \n    group_by(sex) %>% \n    meantables::mean_table(.x=age)survdiff(Surv.1 ~ lung$sex + strata(lung$age))\nCall:\nsurvdiff(formula = Surv.1 ~ lung$sex + strata(lung$age))\n\n                  N Observed Expected (O-E)^2/E (O-E)^2/V\nlung$sex=Male   138      112     96.6      2.47      9.42\nlung$sex=Female  90       53     68.4      3.48      9.42\n\n Chisq= 9.4  on 1 degrees of freedom, p= 0.002 "},{"path":"survival-analysis.html","id":"cox-regression","chapter":"25 Survival analysis","heading":"25.9 Cox regression","text":"Similar observation performed chi-squared test categorical data analysis, generated p-value comparable effect like odds ratio risk ratio used log-rank test. instance say hazard males twice females. Cox regression analysis kicks . Cox regression analysis generates Hazard Ratio (HR) ratio similar odds ratio time compares hazards various groups. Like , HR null value 1, hazard equal\ngroups compared.\nCox regression added advantage able determine HR one variable time thereby correcting respective effects stratifying time.\nCox proportional modelling implemented R using function coxph(). determine association sex hazard lung data.Table 13.6:  Just like regression functions R, output scanty. However, negative coeff indicated hazard less females compared males (referent group). Exponentiating coefficient gives hazard ratio. shown second column matrix heading exp(coef) value 0.588. value indicates females half hazard males. concert already know. last column respective p-value associated coefficient hazard ratio. tests hypothesis hazard ratio equal 1 (null value). Therefore small p-value makes significant finding. last 2 columns confidence interval.mentioned one advantages using Cox proportional regression ability put one variable time. determine hazard ratios sex age concurrently correcting .Table 13.7:  finally determine HRs stratifying ECOG performance score (0=good 5=dead)Table 25.1:  Putting three one Cox’s regression equation table belowConclusion: can concluded hazards differ significantly two sexes,\nfemales surviving better males lung cancer. Though ages males \nstudy relatively higher females difference age explain completely\nmales worse females. words, difference survival two sexes \nirrespective age.","code":"\ncox.1 <- coxph(Surv.1 ~ sex, data = lung)\ncox.1 %>% \n    broom::tidy(exponentiate = TRUE, conf.int = TRUE)\ncox.2 <- coxph(Surv.1 ~ sex + age, data = lung)\ncox.2 %>% \n    broom::tidy(exponentiate = TRUE, conf.int = TRUE)\ncox.3 <- coxph(Surv.1 ~ sex + age + strata(ph.ecog), data = lung)\ncox.3 %>% \n    broom::tidy(exponentiate = TRUE, conf.int = TRUE)\nlung %>%\n    select(sex, ph.ecog, age, status, time) %>%\n    survival::coxph(\n        survival::Surv(event = status==2, time = time) ~ ., \n        data = .) %>%\n    gtsummary::tbl_regression(\n        label = list(\n            age ~ \"Age (years)\",\n            sex ~ \"Sex\",\n            ph.ecog = \"PH.ECOG\"),\n        pvalue_fun = ~ gtsummary::style_pvalue(.x, digits = 3),\n        exponentiate = TRUE) %>%\n    gtsummary::bold_labels() %>%\n    gtsummary::bold_p()"},{"path":"cox-regression-1.html","id":"cox-regression-1","chapter":"26 Cox Regression","heading":"26 Cox Regression","text":"","code":""},{"path":"blocks.html","id":"blocks","chapter":"27 Blocks","heading":"27 Blocks","text":"","code":""},{"path":"blocks.html","id":"equations","chapter":"27 Blocks","heading":"27.1 Equations","text":"equation.\\[\\begin{equation}\n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  \\tag{27.1}\n\\end{equation}\\]may refer using \\@ref(eq:binom), like see Equation (27.1).","code":""},{"path":"blocks.html","id":"theorems-and-proofs","chapter":"27 Blocks","heading":"27.2 Theorems and proofs","text":"Labeled theorems can referenced text using \\@ref(thm:tri), example, check smart theorem 27.1.Theorem 27.1  right triangle, \\(c\\) denotes length hypotenuse\n\\(\\) \\(b\\) denote lengths two sides, \n\\[^2 + b^2 = c^2\\]Read https://bookdown.org/yihui/bookdown/markdown-extensions--bookdown.html.","code":""},{"path":"blocks.html","id":"callout-blocks","chapter":"27 Blocks","heading":"27.3 Callout blocks","text":"bs4_book theme also includes special callout blocks, like .rmdnote.can use markdown inside block.user define appearance blocks LaTeX output.may also use: .rmdcaution, .rmdimportant, .rmdtip, .rmdwarning block name.R Markdown Cookbook provides help use custom blocks design callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html","code":"head(beaver1, n = 5)\n  day time  temp activ\n1 346  840 36.33     0\n2 346  850 36.34     0\n3 346  900 36.35     0\n4 346  910 36.42     0\n5 346  920 36.55     0"},{"path":"footnotes-and-citations.html","id":"footnotes-and-citations","chapter":"28 Footnotes and citations","heading":"28 Footnotes and citations","text":"","code":""},{"path":"footnotes-and-citations.html","id":"footnotes","chapter":"28 Footnotes and citations","heading":"28.1 Footnotes","text":"Footnotes put inside square brackets caret ^[]. Like one 1.","code":""},{"path":"footnotes-and-citations.html","id":"citations","chapter":"28 Footnotes and citations","heading":"28.2 Citations","text":"Reference items bibliography file(s) using @key.example, using bookdown package2 (check last code chunk index.Rmd see citation key added) sample book, built top R Markdown knitr3 (citation added manually external file book.bib).\nNote .bib files need listed index.Rmd YAML bibliography key.bs4_book theme makes footnotes appear inline click . example book, added csl: chicago-fullnote-bibliography.csl index.Rmd YAML, include .csl file. download new style, recommend: https://www.zotero.org/styles/RStudio Visual Markdown Editor can also make easier insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations","code":""},{"path":"cross.html","id":"cross","chapter":"29 Cross-references","heading":"29 Cross-references","text":"Cross-references make easier readers find link elements book.","code":""},{"path":"cross.html","id":"chapters-and-sub-chapters","chapter":"29 Cross-references","heading":"29.1 Chapters and sub-chapters","text":"two steps cross-reference heading:Label heading: # Hello world {#nice-label}.\nLeave label like automated heading generated based heading title: example, # Hello world = # Hello world {#hello-world}.\nlabel un-numbered heading, use: # Hello world {-#nice-label} {# Hello world .unnumbered}.\nLeave label like automated heading generated based heading title: example, # Hello world = # Hello world {#hello-world}.label un-numbered heading, use: # Hello world {-#nice-label} {# Hello world .unnumbered}.Next, reference labeled heading anywhere text using \\@ref(nice-label); example, please see Chapter 29.\nprefer text link instead numbered reference use: text want can go .\nprefer text link instead numbered reference use: text want can go .","code":""},{"path":"cross.html","id":"captioned-figures-and-tables","chapter":"29 Cross-references","heading":"29.2 Captioned figures and tables","text":"Figures tables captions can also cross-referenced elsewhere book using \\@ref(fig:chunk-label) \\@ref(tab:chunk-label), respectively.See Figure 29.1.\nFigure 29.1: nice figure!\nDon’t miss Table 29.1.Table 29.1: nice table!","code":"\npar(mar = c(4, 4, .1, .1))\nplot(pressure, type = 'b', pch = 19)\nknitr::kable(\n  head(pressure, 10), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)"},{"path":"parts.html","id":"parts","chapter":"30 Parts","heading":"30 Parts","text":"can add parts organize one book chapters together. Parts can inserted top .Rmd file, first-level chapter heading file.Add numbered part: # (PART) Act one {-} (followed # chapter)Add unnumbered part: # (PART\\*) Act one {-} (followed # chapter)Add appendix special kind un-numbered part: # (APPENDIX) stuff {-} (followed # chapter). Chapters appendix prepended letters instead numbers.","code":""},{"path":"sharing-your-book.html","id":"sharing-your-book","chapter":"31 Sharing your book","heading":"31 Sharing your book","text":"","code":""},{"path":"sharing-your-book.html","id":"publishing","chapter":"31 Sharing your book","heading":"31.1 Publishing","text":"HTML books can published online, see: https://bookdown.org/yihui/bookdown/publishing.html","code":""},{"path":"sharing-your-book.html","id":"pages","chapter":"31 Sharing your book","heading":"31.2 404 pages","text":"default, users directed 404 page try access webpage found. ’d like customize 404 page instead using default, may add either _404.Rmd _404.md file project root use code /Markdown syntax.","code":""},{"path":"sharing-your-book.html","id":"metadata-for-sharing","chapter":"31 Sharing your book","heading":"31.3 Metadata for sharing","text":"Bookdown HTML books provide HTML metadata social sharing platforms like Twitter, Facebook, LinkedIn, using information provide index.Rmd YAML. setup, set url book path cover-image file. book’s title description also used.bs4_book provides enhanced metadata social sharing, chapter shared unique description, auto-generated based content.Specify book’s source repository GitHub repo _output.yml file, allows users view chapter’s source file suggest edit. Read features output format :https://pkgs.rstudio.com/bookdown/reference/bs4_book.htmlOr use:","code":"\n?bookdown::bs4_book"}]
